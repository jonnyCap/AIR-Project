{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# User Interaction with our Model\n",
    "This Notebook a simple pipeline, chaining our submodels and allowing users to interact with our model. The user should be allowed to type in his idea and then get a response including the following information:\n",
    "\n",
    "1. Listing and Information about similar Businesses\n",
    "2. A Stock market prediction\n",
    "3. A success score and how it will perform compared to its competitors (ranking + score)\n",
    "\n",
    "## Important notes\n",
    "The following parameters must be kept the same, otherwise the training wont have much effect on the outcome.\n",
    "- The PredictionModel.forward() method has the param **retrieval_number**, which must be set to **10**.\n",
    "- The PredictionModel has the param **forecaset_steps** which must be set to **12**, to align with the ranking models inputs."
   ],
   "id": "4c4701dcd0b0e8b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:44:33.098655Z",
     "start_time": "2025-01-10T13:44:25.587775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from RetrievalSystem.RetrievalSystem import RetrievalSystem\n",
    "from PredictionModel.RetrievalAugmentedPredictionModel import RetrievalAugmentedPredictionModel\n",
    "from RankingSystem.RankingModelNew import EnhancedRankingModel\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "IDEA_IDENTIFIER = \"NEW_IDEA\"\n",
    "\n",
    "class UserInterface:\n",
    "    def __init__(self):\n",
    "        self.max_length = 512\n",
    "\n",
    "        self.dataset_path = \"Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "        self.embeddings_path = \"RetrievalSystem/Embeddings/embeddings.csv\"\n",
    "\n",
    "        self.rap_model_weights_path = \"Training/Model/trained_model_aux-true_CustomLoss.pth\"\n",
    "        self.ranking_model_weights_path = \"non_existent\"\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=False)\n",
    "\n",
    "        self.dataset = pd.read_csv(self.dataset_path)\n",
    "        self.retrieval_model = RetrievalSystem(self.embeddings_path, 10)\n",
    "\n",
    "        self.historical_scaler_path = \"Dataset/Data/Scaler/historical_scaler.pkl\"\n",
    "        self.ranking_historical_scaler_path = \"Dataset/Data/Scaler/ranking_historical_scaler.pkl\"\n",
    "\n",
    "        self.prediction_model = RetrievalAugmentedPredictionModel(ret_sys=self.retrieval_model, retrieval_number=10, forecast_steps=12)\n",
    "        if os.path.exists(self.rap_model_weights_path):\n",
    "            state_dict = torch.load(self.rap_model_weights_path, map_location=torch.device('cpu'))\n",
    "            self.prediction_model.load_state_dict(state_dict)\n",
    "            print(\"Loading RAP model weights...\")\n",
    "\n",
    "        self.ranking_model = EnhancedRankingModel()\n",
    "        if os.path.exists(self.ranking_model_weights_path):\n",
    "            state_dict = torch.load(self.ranking_model_weights_path, map_location=torch.device('cpu'))\n",
    "            self.ranking_model.load_state_dict(state_dict)\n",
    "\n",
    "    def predict(self, text: str, retrieval_number: int = 10):\n",
    "        if not text:\n",
    "            raise ValueError(\"Please provide some text\")\n",
    "\n",
    "        # Retrieval\n",
    "        retrieval_result = self.retrieval_model.find_similar_entries_for_batch(texts=[text], top_n=retrieval_number)\n",
    "        idea_embedding, retrieved_documents = retrieval_result[0]\n",
    "        tickers = retrieved_documents[\"tickers\"].values\n",
    "\n",
    "        documents = self.dataset.copy()\n",
    "        documents = documents[documents[\"tickers\"].isin(tickers)]\n",
    "\n",
    "        # Prediction\n",
    "        prediction = self.prediction_model(\n",
    "            retrieval_result=retrieval_result,\n",
    "            dataset=self.dataset,\n",
    "            use_auxiliary_inputs=False\n",
    "        )\n",
    "\n",
    "        # TODO: SHOULD BE DONE AFTER RANKING MODEL\n",
    "        month_columns = [col for col in self.dataset.columns if col.startswith(\"month\")]\n",
    "        if os.path.exists(self.historical_scaler_path) and os.path.exists(self.ranking_historical_scaler_path):\n",
    "            with open(self.historical_scaler_path, \"rb\") as scaler_file:\n",
    "                historical_scaler = joblib.load(scaler_file)\n",
    "\n",
    "            with open(self.ranking_historical_scaler_path, \"rb\") as ranking_scaler_file:\n",
    "                ranking_historical_scaler = joblib.load(ranking_scaler_file)\n",
    "\n",
    "            norm_predictions = torch.zeros(1, 72)\n",
    "            norm_predictions[0, 60:72] = prediction\n",
    "            norm_predictions_np = norm_predictions.cpu().detach().numpy()\n",
    "            denorm_predictions_np = historical_scaler.inverse_transform(norm_predictions_np)\n",
    "            documents[month_columns] = denorm_predictions_np[:, 60:72]\n",
    "\n",
    "            #TODO: Scale with ranking scaler back down\n",
    "        else:\n",
    "            print(\"Couldnt load all required scalers\")\n",
    "            # raise FileNotFoundError(\"Historical Scaler not found\")\n",
    "\n",
    "        # Ranking\n",
    "        ratings = {}\n",
    "\n",
    "        idea_embedding = torch.tensor(idea_embedding, dtype=torch.float).unsqueeze(0)\n",
    "        print(f\"Shape of idea embedding: { idea_embedding.shape }\")\n",
    "        ratings[IDEA_IDENTIFIER] = self.ranking_model(idea_encoding=idea_embedding, stock_performance=prediction)\n",
    "\n",
    "        # Ratings for similar companies\n",
    "        print(f\"Ranking competitors\")\n",
    "        competitors = []\n",
    "        for ticker in tickers:\n",
    "            document = documents[documents[\"tickers\"] == ticker]\n",
    "            stock_performance = document[month_columns].iloc[:, -24:]\n",
    "            stock_performance_tensor = torch.tensor(stock_performance.values, dtype=torch.float32)\n",
    "            similar_document_embeddings = retrieved_documents[retrieved_documents[\"tickers\"] == ticker][\"embedding\"]\n",
    "            similar_embeddings_array = np.stack(similar_document_embeddings.values)\n",
    "            similar_embeddings_tensor = torch.tensor(similar_embeddings_array, dtype=torch.float32)\n",
    "\n",
    "            # Add to ratings\n",
    "            ratings[ticker] = self.ranking_model(idea_encoding=similar_embeddings_tensor, stock_performance=stock_performance_tensor)\n",
    "\n",
    "            # Collect competitor info\n",
    "            competitors.append({\n",
    "                \"ticker\": ticker,\n",
    "                \"business_description\": document[\"business_description\"].values[0],\n",
    "                \"rating\": ratings[ticker].item()\n",
    "            })\n",
    "\n",
    "        # Unified output\n",
    "        result = {\n",
    "            \"retrieved_documents\": documents.to_dict(orient=\"records\"),\n",
    "            \"prediction\": prediction[0].tolist(),\n",
    "            \"ratings\": {\n",
    "                \"new_idea\": ratings[IDEA_IDENTIFIER].item(),\n",
    "                \"competitors\": competitors\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def visualize_results(self, result):\n",
    "        # Extract data from the result\n",
    "        retrieved_documents = result[\"retrieved_documents\"]\n",
    "        prediction = result[\"prediction\"]\n",
    "        ratings = result[\"ratings\"]\n",
    "\n",
    "        # Preview retrieved documents with a short description\n",
    "        print(\"Retrieved Documents Preview:\")\n",
    "        for doc in retrieved_documents:\n",
    "            print(f\"- {doc['tickers']}: {doc['business_description'][:50]}...\")\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        month_columns = [col for col in self.dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "        # Plot each company's performance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for doc in retrieved_documents:\n",
    "            ticker = doc[\"tickers\"]\n",
    "            month_data = [doc[col] for col in month_columns if col in doc]\n",
    "            plt.plot(month_data, label=ticker)\n",
    "        plt.title(\"Performance of Retrieved Companies\")\n",
    "        plt.xlabel(\"Months\")\n",
    "        plt.ylabel(\"Performance\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the predicted idea performance\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(prediction, label=\"New Idea\", color=\"red\")\n",
    "        plt.title(\"Predicted Performance of New Idea\")\n",
    "        plt.xlabel(\"Months\")\n",
    "        plt.ylabel(\"Performance\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # List companies sorted by rank\n",
    "        print(\"\\nCompanies Ranked by Rating:\")\n",
    "        ranked_companies = sorted(ratings[\"competitors\"], key=lambda x: x[\"rating\"], reverse=True)\n",
    "        for idx, company in enumerate(ranked_companies, start=1):\n",
    "            print(f\"{idx}. {company['ticker']} - Rating: {company['rating']:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "1357790a063a6a05",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan-maier/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main\n",
    "Here we can now test our UserInterface"
   ],
   "id": "39d02ae198a632ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T13:44:41.370256Z",
     "start_time": "2025-01-10T13:44:33.105796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    idea = \"I want to build a stockprediction model that can be used to predict stock performance for potential business idea. This should help to gather first insights into the potential of an idea. Additionally it should lookup similar companies so that on overview over competitors is given. It should also rate the new idea based on the predicted stock performance and rank them among its competitors.\"\n",
    "\n",
    "    # idea = \"Lego Shop\"\n",
    "\n",
    "    idea = \"Able View Global Inc. operates as brand management partners of beauty and personal care brands in China. Its brand management services encompass various segments of the brand management value chain, including strategy, branding, digital and social marketing, omni-channel sales, customer services, overseas logistics, and warehouse and fulfilment. The company was incorporated in 2021 and is based in Shanghai, China.\"\n",
    "\n",
    "    idea = \"An AI-driven personal finance platform that provides automated budgeting, expense tracking, and investment advice, with customizable dashboards tailored to individual financial goals.\"\n",
    "\n",
    "    userinterface = UserInterface()\n",
    "    response = userinterface.predict(idea)\n",
    "    userinterface.visualize_results(response)\n",
    "\n",
    "    # print(response)"
   ],
   "id": "904d87b97ef8d1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAP model weights...\n",
      "Retrieved tickers:  [array(['AIVI', 'UTES', 'MVV', 'LDUR', 'FNDX', 'YCS', 'YCL', 'FNDA', 'IAU',\n",
      "       'SSO'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([1, 10, 106])\n",
      "Shapes: month_data: (10, 72), static_data: (10, 34)\n",
      "Static Tensor Shape: torch.Size([1, 340]), Historical Tensor Shape: torch.Size([1, 720])\n",
      "Shape of weighted_sum: torch.Size([1, 384]), attention_weights: torch.Size([1, 10, 1])\n",
      "Shape of static_output: torch.Size([1, 128]), similarity: torch.Size([1, 10]), historical: torch.Size([1, 256])\n",
      "Shapes: weighted_sum: torch.Size([1, 384]), attention_weights: torch.Size([1, 10]), combined_static_output: torch.Size([1, 128]), combined_historical: torch.Size([1, 256]), similarity: torch.Size([1, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([1, 788])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Shapes of static_output: torch.Size([1, 16]), historical_output: torch.Size([1, 32]), idea: torch.Size([1, 128]), attention_output: torch.Size([1, 384])\n",
      "Couldnt load all required scalers\n",
      "Shape of idea embedding: torch.Size([1, 384])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 268, but got 512",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m idea \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn AI-driven personal finance platform that provides automated budgeting, expense tracking, and investment advice, with customizable dashboards tailored to individual financial goals.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m userinterface \u001B[38;5;241m=\u001B[39m UserInterface()\n\u001B[0;32m---> 11\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43muserinterface\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43midea\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m userinterface\u001B[38;5;241m.\u001B[39mvisualize_results(response)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# print(response)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[1], line 87\u001B[0m, in \u001B[0;36mUserInterface.predict\u001B[0;34m(self, text, retrieval_number)\u001B[0m\n\u001B[1;32m     85\u001B[0m idea_embedding \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(idea_embedding, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape of idea embedding: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;250m \u001B[39midea_embedding\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 87\u001B[0m ratings[IDEA_IDENTIFIER] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mranking_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43midea_encoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midea_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstock_performance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprediction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Ratings for similar companies\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRanking competitors\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/RankingSystem/RankingModelNew.py:142\u001B[0m, in \u001B[0;36mEnhancedRankingModel.forward\u001B[0;34m(self, idea_encoding, stock_performance)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;66;03m# Concatenate the features\u001B[39;00m\n\u001B[1;32m    140\u001B[0m combined_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((idea_features, stock_features), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 142\u001B[0m attention_output, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_features\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mcombined_features\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mcombined_features\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# Remove the sequence dimension added by attention (squeeze dim=1)\u001B[39;00m\n\u001B[1;32m    147\u001B[0m attention_output \u001B[38;5;241m=\u001B[39m attention_output\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1368\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   1342\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[1;32m   1343\u001B[0m         query,\n\u001B[1;32m   1344\u001B[0m         key,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1365\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal,\n\u001B[1;32m   1366\u001B[0m     )\n\u001B[1;32m   1367\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1368\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1369\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1370\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1371\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1372\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1374\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1375\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1378\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1380\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1381\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1386\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1388\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[1;32m   1390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/functional.py:6070\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   6063\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key_padding_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   6064\u001B[0m         \u001B[38;5;66;03m# We have the attn_mask, and use that to merge kpm into it.\u001B[39;00m\n\u001B[1;32m   6065\u001B[0m         \u001B[38;5;66;03m# Turn off use of is_causal hint, as the merged mask is no\u001B[39;00m\n\u001B[1;32m   6066\u001B[0m         \u001B[38;5;66;03m# longer causal.\u001B[39;00m\n\u001B[1;32m   6067\u001B[0m         is_causal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   6069\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m-> 6070\u001B[0m     embed_dim \u001B[38;5;241m==\u001B[39m embed_dim_to_check\n\u001B[1;32m   6071\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwas expecting embedding dimension of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00membed_dim_to_check\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00membed_dim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   6072\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(embed_dim, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m   6073\u001B[0m     \u001B[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001B[39;00m\n\u001B[1;32m   6074\u001B[0m     head_dim \u001B[38;5;241m=\u001B[39m embed_dim\u001B[38;5;241m.\u001B[39mdiv(num_heads, rounding_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrunc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: was expecting embedding dimension of 268, but got 512"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
