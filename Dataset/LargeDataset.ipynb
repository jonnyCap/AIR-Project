{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hybrid Mode - Dataset Creation \n",
    "we want to use the following structure to train our model:\n",
    "\n",
    "Input:\n",
    "- idea_text: \"AI-powered e-commerce platform...\"\n",
    "- static_features: [market_size, investment, competition, team_strength]\n",
    "- historical_stock: [month_1, month_2, ..., month_36]\n",
    "\n",
    "Target:\n",
    "- stock_performance (next 12 months): [month_37, month_38, ..., month_48]\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This should then produce the follwoing output with our model:\n",
    "\n",
    "Input:\n",
    "- idea_text: \"AI-powered e-commerce platform...\"\n",
    "\n",
    "Optional Input:\n",
    "- static_features: [market_size, investment, competition, team_strength]\n",
    "\n",
    "Output:\n",
    "- Predicted stock performance (next 12 months): [month_1, month_2, ..., month_12]\n"
   ],
   "id": "2c0ff1aebfe8d5b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Huggingface Dataset\n",
    "We will use the shortbread/tickers dataset to get tickers of US companies"
   ],
   "id": "642373f903f45483"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.458495Z",
     "start_time": "2024-12-30T18:13:52.412900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "def get_tickers_dataset(file_path: str) -> (pd.DataFrame, dict):\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename Symbol column to tickers\n",
    "    dataset.rename(columns={'Symbol': 'tickers'}, inplace=True)\n",
    "\n",
    "    # Transform IPO Year into Company Age\n",
    "    current_year = datetime.now().year\n",
    "    dataset['Company_Age'] = current_year - dataset['IPO Year'].fillna(current_year).astype(int)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    dataset = dataset.drop(columns=['Name', 'Net Change', 'IPO Year'])\n",
    "\n",
    "    # Clean monetary and percentage columns\n",
    "    monetary_columns = ['Market Cap', 'Last Sale']\n",
    "    percentage_columns = ['% Change']\n",
    "\n",
    "    # Remove symbols and convert to numeric\n",
    "    for col in monetary_columns:\n",
    "        dataset[col] = dataset[col].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "    for col in percentage_columns:\n",
    "        dataset[col] = dataset[col].replace(r'[\\%]', '', regex=True).astype(float)\n",
    "\n",
    "    # Handle missing values\n",
    "    # Fill numeric columns with 0\n",
    "    numeric_columns = ['Market Cap', 'Volume', '% Change', 'Last Sale']\n",
    "    dataset[numeric_columns] = dataset[numeric_columns].fillna(0)\n",
    "\n",
    "    # Fill categorical columns with \"General\" or \"Unknown\"\n",
    "    categorical_columns = ['Country', 'Sector', 'Industry']\n",
    "    dataset[categorical_columns] = dataset[categorical_columns].fillna(\"General\")\n",
    "\n",
    "    # Apply Label Encoding to categorical columns\n",
    "    encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        dataset[col] = encoder.fit_transform(dataset[col])\n",
    "        encoders[col] = encoder  # Save the encoder for reuse\n",
    "\n",
    "    # Set tickers as the index\n",
    "    dataset.set_index('tickers', inplace=True)\n",
    "    return dataset, encoders\n",
    "\n",
    "\n",
    "def extend_dataset(initial_dataset: pd.DataFrame, encoders: dict) -> pd.DataFrame:\n",
    "    # Load the new dataset\n",
    "    new_dataset = load_dataset(\"shortbread/tickers\", split=\"train\")\n",
    "\n",
    "    # Convert the new dataset to a pandas DataFrame\n",
    "    new_df = pd.DataFrame(new_dataset)\n",
    "\n",
    "    # Select only the required column and rename 'symbol' to 'tickers'\n",
    "    updated_dataset = new_df[['symbol']].rename(columns={'symbol': 'tickers'})\n",
    "\n",
    "    # Ensure 'tickers' is accessible in initial_dataset\n",
    "    if 'tickers' not in initial_dataset.columns:\n",
    "        # If tickers is the index, reset it\n",
    "        if initial_dataset.index.name == 'tickers':\n",
    "            initial_dataset = initial_dataset.reset_index()\n",
    "        else:\n",
    "            raise KeyError(\"'tickers' column is missing in the initial dataset.\")\n",
    "\n",
    "    # Check for tickers in the updated_dataset that are not in the initial_dataset\n",
    "    initial_tickers = set(initial_dataset['tickers'])\n",
    "    updated_tickers = set(updated_dataset['tickers'])\n",
    "\n",
    "    # Get the tickers that are unique to the updated dataset\n",
    "    unique_tickers = updated_tickers - initial_tickers\n",
    "    print(f\"Number of tickers unique to the updated dataset: {len(unique_tickers)}\")\n",
    "\n",
    "    # Filter the updated dataset to include only unique tickers\n",
    "    unique_tickers_df = updated_dataset[updated_dataset['tickers'].isin(unique_tickers)]\n",
    "\n",
    "    # Prepare to enrich the unique tickers with additional data\n",
    "    enriched_data = []\n",
    "    required_columns = [\n",
    "        \"Last Sale\", \"Market Cap\", \"% Change\", \"Volume\", \"Country\", \"Sector\", \"Industry\"\n",
    "    ]\n",
    "\n",
    "    # Process each unique ticker\n",
    "    for ticker in unique_tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info  # Fetch data from yfinance\n",
    "\n",
    "            # Enrichment logic\n",
    "            previous_close = info.get(\"regularMarketPreviousClose\", None)\n",
    "            current_open = info.get(\"regularMarketOpen\", None)\n",
    "\n",
    "            enriched_row = {\n",
    "                \"tickers\": ticker,\n",
    "                # Use totalAssets as a proxy for Market Cap if marketCap is not available\n",
    "                \"Market Cap\": info.get(\"marketCap\", None) or info.get(\"totalAssets\", None),\n",
    "                # Use regularMarketPreviousClose or regularMarketDayHigh for Last Sale\n",
    "                \"Last Sale\": info.get(\"regularMarketPreviousClose\", None) or info.get(\"regularMarketDayHigh\", None),\n",
    "                # Calculate % Change from regularMarketOpen and regularMarketPreviousClose if not directly available\n",
    "                \"% Change\": (\n",
    "                                ((current_open - previous_close) / previous_close) * 100\n",
    "                                if previous_close and current_open else None\n",
    "                            ) or info.get(\"regularMarketChangePercent\", None),\n",
    "                \"Volume\": info.get(\"regularMarketVolume\", None),\n",
    "                \"Country\": info.get(\"country\", \"General\"),\n",
    "                \"Sector\": info.get(\"sector\", \"General\"),\n",
    "                \"Industry\": info.get(\"industry\", \"General\"),\n",
    "            }\n",
    "\n",
    "            enriched_data.append(enriched_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data for ticker {ticker}: {e}\")\n",
    "\n",
    "    # Convert enriched data to a DataFrame\n",
    "    enriched_df = pd.DataFrame(enriched_data)\n",
    "\n",
    "    # Apply the same LabelEncoder to categorical columns in the enriched data\n",
    "    categorical_columns = ['Country', 'Sector', 'Industry']\n",
    "    for col in categorical_columns:\n",
    "        if col in enriched_df.columns:\n",
    "            enriched_df[col] = enriched_df[col].map(\n",
    "                lambda x: encoders[col].transform([x])[0] if x in encoders[col].classes_ else -1\n",
    "            )\n",
    "\n",
    "    # Merge the initial dataset with the enriched dataset\n",
    "    extended_dataset = pd.concat([initial_dataset, enriched_df], ignore_index=True)\n",
    "\n",
    "    # Set tickers as the index\n",
    "    extended_dataset.set_index('tickers', inplace=True)\n",
    "\n",
    "    return extended_dataset\n"
   ],
   "id": "f70cbe34b1001e85",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fetching additional data\n",
    "We use the dataset from Hugginface as startingpoint and will now provide methods to fetch additional data"
   ],
   "id": "2aa42139fb1eced4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.627585Z",
     "start_time": "2024-12-30T18:13:53.536856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "\n",
    "def fetch_all_data_for_ticker(ticker, months=72):\n",
    "    \"\"\"\n",
    "    Fetch all required information for a given ticker in a single API call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "\n",
    "        # Fetch historical stock data\n",
    "        hist = stock.history(period=\"max\", interval=\"1mo\")\n",
    "\n",
    "        if hist.empty:\n",
    "            # print(f\"$ {ticker}: No price data found (possibly delisted or insufficient history).\")\n",
    "            return None\n",
    "\n",
    "        # Adjust the range to available data if it's less than the requested months\n",
    "        padding = max(0, months - len(hist))\n",
    "        months = min(months, len(hist))\n",
    "\n",
    "        # Extract the last `months` of closing prices\n",
    "        monthly_close = [None] * padding + hist[\"Close\"][-months:].tolist()\n",
    "\n",
    "        # Fetch business summary, investment, team strength, and other metrics\n",
    "        info = stock.info  # Single call to retrieve all general info\n",
    "\n",
    "        description = info.get(\"longBusinessSummary\", None)\n",
    "        if description is None:\n",
    "            return None\n",
    "\n",
    "        investment = stock.balance_sheet.loc[\"Total Assets\"].iloc[0] if not stock.balance_sheet.empty else None\n",
    "        team_strength = info.get(\"fullTimeEmployees\", None)\n",
    "\n",
    "        # Extract additional metrics\n",
    "        ebit = info.get(\"ebitda\", None)  # Approximation if EBITDA is provided\n",
    "        revenue = info.get(\"totalRevenue\", None)\n",
    "        revenue_growth = info.get(\"revenueGrowth\", None)\n",
    "        net_income = info.get(\"netIncomeToCommon\", None)\n",
    "        operating_margins = info.get(\"operatingMargins\", None)\n",
    "        ebitda_margins = info.get(\"ebitdaMargins\", None)\n",
    "        free_cashflow = info.get(\"freeCashflow\", None)\n",
    "        total_debt = info.get(\"totalDebt\", None)\n",
    "        current_ratio = info.get(\"currentRatio\", None)\n",
    "        quick_ratio = info.get(\"quickRatio\", None)\n",
    "\n",
    "        # Valuation and market metrics\n",
    "        enterprise_value = info.get(\"enterpriseValue\", None)\n",
    "        price_to_sales = info.get(\"priceToSalesTrailing12Months\", None)\n",
    "        trailing_pe = info.get(\"trailingPE\", None)\n",
    "        forward_pe = info.get(\"forwardPE\", None)\n",
    "        profit_margins = info.get(\"profitMargins\", None)\n",
    "        return_on_assets = info.get(\"returnOnAssets\", None)\n",
    "\n",
    "        # Stock price and volatility\n",
    "        fifty_two_week_high = info.get(\"fiftyTwoWeekHigh\", None)\n",
    "        fifty_two_week_low = info.get(\"fiftyTwoWeekLow\", None)\n",
    "        beta = info.get(\"beta\", None)\n",
    "\n",
    "        # Analyst sentiment\n",
    "        recommendation_mean = info.get(\"recommendationMean\", None)\n",
    "        recommendation_key = info.get(\"recommendationKey\", None)\n",
    "        target_high_price = info.get(\"targetHighPrice\", None)\n",
    "        target_low_price = info.get(\"targetLowPrice\", None)\n",
    "        target_mean_price = info.get(\"targetMeanPrice\", None)\n",
    "\n",
    "        # Return data as a dictionary\n",
    "        return {\n",
    "            \"tickers\": ticker,\n",
    "            \"business_description\": description,\n",
    "            \"investment\": investment,\n",
    "            \"team_strength\": team_strength,\n",
    "            \"ebit\": ebit,\n",
    "            \"revenue\": revenue,\n",
    "            \"revenue_growth\": revenue_growth,\n",
    "            \"net_income\": net_income,\n",
    "            \"operating_margins\": operating_margins,\n",
    "            \"ebitda_margins\": ebitda_margins,\n",
    "            \"free_cashflow\": free_cashflow,\n",
    "            \"total_debt\": total_debt,\n",
    "            \"current_ratio\": current_ratio,\n",
    "            \"quick_ratio\": quick_ratio,\n",
    "            \"enterprise_value\": enterprise_value,\n",
    "            \"price_to_sales\": price_to_sales,\n",
    "            \"trailing_pe\": trailing_pe,\n",
    "            \"forward_pe\": forward_pe,\n",
    "            \"profit_margins\": profit_margins,\n",
    "            \"return_on_assets\": return_on_assets,\n",
    "            \"fifty_two_week_high\": fifty_two_week_high,\n",
    "            \"fifty_two_week_low\": fifty_two_week_low,\n",
    "            \"beta\": beta,\n",
    "            \"recommendation_mean\": recommendation_mean,\n",
    "            \"recommendation_key\": recommendation_key,\n",
    "            \"target_high_price\": target_high_price,\n",
    "            \"target_low_price\": target_low_price,\n",
    "            \"target_mean_price\": target_mean_price,\n",
    "            **{f\"month_{i+1}_performance\": sp for i, sp in enumerate(monthly_close or [None] * months)}\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_real_dataset(tickers, months):\n",
    "    \"\"\"\n",
    "    Create dataset using a single API call per ticker while retaining existing data.\n",
    "    \"\"\"\n",
    "    # List to collect new fetched data\n",
    "    new_data = []\n",
    "\n",
    "    for _, entry in tickers.iterrows():\n",
    "        ticker = entry['tickers']  # Access 'tickers' column\n",
    "\n",
    "        # Fetch all data for the ticker\n",
    "        ticker_data = fetch_all_data_for_ticker(ticker, months)\n",
    "\n",
    "        if ticker_data is None:\n",
    "            continue\n",
    "\n",
    "        # Append fetched data to the new_data list\n",
    "        new_data.append(ticker_data)\n",
    "\n",
    "    # Convert new_data to a DataFrame\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    if new_data_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Filter the original dataset to only include tickers with fetched data\n",
    "    valid_tickers = new_data_df['tickers']\n",
    "    filtered_tickers = tickers[tickers['tickers'].isin(valid_tickers)]\n",
    "\n",
    "    # Merge the original dataset with the new data\n",
    "    merged_dataset = filtered_tickers.merge(new_data_df, on='tickers', how='inner')\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def process_tickers_in_batches(tickers, batch_size=50, output_file=\"Data/real_company_stock_dataset.csv\", months=72):\n",
    "    # Ensure 'tickers' column exists\n",
    "    if tickers.index.name == 'tickers':\n",
    "        tickers.reset_index(inplace=True)\n",
    "\n",
    "    # Process tickers in batches\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers.iloc[i:i + batch_size].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\"Processing batch {i // batch_size + 1} of {len(tickers) // batch_size + 1}...\")\n",
    "\n",
    "        # Process the batch using the existing method\n",
    "        batch_data = create_real_dataset(batch, months=months)\n",
    "\n",
    "        # Check if batch_data is empty\n",
    "        if batch_data.empty:\n",
    "            # Log all tickers from this batch as skipped\n",
    "            batch['reason'] = 'Missing Data'  # Safe because of .copy()\n",
    "            batch.to_csv(\"skipped_tickers.csv\", mode='a', header=False, index=False)\n",
    "            continue\n",
    "\n",
    "        # Log skipped tickers\n",
    "        skipped_tickers = batch[~batch['tickers'].isin(batch_data['tickers'])].copy()  # Use .copy()\n",
    "        if not skipped_tickers.empty:\n",
    "            skipped_tickers['reason'] = 'Missing Data'  # Safe because of .copy()\n",
    "            skipped_tickers.to_csv(\"skipped_tickers.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "        # Save the batch to the output file\n",
    "        if not batch_data.empty:\n",
    "            if not os.path.exists(output_file):\n",
    "                batch_data.to_csv(output_file, index=False)\n",
    "            else:\n",
    "                batch_data.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Batch {i // batch_size + 1} processed and saved. Skipped Tickers: {len(skipped_tickers)}\")\n",
    "\n",
    "        # Add a delay between batches to respect API rate limits\n",
    "        time.sleep(10)  # Adjust as needed\n"
   ],
   "id": "38faad8c6043dd59",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalization Function",
   "id": "9eb7873025674662"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.639350Z",
     "start_time": "2024-12-30T18:13:53.633149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_full_dataset(input_file=\"Data/real_company_stock_dataset.csv\",\n",
    "                           output_file=\"Data/normalized_real_company_stock_dataset.csv\",\n",
    "                           scalar_directory=\"Data/Scalar/\"):\n",
    "    try:\n",
    "        dataset = pd.read_csv(input_file)\n",
    "\n",
    "        # Identify static and historical columns\n",
    "        excluded_columns = ['tickers', 'business_description']\n",
    "        static_columns = [col for col in dataset.columns if col not in excluded_columns and not col.startswith('month_')]\n",
    "        historical_columns = [col for col in dataset.columns if col.startswith('month_')]\n",
    "\n",
    "        # Create a dictionary to store scalers for each static column\n",
    "        static_scalers = {}\n",
    "\n",
    "        # Ensure the scalar directory exists\n",
    "        os.makedirs(scalar_directory, exist_ok=True)\n",
    "\n",
    "        # Normalize static columns\n",
    "        for col in static_columns:\n",
    "            try:\n",
    "                scaler = StandardScaler()  # or MinMaxScaler()\n",
    "                dataset[col] = scaler.fit_transform(dataset[[col]])\n",
    "                static_scalers[col] = scaler\n",
    "\n",
    "                # Save the scaler for the column\n",
    "                scaler_path = f\"{scalar_directory}/{col}_scaler.pkl\"\n",
    "                joblib.dump(scaler, scaler_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing column '{col}': {e}\")\n",
    "                print(f\"First few problematic entries in column '{col}':\")\n",
    "                print(dataset[[col]].head())  # Display the first few entries in the problematic column\n",
    "                return  # Abort the normalization process\n",
    "\n",
    "        # Normalize historical columns using a single scaler\n",
    "        try:\n",
    "            historical_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset[historical_columns] = historical_scaler.fit_transform(dataset[historical_columns])\n",
    "\n",
    "            # Save the historical scaler\n",
    "            historical_scaler_path = f\"{scalar_directory}/historical_scaler.pkl\"\n",
    "            joblib.dump(historical_scaler, historical_scaler_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing historical columns: {e}\")\n",
    "            print(f\"First few problematic entries in historical columns:\")\n",
    "            print(dataset[historical_columns].head())  # Display the first few rows of the problematic columns\n",
    "            return  # Abort the normalization process\n",
    "\n",
    "        # Save the normalized dataset\n",
    "        dataset.to_csv(output_file, index=False)\n",
    "\n",
    "        print(\"Normalization complete. Static scalers saved for each column. Historical scaler updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during normalization: {e}\")\n"
   ],
   "id": "cb77edd2a9bb30af",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove Null Values\n",
    "Just to make sure that there are no null values, we go through the dataset and replace them with -1.\n"
   ],
   "id": "679c9da954a63ec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.682445Z",
     "start_time": "2024-12-30T18:13:53.679527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_null_values(path):\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        created_dataset = pd.read_csv(path)\n",
    "\n",
    "        # Replace all null values with 0\n",
    "        created_dataset = created_dataset.fillna(0)\n",
    "\n",
    "        # Save the updated dataset back to the same file\n",
    "        created_dataset.to_csv(path, index=False)\n",
    "\n",
    "        print(f\"Null values in {path} replaced with 0.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {path}: {e}\")\n",
    "\n",
    "\n",
    "# Call method\n",
    "# remove_null_values(NORMALIZED_OUTPUT_FILE)\n"
   ],
   "id": "3a29389760251d5f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fixture Method\n",
    "As I made a small mistake for the base dataset here is a method to replace just that part"
   ],
   "id": "f9243cd669673c6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.732012Z",
     "start_time": "2024-12-30T18:13:53.728753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fix_encoded_columns(base_file: str, enriched_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Fix encoding inconsistencies in the enriched dataset by replacing problematic columns.\n",
    "    \"\"\"\n",
    "    # Step 1: Load the base dataset\n",
    "    base_dataset = pd.read_csv(base_file, index_col='tickers')\n",
    "\n",
    "    # Step 2: Load the already enriched dataset\n",
    "    enriched_dataset = pd.read_csv(enriched_file)\n",
    "\n",
    "    # Ensure 'tickers' is present\n",
    "    if 'tickers' not in enriched_dataset.columns:\n",
    "        # If tickers is the index, reset it\n",
    "        if enriched_dataset.index.name == 'tickers':\n",
    "            enriched_dataset.reset_index(inplace=True)\n",
    "        else:\n",
    "            raise KeyError(\"'tickers' column is missing in the enriched dataset.\")\n",
    "\n",
    "    # Step 3: Set 'tickers' as the index for alignment\n",
    "    enriched_dataset.set_index('tickers', inplace=True)\n",
    "\n",
    "    # Filter the base dataset to only include tickers in the enriched dataset\n",
    "    overlapping_tickers = base_dataset.loc[base_dataset.index.intersection(enriched_dataset.index)]\n",
    "\n",
    "    # Step 4: Replace inconsistent columns with properly encoded ones\n",
    "    columns_to_fix = ['Country', 'Sector', 'Industry']\n",
    "    enriched_dataset.update(overlapping_tickers[columns_to_fix])\n",
    "\n",
    "    # Step 5: Save the updated enriched dataset\n",
    "    enriched_dataset.reset_index(inplace=True)\n",
    "    enriched_dataset.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Encoded columns fixed. Updated dataset saved to {output_file}\")\n"
   ],
   "id": "7de056e09c1cfd42",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Label Encoder\n",
    "This method can be used to later on encode additional columns (Used for recommendation_key as of right now)\n"
   ],
   "id": "eeadf20a6f31f1f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.782042Z",
     "start_time": "2024-12-30T18:13:53.778845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def encode_columns(file_path: str, columns_to_encode: list, output_file: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Encodes specified columns in a dataset using LabelEncoder.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the dataset file (CSV).\n",
    "    - columns_to_encode (list): List of column names to encode.\n",
    "    - output_file (str): Optional. Path to save the updated dataset. If None, the dataset won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The dataset with encoded columns.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the columns exist in the dataset\n",
    "    missing_columns = [col for col in columns_to_encode if col not in dataset.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following columns are missing in the dataset: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Apply LabelEncoder to the specified columns\n",
    "    label_encoders = {}  # To store encoders for each column\n",
    "    for col in columns_to_encode:\n",
    "        encoder = LabelEncoder()\n",
    "        dataset[col] = encoder.fit_transform(dataset[col].fillna(\"Unknown\"))\n",
    "        label_encoders[col] = encoder  # Save the encoder in case we need to use it later\n",
    "\n",
    "    # Save the updated dataset if output_file is provided\n",
    "    if not output_file:\n",
    "        print(\"Storing to input path\")\n",
    "        output_file = file_path\n",
    "\n",
    "    dataset.to_csv(output_file, index=False)\n",
    "    print(f\"Updated dataset saved to {output_file}\")\n"
   ],
   "id": "304b297e671f4ab4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Further Processing - Replacing inf by 99 Quartil\n",
    "the 'price_to_sales' column might have the value 'inf' instead of a number which we need to replace"
   ],
   "id": "ebc487ae89a11122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:13:53.831875Z",
     "start_time": "2024-12-30T18:13:53.828639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def handle_inf_in_columns(dataset: pd.DataFrame, columns: list, cap_percentile: float = 0.99) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle infinite values in specified columns of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): The dataset containing the columns to process.\n",
    "    - columns (list): List of column names to process.\n",
    "    - cap_percentile (float): The percentile to cap each column at (default is 99th percentile).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated dataset with infinite values replaced.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in dataset.columns:\n",
    "            raise ValueError(f\"'{col}' column not found in the dataset.\")\n",
    "\n",
    "        # Replace inf and -inf with NaN\n",
    "        dataset[col] = dataset[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Compute the 99th percentile cap value\n",
    "        upper_limit = dataset[col].quantile(cap_percentile)\n",
    "\n",
    "        # Replace NaN values (previously inf) with the median\n",
    "        median_value = dataset[col].median()\n",
    "\n",
    "        print(f\"Handling '{col}': Capping at {upper_limit}, replacing NaN with median {median_value}\")\n",
    "\n",
    "        # Cap the values above the upper limit\n",
    "        dataset[col] = np.where(\n",
    "            dataset[col] > upper_limit,\n",
    "            upper_limit,\n",
    "            dataset[col]\n",
    "        )\n",
    "\n",
    "        # Fill remaining NaN values with the median\n",
    "        dataset[col] = dataset[col].fillna(median_value)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ],
   "id": "d08bdb1e2b7a66f2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Function\n",
    "Here we execute the above functions"
   ],
   "id": "f2c556a39d3930aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T18:15:08.486268Z",
     "start_time": "2024-12-30T18:14:43.689287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "INPUT_FILE = \"Ticker_Datasets/nasdaq_screener_1733257202149.csv\"\n",
    "OUTPUT_FILE = \"Data/real_company_stock_dataset_large.csv\"\n",
    "NORMALIZED_OUTPUT_FILE = \"Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "INTERMEDIATE_OUTPUT_FILE = \"Data/initial_dataset.csv\"\n",
    "SCALER_FOLDER = \"Data/Scaler/\"\n",
    "\n",
    "REPLACE_INITIAL_DATASET = False\n",
    "ENRICH = False\n",
    "REMOVE_NULL = False\n",
    "NORMALIZE = True\n",
    "LABEL_ENCODE = False\n",
    "REPLACE_INF = False\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Simulated list of 10,000 tickers\n",
    "    if RECREATE_INITAL_DATASET:\n",
    "        dataset, encoders = get_tickers_dataset(INPUT_FILE)\n",
    "        extended_dataset = extend_dataset(dataset, encoders)\n",
    "        print(dataset.head())\n",
    "\n",
    "        # Fill null values\n",
    "        dataset = dataset.fillna(0)\n",
    "\n",
    "        # Store meanwhile\n",
    "        extended_dataset.to_csv(INTERMEDIATE_OUTPUT_FILE, index=True)\n",
    "\n",
    "    else:\n",
    "        dataset = pd.read_csv(INTERMEDIATE_OUTPUT_FILE, index_col='tickers')\n",
    "\n",
    "    if ENRICH:\n",
    "        process_tickers_in_batches(dataset, output_file=OUTPUT_FILE, batch_size=100, months=72)\n",
    "\n",
    "    if REPLACE_INITIAL_DATASET:\n",
    "        fix_encoded_columns(INTERMEDIATE_OUTPUT_FILE, OUTPUT_FILE, OUTPUT_FILE)\n",
    "\n",
    "    if REMOVE_NULL:\n",
    "        # Normalize data\n",
    "        remove_null_values(OUTPUT_FILE)\n",
    "\n",
    "    if LABEL_ENCODE:\n",
    "        encode_columns(OUTPUT_FILE, ['recommendation_key'], OUTPUT_FILE)\n",
    "\n",
    "    if REPLACE_INF:\n",
    "        adapted_dataset = pd.read_csv(OUTPUT_FILE)\n",
    "        # Columns to handle for infinite values\n",
    "        columns_to_handle = ['price_to_sales', 'trailing_pe', 'forward_pe']\n",
    "\n",
    "        # Handle infinite values in the specified columns\n",
    "        dataset = handle_inf_in_columns(adapted_dataset, columns_to_handle)\n",
    "\n",
    "        adapted_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "\n",
    "    if NORMALIZE:\n",
    "        normalize_full_dataset(input_file=OUTPUT_FILE, output_file=NORMALIZED_OUTPUT_FILE, scalar_directory=SCALER_FOLDER)\n",
    "\n",
    "\n",
    "    print(\"Dataset creation complete!\")\n"
   ],
   "id": "23593be5af917ef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tickers unique to the updated dataset: 1975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m RECREATE_INITAL_DATASET:\n\u001B[1;32m     20\u001B[0m     dataset, encoders \u001B[38;5;241m=\u001B[39m get_tickers_dataset(INPUT_FILE)\n\u001B[0;32m---> 21\u001B[0m     extended_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mextend_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(dataset\u001B[38;5;241m.\u001B[39mhead())\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# Fill null values\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[1], line 90\u001B[0m, in \u001B[0;36mextend_dataset\u001B[0;34m(initial_dataset, encoders)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     89\u001B[0m     stock \u001B[38;5;241m=\u001B[39m yf\u001B[38;5;241m.\u001B[39mTicker(ticker)\n\u001B[0;32m---> 90\u001B[0m     info \u001B[38;5;241m=\u001B[39m \u001B[43mstock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfo\u001B[49m  \u001B[38;5;66;03m# Fetch data from yfinance\u001B[39;00m\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;66;03m# Enrichment logic\u001B[39;00m\n\u001B[1;32m     93\u001B[0m     previous_close \u001B[38;5;241m=\u001B[39m info\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregularMarketPreviousClose\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/ticker.py:159\u001B[0m, in \u001B[0;36mTicker.info\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minfo\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[0;32m--> 159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/base.py:229\u001B[0m, in \u001B[0;36mTickerBase.get_info\u001B[0;34m(self, proxy)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_info\u001B[39m(\u001B[38;5;28mself\u001B[39m, proxy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_quote\u001B[38;5;241m.\u001B[39mproxy \u001B[38;5;241m=\u001B[39m proxy \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproxy\n\u001B[0;32m--> 229\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_quote\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfo\u001B[49m\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/scrapers/quote.py:508\u001B[0m, in \u001B[0;36mQuote.info\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minfo\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 508\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fetch_info\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    509\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_complementary(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproxy)\n\u001B[1;32m    511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/scrapers/quote.py:598\u001B[0m, in \u001B[0;36mQuote._fetch_info\u001B[0;34m(self, proxy)\u001B[0m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_already_fetched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    597\u001B[0m modules \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinancialData\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquoteType\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefaultKeyStatistics\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124massetProfile\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummaryDetail\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 598\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproxy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodules\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/scrapers/quote.py:587\u001B[0m, in \u001B[0;36mQuote._fetch\u001B[0;34m(self, proxy, modules)\u001B[0m\n\u001B[1;32m    585\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodules\u001B[39m\u001B[38;5;124m\"\u001B[39m: modules, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorsDomain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfinance.yahoo.com\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformatted\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msymbol\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_symbol}\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 587\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_raw_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_QUOTE_SUMMARY_URL_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_symbol\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_agent_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mHTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    589\u001B[0m     utils\u001B[38;5;241m.\u001B[39mget_yf_logger()\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/data.py:410\u001B[0m, in \u001B[0;36mYfData.get_raw_json\u001B[0;34m(self, url, user_agent_headers, params, proxy, timeout)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_raw_json\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, user_agent_headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, proxy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m):\n\u001B[1;32m    409\u001B[0m     utils\u001B[38;5;241m.\u001B[39mget_yf_logger()\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget_raw_json(): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 410\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    411\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/utils.py:104\u001B[0m, in \u001B[0;36mlog_indent_decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEntering \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m IndentationContext():\n\u001B[0;32m--> 104\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExiting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/data.py:333\u001B[0m, in \u001B[0;36mYfData.get\u001B[0;34m(self, url, user_agent_headers, params, proxy, timeout)\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;129m@utils\u001B[39m\u001B[38;5;241m.\u001B[39mlog_indent_decorator\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, user_agent_headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, proxy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m):\n\u001B[0;32m--> 333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_method\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/utils.py:104\u001B[0m, in \u001B[0;36mlog_indent_decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEntering \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m IndentationContext():\n\u001B[0;32m--> 104\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExiting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/yfinance/data.py:378\u001B[0m, in \u001B[0;36mYfData._make_request\u001B[0;34m(self, url, request_method, user_agent_headers, body, params, proxy, timeout)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m body:\n\u001B[1;32m    376\u001B[0m     request_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m body\n\u001B[0;32m--> 378\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrequest_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrequest_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    379\u001B[0m utils\u001B[38;5;241m.\u001B[39mget_yf_logger()\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse code=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m400\u001B[39m:\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;66;03m# Retry with other cookie strategy\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/requests/sessions.py:602\u001B[0m, in \u001B[0;36mSession.get\u001B[0;34m(self, url, **kwargs)\u001B[0m\n\u001B[1;32m    594\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    595\u001B[0m \n\u001B[1;32m    596\u001B[0m \u001B[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001B[39;00m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;124;03m:rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    601\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 602\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;66;03m# Trigger any extra validation we need to do.\u001B[39;00m\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 466\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    468\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mconn\u001B[38;5;241m.\u001B[39mtimeout)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;66;03m# Force connect early to allow us to validate the connection.\u001B[39;00m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m-> 1095\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001B[39;00m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mproxy_is_verified:\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/urllib3/connection.py:693\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    692\u001B[0m     sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[0;32m--> 693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    694\u001B[0m     server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[1;32m    695\u001B[0m     tls_in_tls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/urllib3/connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \n\u001B[1;32m    196\u001B[0m \u001B[38;5;124;03m:return: New socket connection.\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/urllib3/util/connection.py:60\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mUnicodeError\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LocationParseError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, label empty or too long\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSOCK_STREAM\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     61\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n\u001B[1;32m     62\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.12/socket.py:963\u001B[0m, in \u001B[0;36mgetaddrinfo\u001B[0;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;66;03m# We override this function since we want to translate the numeric family\u001B[39;00m\n\u001B[1;32m    961\u001B[0m \u001B[38;5;66;03m# and socket type values to enum constants.\u001B[39;00m\n\u001B[1;32m    962\u001B[0m addrlist \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 963\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_socket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    964\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n\u001B[1;32m    965\u001B[0m     addrlist\u001B[38;5;241m.\u001B[39mappend((_intenum_converter(af, AddressFamily),\n\u001B[1;32m    966\u001B[0m                      _intenum_converter(socktype, SocketKind),\n\u001B[1;32m    967\u001B[0m                      proto, canonname, sa))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
