{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hybrid Mode - Dataset Creation \n",
    "we want to use the following structure to train our model:\n",
    "\n",
    "Input:\n",
    "- idea_text: \"AI-powered e-commerce platform...\"\n",
    "- static_features: [market_size, investment, competition, team_strength]\n",
    "- historical_stock: [month_1, month_2, ..., month_36]\n",
    "\n",
    "Target:\n",
    "- stock_performance (next 12 months): [month_37, month_38, ..., month_48]\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This should then produce the follwoing output with our model:\n",
    "\n",
    "Input:\n",
    "- idea_text: \"AI-powered e-commerce platform...\"\n",
    "\n",
    "Optional Input:\n",
    "- static_features: [market_size, investment, competition, team_strength]\n",
    "\n",
    "Output:\n",
    "- Predicted stock performance (next 12 months): [month_1, month_2, ..., month_12]\n"
   ],
   "id": "2c0ff1aebfe8d5b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Huggingface Dataset\n",
    "We will use the shortbread/tickers dataset to get tickers of US companies"
   ],
   "id": "642373f903f45483"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.385520Z",
     "start_time": "2024-12-06T10:56:18.364296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "def get_tickers_dataset(file_path: str) -> (pd.DataFrame, dict):\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename Symbol column to tickers\n",
    "    dataset.rename(columns={'Symbol': 'tickers'}, inplace=True)\n",
    "\n",
    "    # Transform IPO Year into Company Age\n",
    "    current_year = datetime.now().year\n",
    "    dataset['Company_Age'] = current_year - dataset['IPO Year'].fillna(current_year).astype(int)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    dataset = dataset.drop(columns=['Name', 'Net Change', 'IPO Year'])\n",
    "\n",
    "    # Clean monetary and percentage columns\n",
    "    monetary_columns = ['Market Cap', 'Last Sale']\n",
    "    percentage_columns = ['% Change']\n",
    "\n",
    "    # Remove symbols and convert to numeric\n",
    "    for col in monetary_columns:\n",
    "        dataset[col] = dataset[col].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "    for col in percentage_columns:\n",
    "        dataset[col] = dataset[col].replace(r'[\\%]', '', regex=True).astype(float)\n",
    "\n",
    "    # Handle missing values\n",
    "    # Fill numeric columns with 0\n",
    "    numeric_columns = ['Market Cap', 'Volume', '% Change', 'Last Sale']\n",
    "    dataset[numeric_columns] = dataset[numeric_columns].fillna(0)\n",
    "\n",
    "    # Fill categorical columns with \"General\" or \"Unknown\"\n",
    "    categorical_columns = ['Country', 'Sector', 'Industry']\n",
    "    dataset[categorical_columns] = dataset[categorical_columns].fillna(\"General\")\n",
    "\n",
    "    # Apply Label Encoding to categorical columns\n",
    "    encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        dataset[col] = encoder.fit_transform(dataset[col])\n",
    "        encoders[col] = encoder  # Save the encoder for reuse\n",
    "\n",
    "    # Set tickers as the index\n",
    "    dataset.set_index('tickers', inplace=True)\n",
    "    return dataset, encoders\n",
    "\n",
    "\n",
    "def extend_dataset(initial_dataset: pd.DataFrame, encoders: dict) -> pd.DataFrame:\n",
    "    # Load the new dataset\n",
    "    new_dataset = load_dataset(\"shortbread/tickers\", split=\"train\")\n",
    "\n",
    "    # Convert the new dataset to a pandas DataFrame\n",
    "    new_df = pd.DataFrame(new_dataset)\n",
    "\n",
    "    # Select only the required column and rename 'symbol' to 'tickers'\n",
    "    updated_dataset = new_df[['symbol']].rename(columns={'symbol': 'tickers'})\n",
    "\n",
    "    # Ensure 'tickers' is accessible in initial_dataset\n",
    "    if 'tickers' not in initial_dataset.columns:\n",
    "        # If tickers is the index, reset it\n",
    "        if initial_dataset.index.name == 'tickers':\n",
    "            initial_dataset = initial_dataset.reset_index()\n",
    "        else:\n",
    "            raise KeyError(\"'tickers' column is missing in the initial dataset.\")\n",
    "\n",
    "    # Check for tickers in the updated_dataset that are not in the initial_dataset\n",
    "    initial_tickers = set(initial_dataset['tickers'])\n",
    "    updated_tickers = set(updated_dataset['tickers'])\n",
    "\n",
    "    # Get the tickers that are unique to the updated dataset\n",
    "    unique_tickers = updated_tickers - initial_tickers\n",
    "    print(f\"Number of tickers unique to the updated dataset: {len(unique_tickers)}\")\n",
    "\n",
    "    # Filter the updated dataset to include only unique tickers\n",
    "    unique_tickers_df = updated_dataset[updated_dataset['tickers'].isin(unique_tickers)]\n",
    "\n",
    "    # Prepare to enrich the unique tickers with additional data\n",
    "    enriched_data = []\n",
    "    required_columns = [\n",
    "        \"Last Sale\", \"Market Cap\", \"% Change\", \"Volume\", \"Country\", \"Sector\", \"Industry\"\n",
    "    ]\n",
    "\n",
    "    # Process each unique ticker\n",
    "    for ticker in unique_tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info  # Fetch data from yfinance\n",
    "\n",
    "            # Enrichment logic\n",
    "            previous_close = info.get(\"regularMarketPreviousClose\", None)\n",
    "            current_open = info.get(\"regularMarketOpen\", None)\n",
    "\n",
    "            enriched_row = {\n",
    "                \"tickers\": ticker,\n",
    "                # Use totalAssets as a proxy for Market Cap if marketCap is not available\n",
    "                \"Market Cap\": info.get(\"marketCap\", None) or info.get(\"totalAssets\", None),\n",
    "                # Use regularMarketPreviousClose or regularMarketDayHigh for Last Sale\n",
    "                \"Last Sale\": info.get(\"regularMarketPreviousClose\", None) or info.get(\"regularMarketDayHigh\", None),\n",
    "                # Calculate % Change from regularMarketOpen and regularMarketPreviousClose if not directly available\n",
    "                \"% Change\": (\n",
    "                                ((current_open - previous_close) / previous_close) * 100\n",
    "                                if previous_close and current_open else None\n",
    "                            ) or info.get(\"regularMarketChangePercent\", None),\n",
    "                \"Volume\": info.get(\"regularMarketVolume\", None),\n",
    "                \"Country\": info.get(\"country\", \"General\"),\n",
    "                \"Sector\": info.get(\"sector\", \"General\"),\n",
    "                \"Industry\": info.get(\"industry\", \"General\"),\n",
    "            }\n",
    "\n",
    "            enriched_data.append(enriched_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data for ticker {ticker}: {e}\")\n",
    "\n",
    "    # Convert enriched data to a DataFrame\n",
    "    enriched_df = pd.DataFrame(enriched_data)\n",
    "\n",
    "    # Apply the same LabelEncoder to categorical columns in the enriched data\n",
    "    categorical_columns = ['Country', 'Sector', 'Industry']\n",
    "    for col in categorical_columns:\n",
    "        if col in enriched_df.columns:\n",
    "            enriched_df[col] = enriched_df[col].map(\n",
    "                lambda x: encoders[col].transform([x])[0] if x in encoders[col].classes_ else -1\n",
    "            )\n",
    "\n",
    "    # Merge the initial dataset with the enriched dataset\n",
    "    extended_dataset = pd.concat([initial_dataset, enriched_df], ignore_index=True)\n",
    "\n",
    "    # Set tickers as the index\n",
    "    extended_dataset.set_index('tickers', inplace=True)\n",
    "\n",
    "    return extended_dataset\n"
   ],
   "id": "f70cbe34b1001e85",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fetching additional data\n",
    "We use the dataset from Hugginface as startingpoint and will now provide methods to fetch additional data"
   ],
   "id": "2aa42139fb1eced4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.453198Z",
     "start_time": "2024-12-06T10:56:18.433541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "\n",
    "def fetch_all_data_for_ticker(ticker, months=72):\n",
    "    \"\"\"\n",
    "    Fetch all required information for a given ticker in a single API call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "\n",
    "        # Fetch historical stock data\n",
    "        hist = stock.history(period=\"max\", interval=\"1mo\")\n",
    "\n",
    "        if hist.empty:\n",
    "            # print(f\"$ {ticker}: No price data found (possibly delisted or insufficient history).\")\n",
    "            return None\n",
    "\n",
    "        # Adjust the range to available data if it's less than the requested months\n",
    "        padding = max(0, months - len(hist))\n",
    "        months = min(months, len(hist))\n",
    "\n",
    "        # Extract the last `months` of closing prices\n",
    "        monthly_close = [None] * padding + hist[\"Close\"][-months:].tolist()\n",
    "\n",
    "        # Fetch business summary, investment, team strength, and other metrics\n",
    "        info = stock.info  # Single call to retrieve all general info\n",
    "\n",
    "        description = info.get(\"longBusinessSummary\", None)\n",
    "        if description is None:\n",
    "            return None\n",
    "\n",
    "        investment = stock.balance_sheet.loc[\"Total Assets\"].iloc[0] if not stock.balance_sheet.empty else None\n",
    "        team_strength = info.get(\"fullTimeEmployees\", None)\n",
    "\n",
    "        # Extract additional metrics\n",
    "        ebit = info.get(\"ebitda\", None)  # Approximation if EBITDA is provided\n",
    "        revenue = info.get(\"totalRevenue\", None)\n",
    "        revenue_growth = info.get(\"revenueGrowth\", None)\n",
    "        net_income = info.get(\"netIncomeToCommon\", None)\n",
    "        operating_margins = info.get(\"operatingMargins\", None)\n",
    "        ebitda_margins = info.get(\"ebitdaMargins\", None)\n",
    "        free_cashflow = info.get(\"freeCashflow\", None)\n",
    "        total_debt = info.get(\"totalDebt\", None)\n",
    "        current_ratio = info.get(\"currentRatio\", None)\n",
    "        quick_ratio = info.get(\"quickRatio\", None)\n",
    "\n",
    "        # Valuation and market metrics\n",
    "        enterprise_value = info.get(\"enterpriseValue\", None)\n",
    "        price_to_sales = info.get(\"priceToSalesTrailing12Months\", None)\n",
    "        trailing_pe = info.get(\"trailingPE\", None)\n",
    "        forward_pe = info.get(\"forwardPE\", None)\n",
    "        profit_margins = info.get(\"profitMargins\", None)\n",
    "        return_on_assets = info.get(\"returnOnAssets\", None)\n",
    "\n",
    "        # Stock price and volatility\n",
    "        fifty_two_week_high = info.get(\"fiftyTwoWeekHigh\", None)\n",
    "        fifty_two_week_low = info.get(\"fiftyTwoWeekLow\", None)\n",
    "        beta = info.get(\"beta\", None)\n",
    "\n",
    "        # Analyst sentiment\n",
    "        recommendation_mean = info.get(\"recommendationMean\", None)\n",
    "        recommendation_key = info.get(\"recommendationKey\", None)\n",
    "        target_high_price = info.get(\"targetHighPrice\", None)\n",
    "        target_low_price = info.get(\"targetLowPrice\", None)\n",
    "        target_mean_price = info.get(\"targetMeanPrice\", None)\n",
    "\n",
    "        # Return data as a dictionary\n",
    "        return {\n",
    "            \"tickers\": ticker,\n",
    "            \"business_description\": description,\n",
    "            \"investment\": investment,\n",
    "            \"team_strength\": team_strength,\n",
    "            \"ebit\": ebit,\n",
    "            \"revenue\": revenue,\n",
    "            \"revenue_growth\": revenue_growth,\n",
    "            \"net_income\": net_income,\n",
    "            \"operating_margins\": operating_margins,\n",
    "            \"ebitda_margins\": ebitda_margins,\n",
    "            \"free_cashflow\": free_cashflow,\n",
    "            \"total_debt\": total_debt,\n",
    "            \"current_ratio\": current_ratio,\n",
    "            \"quick_ratio\": quick_ratio,\n",
    "            \"enterprise_value\": enterprise_value,\n",
    "            \"price_to_sales\": price_to_sales,\n",
    "            \"trailing_pe\": trailing_pe,\n",
    "            \"forward_pe\": forward_pe,\n",
    "            \"profit_margins\": profit_margins,\n",
    "            \"return_on_assets\": return_on_assets,\n",
    "            \"fifty_two_week_high\": fifty_two_week_high,\n",
    "            \"fifty_two_week_low\": fifty_two_week_low,\n",
    "            \"beta\": beta,\n",
    "            \"recommendation_mean\": recommendation_mean,\n",
    "            \"recommendation_key\": recommendation_key,\n",
    "            \"target_high_price\": target_high_price,\n",
    "            \"target_low_price\": target_low_price,\n",
    "            \"target_mean_price\": target_mean_price,\n",
    "            **{f\"month_{i+1}_performance\": sp for i, sp in enumerate(monthly_close or [None] * months)}\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_real_dataset(tickers, months):\n",
    "    \"\"\"\n",
    "    Create dataset using a single API call per ticker while retaining existing data.\n",
    "    \"\"\"\n",
    "    # List to collect new fetched data\n",
    "    new_data = []\n",
    "\n",
    "    for _, entry in tickers.iterrows():\n",
    "        ticker = entry['tickers']  # Access 'tickers' column\n",
    "\n",
    "        # Fetch all data for the ticker\n",
    "        ticker_data = fetch_all_data_for_ticker(ticker, months)\n",
    "\n",
    "        if ticker_data is None:\n",
    "            continue\n",
    "\n",
    "        # Append fetched data to the new_data list\n",
    "        new_data.append(ticker_data)\n",
    "\n",
    "    # Convert new_data to a DataFrame\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    if new_data_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Filter the original dataset to only include tickers with fetched data\n",
    "    valid_tickers = new_data_df['tickers']\n",
    "    filtered_tickers = tickers[tickers['tickers'].isin(valid_tickers)]\n",
    "\n",
    "    # Merge the original dataset with the new data\n",
    "    merged_dataset = filtered_tickers.merge(new_data_df, on='tickers', how='inner')\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def process_tickers_in_batches(tickers, batch_size=50, output_file=\"Data/real_company_stock_dataset.csv\", months=72):\n",
    "    # Ensure 'tickers' column exists\n",
    "    if tickers.index.name == 'tickers':\n",
    "        tickers.reset_index(inplace=True)\n",
    "\n",
    "    # Process tickers in batches\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers.iloc[i:i + batch_size].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\"Processing batch {i // batch_size + 1} of {len(tickers) // batch_size + 1}...\")\n",
    "\n",
    "        # Process the batch using the existing method\n",
    "        batch_data = create_real_dataset(batch, months=months)\n",
    "\n",
    "        # Check if batch_data is empty\n",
    "        if batch_data.empty:\n",
    "            # Log all tickers from this batch as skipped\n",
    "            batch['reason'] = 'Missing Data'  # Safe because of .copy()\n",
    "            batch.to_csv(\"skipped_tickers.csv\", mode='a', header=False, index=False)\n",
    "            continue\n",
    "\n",
    "        # Log skipped tickers\n",
    "        skipped_tickers = batch[~batch['tickers'].isin(batch_data['tickers'])].copy()  # Use .copy()\n",
    "        if not skipped_tickers.empty:\n",
    "            skipped_tickers['reason'] = 'Missing Data'  # Safe because of .copy()\n",
    "            skipped_tickers.to_csv(\"skipped_tickers.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "        # Save the batch to the output file\n",
    "        if not batch_data.empty:\n",
    "            if not os.path.exists(output_file):\n",
    "                batch_data.to_csv(output_file, index=False)\n",
    "            else:\n",
    "                batch_data.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Batch {i // batch_size + 1} processed and saved. Skipped Tickers: {len(skipped_tickers)}\")\n",
    "\n",
    "        # Add a delay between batches to respect API rate limits\n",
    "        time.sleep(10)  # Adjust as needed\n"
   ],
   "id": "38faad8c6043dd59",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalization Function",
   "id": "9eb7873025674662"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.483906Z",
     "start_time": "2024-12-06T10:56:18.477179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_full_dataset(input_file=\"Data/real_company_stock_dataset.csv\",\n",
    "                           output_file=\"Data/normalized_real_company_stock_dataset.csv\",\n",
    "                           scalar_directory=\"Data/Scalar/\"):\n",
    "    try:\n",
    "        dataset = pd.read_csv(input_file)\n",
    "\n",
    "        # Identify static and historical columns\n",
    "        excluded_columns = ['tickers', 'business_description']\n",
    "        static_columns = [col for col in dataset.columns if col not in excluded_columns and not col.startswith('month_')]\n",
    "        historical_columns = [col for col in dataset.columns if col.startswith('month_')]\n",
    "\n",
    "        # Create a dictionary to store scalers for each static column\n",
    "        static_scalers = {}\n",
    "\n",
    "        # Ensure the scalar directory exists\n",
    "        os.makedirs(scalar_directory, exist_ok=True)\n",
    "\n",
    "        # Normalize static columns\n",
    "        for col in static_columns:\n",
    "            try:\n",
    "                scaler = StandardScaler()  # or MinMaxScaler()\n",
    "                dataset[col] = scaler.fit_transform(dataset[[col]])\n",
    "                static_scalers[col] = scaler\n",
    "\n",
    "                # Save the scaler for the column\n",
    "                scaler_path = f\"{scalar_directory}/{col}_scaler.pkl\"\n",
    "                joblib.dump(scaler, scaler_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing column '{col}': {e}\")\n",
    "                print(f\"First few problematic entries in column '{col}':\")\n",
    "                print(dataset[[col]].head())  # Display the first few entries in the problematic column\n",
    "                return  # Abort the normalization process\n",
    "\n",
    "        # Normalize historical columns using a single scaler\n",
    "        try:\n",
    "            historical_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            dataset[historical_columns] = historical_scaler.fit_transform(dataset[historical_columns])\n",
    "\n",
    "            # Save the historical scaler\n",
    "            historical_scaler_path = f\"{scalar_directory}/historical_scaler.pkl\"\n",
    "            joblib.dump(historical_scaler, historical_scaler_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing historical columns: {e}\")\n",
    "            print(f\"First few problematic entries in historical columns:\")\n",
    "            print(dataset[historical_columns].head())  # Display the first few rows of the problematic columns\n",
    "            return  # Abort the normalization process\n",
    "\n",
    "        # Save the normalized dataset\n",
    "        dataset.to_csv(output_file, index=False)\n",
    "\n",
    "        print(\"Normalization complete. Static scalers saved for each column. Historical scaler updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during normalization: {e}\")\n"
   ],
   "id": "cb77edd2a9bb30af",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Remove Null Values\n",
    "Just to make sure that there are no null values, we go through the dataset and replace them with -1.\n"
   ],
   "id": "679c9da954a63ec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.532161Z",
     "start_time": "2024-12-06T10:56:18.528402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_null_values(path):\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        created_dataset = pd.read_csv(path)\n",
    "\n",
    "        # Replace all null values with 0\n",
    "        created_dataset = created_dataset.fillna(0)\n",
    "\n",
    "        # Save the updated dataset back to the same file\n",
    "        created_dataset.to_csv(path, index=False)\n",
    "\n",
    "        print(f\"Null values in {path} replaced with 0.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {path}: {e}\")\n",
    "\n",
    "\n",
    "# Call method\n",
    "# remove_null_values(NORMALIZED_OUTPUT_FILE)\n"
   ],
   "id": "3a29389760251d5f",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fixture Method\n",
    "As I made a small mistake for the base dataset here is a method to replace just that part"
   ],
   "id": "f9243cd669673c6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.586849Z",
     "start_time": "2024-12-06T10:56:18.578719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fix_encoded_columns(base_file: str, enriched_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Fix encoding inconsistencies in the enriched dataset by replacing problematic columns.\n",
    "    \"\"\"\n",
    "    # Step 1: Load the base dataset\n",
    "    base_dataset = pd.read_csv(base_file, index_col='tickers')\n",
    "\n",
    "    # Step 2: Load the already enriched dataset\n",
    "    enriched_dataset = pd.read_csv(enriched_file)\n",
    "\n",
    "    # Ensure 'tickers' is present\n",
    "    if 'tickers' not in enriched_dataset.columns:\n",
    "        # If tickers is the index, reset it\n",
    "        if enriched_dataset.index.name == 'tickers':\n",
    "            enriched_dataset.reset_index(inplace=True)\n",
    "        else:\n",
    "            raise KeyError(\"'tickers' column is missing in the enriched dataset.\")\n",
    "\n",
    "    # Step 3: Set 'tickers' as the index for alignment\n",
    "    enriched_dataset.set_index('tickers', inplace=True)\n",
    "\n",
    "    # Filter the base dataset to only include tickers in the enriched dataset\n",
    "    overlapping_tickers = base_dataset.loc[base_dataset.index.intersection(enriched_dataset.index)]\n",
    "\n",
    "    # Step 4: Replace inconsistent columns with properly encoded ones\n",
    "    columns_to_fix = ['Country', 'Sector', 'Industry']\n",
    "    enriched_dataset.update(overlapping_tickers[columns_to_fix])\n",
    "\n",
    "    # Step 5: Save the updated enriched dataset\n",
    "    enriched_dataset.reset_index(inplace=True)\n",
    "    enriched_dataset.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Encoded columns fixed. Updated dataset saved to {output_file}\")\n"
   ],
   "id": "7de056e09c1cfd42",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Label Encoder\n",
    "This method can be used to later on encode additional columns (Used for recommendation_key as of right now)\n"
   ],
   "id": "eeadf20a6f31f1f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.644296Z",
     "start_time": "2024-12-06T10:56:18.634410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def encode_columns(file_path: str, columns_to_encode: list, output_file: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Encodes specified columns in a dataset using LabelEncoder.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the dataset file (CSV).\n",
    "    - columns_to_encode (list): List of column names to encode.\n",
    "    - output_file (str): Optional. Path to save the updated dataset. If None, the dataset won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The dataset with encoded columns.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the columns exist in the dataset\n",
    "    missing_columns = [col for col in columns_to_encode if col not in dataset.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following columns are missing in the dataset: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Apply LabelEncoder to the specified columns\n",
    "    label_encoders = {}  # To store encoders for each column\n",
    "    for col in columns_to_encode:\n",
    "        encoder = LabelEncoder()\n",
    "        dataset[col] = encoder.fit_transform(dataset[col].fillna(\"Unknown\"))\n",
    "        label_encoders[col] = encoder  # Save the encoder in case we need to use it later\n",
    "\n",
    "    # Save the updated dataset if output_file is provided\n",
    "    if not output_file:\n",
    "        print(\"Storing to input path\")\n",
    "        output_file = file_path\n",
    "\n",
    "    dataset.to_csv(output_file, index=False)\n",
    "    print(f\"Updated dataset saved to {output_file}\")\n"
   ],
   "id": "304b297e671f4ab4",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Further Processing - Replacing inf by 99 Quartil\n",
    "the 'price_to_sales' column might have the value 'inf' instead of a number which we need to replace"
   ],
   "id": "ebc487ae89a11122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:18.736165Z",
     "start_time": "2024-12-06T10:56:18.731294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def handle_inf_in_columns(dataset: pd.DataFrame, columns: list, cap_percentile: float = 0.99) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle infinite values in specified columns of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): The dataset containing the columns to process.\n",
    "    - columns (list): List of column names to process.\n",
    "    - cap_percentile (float): The percentile to cap each column at (default is 99th percentile).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated dataset with infinite values replaced.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in dataset.columns:\n",
    "            raise ValueError(f\"'{col}' column not found in the dataset.\")\n",
    "\n",
    "        # Replace inf and -inf with NaN\n",
    "        dataset[col] = dataset[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Compute the 99th percentile cap value\n",
    "        upper_limit = dataset[col].quantile(cap_percentile)\n",
    "\n",
    "        # Replace NaN values (previously inf) with the median\n",
    "        median_value = dataset[col].median()\n",
    "\n",
    "        print(f\"Handling '{col}': Capping at {upper_limit}, replacing NaN with median {median_value}\")\n",
    "\n",
    "        # Cap the values above the upper limit\n",
    "        dataset[col] = np.where(\n",
    "            dataset[col] > upper_limit,\n",
    "            upper_limit,\n",
    "            dataset[col]\n",
    "        )\n",
    "\n",
    "        # Fill remaining NaN values with the median\n",
    "        dataset[col] = dataset[col].fillna(median_value)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n"
   ],
   "id": "d08bdb1e2b7a66f2",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Function\n",
    "Here we execute the above functions"
   ],
   "id": "f2c556a39d3930aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:56:20.610809Z",
     "start_time": "2024-12-06T10:56:18.755084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "INPUT_FILE = \"Ticker_Datasets/nasdaq_screener_1733257202149.csv\"\n",
    "OUTPUT_FILE = \"Data/real_company_stock_dataset_large.csv\"\n",
    "NORMALIZED_OUTPUT_FILE = \"Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "INTERMEDIATE_OUTPUT_FILE = \"Data/initial_dataset.csv\"\n",
    "SCALER_FOLDER = \"Data/Scaler/\"\n",
    "\n",
    "REPLACE_INITIAL_DATASET = False\n",
    "ENRICH = False\n",
    "REMOVE_NULL = False\n",
    "NORMALIZE = True\n",
    "LABEL_ENCODE = False\n",
    "REPLACE_INF = False\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Simulated list of 10,000 tickers\n",
    "    if RECREATE_INITAL_DATASET:\n",
    "        dataset, encoders = get_tickers_dataset(INPUT_FILE)\n",
    "        extended_dataset = extend_dataset(dataset, encoders)\n",
    "        print(dataset.head())\n",
    "\n",
    "        # Fill null values\n",
    "        dataset = dataset.fillna(0)\n",
    "\n",
    "        # Store meanwhile\n",
    "        extended_dataset.to_csv(INTERMEDIATE_OUTPUT_FILE, index=True)\n",
    "\n",
    "    else:\n",
    "        dataset = pd.read_csv(INTERMEDIATE_OUTPUT_FILE, index_col='tickers')\n",
    "\n",
    "    if ENRICH:\n",
    "        process_tickers_in_batches(dataset, output_file=OUTPUT_FILE, batch_size=100, months=72)\n",
    "\n",
    "    if REPLACE_INITIAL_DATASET:\n",
    "        fix_encoded_columns(INTERMEDIATE_OUTPUT_FILE, OUTPUT_FILE, OUTPUT_FILE)\n",
    "\n",
    "    if REMOVE_NULL:\n",
    "        # Normalize data\n",
    "        remove_null_values(OUTPUT_FILE)\n",
    "\n",
    "    if LABEL_ENCODE:\n",
    "        encode_columns(OUTPUT_FILE, ['recommendation_key'], OUTPUT_FILE)\n",
    "\n",
    "    if REPLACE_INF:\n",
    "        adapted_dataset = pd.read_csv(OUTPUT_FILE)\n",
    "        # Columns to handle for infinite values\n",
    "        columns_to_handle = ['price_to_sales', 'trailing_pe', 'forward_pe']\n",
    "\n",
    "        # Handle infinite values in the specified columns\n",
    "        dataset = handle_inf_in_columns(adapted_dataset, columns_to_handle)\n",
    "\n",
    "        adapted_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "\n",
    "    if NORMALIZE:\n",
    "        normalize_full_dataset(input_file=OUTPUT_FILE, output_file=NORMALIZED_OUTPUT_FILE, scalar_directory=SCALER_FOLDER)\n",
    "\n",
    "\n",
    "    print(\"Dataset creation complete!\")\n"
   ],
   "id": "23593be5af917ef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete. Static scalers saved for each column. Historical scaler updated.\n",
      "Dataset creation complete!\n"
     ]
    }
   ],
   "execution_count": 147
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
