{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Attention Optimized Retrieval Augmented Prediction Model\n",
    "\n",
    "This Model, specifically created to make Stock Predictions for upcoming Businesses, means this model predicts the market startup of any new business idea.\n",
    "In attempt to improve the performance of the RetrievalAugmentedPredictionModel (RAP-Model) this model was created.\n",
    "Due to its lacking success-rate, we dropped this approach and continued with the original RAP-Model.\n",
    "\n",
    "**Note**: This is no longer in use, because of insufficient results.\n"
   ],
   "id": "5bfd1cf4c21a9cf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T17:18:44.275461Z",
     "start_time": "2025-01-12T17:18:37.293109Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "from RetrievalSystem.RetrievalSystem import RetrievalSystem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PredictionModel.Layers.AttentionOptimizedLayers import IdeaLayer, IdeaStaticLayer, IdeaHistoricalLayer, OutputLayer, FirstFusionLayer, SecondFusionLayer\n",
    "\n",
    "INPUT_PATH = \"../RetrievalSystem/Embeddings/embeddings.csv\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "BERT_DIM = 384\n",
    "\n",
    "class RetrievalAugmentedPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, ret_sys: RetrievalSystem = None, static_dim = 34, historical_dim = 72, forecast_steps: int = 6, retrieval_number: int = 16):\n",
    "        super(RetrievalAugmentedPredictionModel, self).__init__()\n",
    "\n",
    "        if forecast_steps % 3 != 0:\n",
    "            raise ValueError(\"forecast_steps must be a multiple of 3\")\n",
    "\n",
    "        self.forecast_steps = forecast_steps\n",
    "        self.static_feature_dim = static_dim\n",
    "        self.historical_feature_dim = historical_dim\n",
    "        self.historical_idea_dim = forecast_steps\n",
    "        self.retrieval_number = retrieval_number\n",
    "\n",
    "        # Retrieval Model\n",
    "        if ret_sys:\n",
    "            self.retrieval_system = ret_sys\n",
    "        else:\n",
    "            self.retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number)\n",
    "\n",
    "        # Layers for new Idea\n",
    "        self.idea_fc = IdeaLayer(bert_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "        self.idea_static_fc = IdeaStaticLayer(static_feature_dim=self.static_feature_dim)\n",
    "        self.idea_historical_fc = IdeaHistoricalLayer(historical_idea_dim=self.historical_idea_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "        self.document_fusion_fc = FirstFusionLayer(input_dim=self.historical_feature_dim + self.static_feature_dim + BERT_DIM + 1, hidden_dim=hidden_dim)\n",
    "\n",
    "        self.idea_fusion_fc = SecondFusionLayer(hidden_dim=hidden_dim)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.pre_attention = nn.MultiheadAttention(embed_dim=2 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Multi-layer LSTM with residual connection\n",
    "        self.lstm = nn.LSTM(input_size=2*hidden_dim, hidden_size=2*hidden_dim, num_layers=4, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.post_attention = nn.MultiheadAttention(embed_dim=2 * hidden_dim, num_heads=2, batch_first=True)\n",
    "\n",
    "        # Output layer for forecasting\n",
    "        self.output_fc = OutputLayer(hidden_dim=2*hidden_dim, retrieval_number=self.retrieval_number)\n",
    "\n",
    "\n",
    "    def forward(self, ideas: list=None, retrieval_result=None, dataset: pd.DataFrame = None, static_features=None, historical_data=None, use_auxiliary_inputs=True, excluded_tickers: dict = None):\n",
    "        # Ensure device compatibility\n",
    "\n",
    "        if excluded_tickers is None:\n",
    "            excluded_tickers = {}\n",
    "\n",
    "        if dataset is None:\n",
    "            print(\"We need a dataset for retrieval\")\n",
    "            return None\n",
    "\n",
    "        if not ideas and not retrieval_result:\n",
    "            print(\"We need either an idea text or a retrieval result\")\n",
    "            return None\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # --- Retrieval Model ---\n",
    "        # Batch retrieve embeddings and documents\n",
    "        if not retrieval_result:\n",
    "            retrieval_result = self.retrieval_system.find_similar_entries_for_batch(texts=ideas, top_n=self.retrieval_number, excluded_tickers=excluded_tickers)\n",
    "\n",
    "        # Define static and month columns\n",
    "        static_columns = [\n",
    "            col for col in dataset.columns\n",
    "            if col not in [\"tickers\", \"business_description\", \"embedding\", \"similarity\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "        # Extract embeddings, similarities, and tickers for the batch\n",
    "        idea_embeddings, retrieved_embeddings, combined_data = [], [], []\n",
    "\n",
    "        for embedding, documents in retrieval_result:\n",
    "            idea_embeddings.append(embedding)\n",
    "\n",
    "            # Convert documents to a DataFrame if necessary\n",
    "            if isinstance(documents, list):\n",
    "                documents = pd.DataFrame(documents)\n",
    "\n",
    "            # Ensure `tickers` column has the same type in both DataFrames\n",
    "            documents['tickers'] = documents['tickers'].astype(str)\n",
    "            if dataset.index.name == 'tickers':\n",
    "                dataset = dataset.reset_index()\n",
    "            dataset['tickers'] = dataset['tickers'].astype(str)\n",
    "\n",
    "            # Join dataset on `tickers`\n",
    "            joined_data = documents.join(dataset.set_index('tickers'), on='tickers', how='left')\n",
    "\n",
    "            # Convert embedding and similarity columns to PyTorch tensors\n",
    "            embeddings_tensor = torch.stack(\n",
    "                [torch.tensor(e, dtype=torch.float32) for e in joined_data['embedding']],\n",
    "                dim=0\n",
    "            ).to(device)  # Shape: [num_documents, embedding_dim]\n",
    "\n",
    "            similarities_tensor = torch.stack(\n",
    "                [torch.tensor(s, dtype=torch.float32) for s in joined_data['similarity']],\n",
    "                dim=0\n",
    "            ).to(device)  # Shape: [num_documents, similarity_dim]\n",
    "\n",
    "            # Drop `embedding` and `similarity` columns\n",
    "            joined_data = joined_data.drop(columns=['embedding', 'similarity'])\n",
    "\n",
    "            # Select and process static and month columns\n",
    "            numeric_data = joined_data[static_columns + month_columns].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            numeric_tensor = torch.tensor(numeric_data, dtype=torch.float32).to(device)  # Shape: [num_documents, static_dim + month_dim]\n",
    "\n",
    "            # Concatenate tensors along feature dimension (dim=1)\n",
    "            print(\"Embeddings Tensor Shape:\", embeddings_tensor.shape)\n",
    "            print(\"Similarities Tensor Shape:\", similarities_tensor.shape)\n",
    "            print(\"Numeric Tensor Shape:\", numeric_tensor.shape)\n",
    "\n",
    "            combined_tensor = torch.cat((embeddings_tensor, similarities_tensor.unsqueeze(1), numeric_tensor), dim=1)  # Shape: [num_documents, total_feature_dim]\n",
    "            combined_data.append(combined_tensor)\n",
    "\n",
    "\n",
    "        # Convert to tensors for further processing\n",
    "        combined_tensor = torch.stack(combined_data, dim=0)  # Shape: [batch_size, sequence_length, feature_dim]\n",
    "\n",
    "        combined_output = self.document_fusion_fc(combined_tensor)\n",
    "\n",
    "        print(f\"Shape of combined_tensor: {combined_output.shape}\")\n",
    "\n",
    "        # Put new ideas data into input layers\n",
    "        idea_embeddings = torch.tensor(np.array(idea_embeddings, dtype=np.float32), dtype=torch.float32).to(device).squeeze(1)\n",
    "        idea_output = self.idea_fc(idea_embeddings)\n",
    "\n",
    "        batch_size = idea_embeddings.size(0)\n",
    "        if use_auxiliary_inputs:\n",
    "            static_tensor = static_features.clone().to(torch.float32).to(device)\n",
    "            historical_tensor = historical_data.clone().to(device)\n",
    "        else:\n",
    "            static_tensor = torch.zeros((batch_size, self.static_feature_dim), dtype=torch.float32).to(device)\n",
    "            historical_tensor = torch.zeros((batch_size, self.historical_idea_dim), dtype=torch.float32).to(device)\n",
    "\n",
    "        static_output = self.idea_static_fc(static_tensor) # This wont change within the autoregressiv prediction\n",
    "\n",
    "        # --- Autoregressive prediction ---\n",
    "        predictions = []\n",
    "        pre_attention_weights = []\n",
    "        post_attention_weights = []\n",
    "        lstm_hidden_states = []  # To store the second output (hidden states) of the LSTM\n",
    "\n",
    "        for step in range(self.forecast_steps // 3):  # Predict 3 steps at a time\n",
    "            historical_output = self.idea_historical_fc(historical_tensor)\n",
    "            combined_input = torch.cat((static_output, historical_output, idea_output), dim=1)\n",
    "            idea_tensor = self.idea_fusion_fc(combined_input).unsqueeze(1)\n",
    "\n",
    "            # Pre attention\n",
    "            combined_tensor_with_idea = torch.cat((combined_output, idea_tensor), dim=1)\n",
    "            lstm_attention_output, pre_weights = self.pre_attention(\n",
    "                combined_tensor_with_idea, combined_tensor_with_idea, combined_tensor_with_idea\n",
    "            )\n",
    "            pre_attention_weights.append(pre_weights)  # Store pre-attention weights\n",
    "\n",
    "            # LSTM\n",
    "            lstm_output, (h_n, c_n) = self.lstm(lstm_attention_output)  # Capture LSTM's second output\n",
    "            lstm_hidden_states.append((h_n, c_n))  # Store hidden and cell states\n",
    "\n",
    "            # Post attention\n",
    "            lstm_attention_output, post_weights = self.post_attention(lstm_output, lstm_output, lstm_output)\n",
    "            post_attention_weights.append(post_weights)  # Store post-attention weights\n",
    "\n",
    "            # Aggregate using mean pooling\n",
    "            aggregated_output = torch.mean(lstm_attention_output, dim=1)  # Shape: [batch_size, hidden_dim]\n",
    "\n",
    "            # OUTPUT\n",
    "            final_prediction = self.output_fc(aggregated_output)  # Now returns [batch_size, 3]\n",
    "\n",
    "            # Append to predictions\n",
    "            predictions.append(final_prediction)  # Shape: [batch_size, 3]\n",
    "\n",
    "            # Update historical tensor for next step\n",
    "            historical_tensor = torch.cat((historical_tensor[:, 3:], final_prediction), dim=1)\n",
    "\n",
    "        # Stack predictions into a single tensor\n",
    "        predictions = torch.cat(predictions, dim=1)  # Shape: [batch_size, forecast_steps]\n",
    "\n",
    "        # Convert pre- and post-attention weights to tensors (optional)\n",
    "        pre_attention_weights = torch.stack(pre_attention_weights, dim=0)  # [steps, batch_size, num_heads, seq_len, seq_len]\n",
    "        post_attention_weights = torch.stack(post_attention_weights, dim=0)  # [steps, batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # Return predictions, attention weights, and LSTM hidden states\n",
    "        return predictions, pre_attention_weights, post_attention_weights, lstm_hidden_states\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan-maier/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example usage\n",
    "Here is an example of how to use our newly created model:"
   ],
   "id": "ffb934d9de7941ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T17:18:44.286263Z",
     "start_time": "2025-01-12T17:18:44.278910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Initialize the model - HAVE TO BE ADAPTED TO DATASET (Values are likely correct)\n",
    "def example_usage():\n",
    "    static_feature_dim_num = 4    # Number of static features\n",
    "    historical_dim_num = 12       # Number of historical stock performance points\n",
    "    hidden_dim_num = 128          # Hidden layer size\n",
    "    forecast_steps_num = 12       # Predict next 12 months\n",
    "\n",
    "    batch_size = 2\n",
    "\n",
    "    DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "    dataset = pd.read_csv(DATASET_PATH)\n",
    "    print(f\"Datasetshape: {dataset.shape}\")\n",
    "\n",
    "    print(f\"Datasetshape: {dataset.shape}\")\n",
    "\n",
    "    retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=10)\n",
    "\n",
    "    model = RetrievalAugmentedPredictionModel(\n",
    "        forecast_steps=forecast_steps_num,\n",
    "        ret_sys = retrieval_system,\n",
    "        retrieval_number=10\n",
    "    )\n",
    "\n",
    "    current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Random entry\n",
    "    idea_entries = dataset.iloc[10:10 + batch_size, :]  # Get a batch of rows\n",
    "\n",
    "    # Removed tickers: Select rows from index 3 onward\n",
    "    removed_tickers = [] # dataset.iloc[100:, :][\"tickers\"].tolist()\n",
    "\n",
    "    # Create excluded_tickers map\n",
    "    excluded_tickers = {\n",
    "        i: [ticker] + removed_tickers  # Include the ticker itself and all removed tickers\n",
    "        for i, ticker in enumerate(dataset[\"tickers\"])\n",
    "    }\n",
    "\n",
    "    ideas = idea_entries[\"business_description\"].tolist()\n",
    "\n",
    "    static_columns = [\n",
    "        col for col in dataset.columns\n",
    "        if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "    ]\n",
    "    month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "    # Prepare static and historical data for the batch\n",
    "    static_data = idea_entries[static_columns]\n",
    "    historical_data = idea_entries[month_columns]\n",
    "\n",
    "    # Ensure numeric data and handle missing values\n",
    "    static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "    historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "    # Convert to tensors with batch dimension\n",
    "    static_data = torch.tensor(static_data, dtype=torch.float32).to(current_device)  # [batch_size, static_feature_dim_num]\n",
    "    historical_data = torch.tensor(historical_data[:, -2 * forecast_steps_num:-forecast_steps_num], dtype=torch.float32).to(current_device)  # [batch_size, historical_dim_num]\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction, _, _, _ = model(\n",
    "        ideas=ideas,\n",
    "        dataset=dataset,\n",
    "        static_features=static_data,\n",
    "        historical_data=historical_data,\n",
    "        use_auxiliary_inputs=True,\n",
    "        excluded_tickers=excluded_tickers,\n",
    "    )\n",
    "    print(prediction)  # Co\n",
    "    print(prediction.shape)\n",
    "\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction, _, _, _ = model(\n",
    "        ideas=ideas,\n",
    "        dataset=dataset,\n",
    "        use_auxiliary_inputs=False\n",
    "    )\n",
    "    print(prediction)  # Co\n",
    "    print(prediction.shape)\n",
    "\n",
    "\n",
    "\n",
    "    retrieval_result = retrieval_system.find_similar_entries_for_batch(texts=ideas, top_n=5)\n",
    "    prediction, _, _, _ = model(\n",
    "        dataset=dataset,\n",
    "        retrieval_result=retrieval_result,\n",
    "        use_auxiliary_inputs=False,\n",
    "    )\n",
    "\n",
    "    print(prediction)\n",
    "    print(prediction.shape)\n",
    "\n",
    "\n"
   ],
   "id": "43396d2bee8576ad",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main\n",
    "Here the test functions can be executed\n"
   ],
   "id": "295789f74add45ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T17:19:03.229789Z",
     "start_time": "2025-01-12T17:18:44.413958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ],
   "id": "a832a8a83ecebc55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasetshape: (7089, 108)\n",
      "Datasetshape: (7089, 108)\n",
      "Embeddings Tensor Shape: torch.Size([10, 384])\n",
      "Similarities Tensor Shape: torch.Size([10])\n",
      "Numeric Tensor Shape: torch.Size([10, 106])\n",
      "Embeddings Tensor Shape: torch.Size([10, 384])\n",
      "Similarities Tensor Shape: torch.Size([10])\n",
      "Numeric Tensor Shape: torch.Size([10, 106])\n",
      "Shape of combined_tensor: torch.Size([2, 10, 256])\n",
      "tensor([[0.2587, 0.0723, 0.1557, 0.2561, 0.0711, 0.1549, 0.2553, 0.0735, 0.1561,\n",
      "         0.2588, 0.0727, 0.1551],\n",
      "        [0.2583, 0.0715, 0.1567, 0.2595, 0.0712, 0.1558, 0.2587, 0.0715, 0.1571,\n",
      "         0.2586, 0.0743, 0.1564]], grad_fn=<CatBackward0>)\n",
      "torch.Size([2, 12])\n",
      "Embeddings Tensor Shape: torch.Size([10, 384])\n",
      "Similarities Tensor Shape: torch.Size([10])\n",
      "Numeric Tensor Shape: torch.Size([10, 106])\n",
      "Embeddings Tensor Shape: torch.Size([10, 384])\n",
      "Similarities Tensor Shape: torch.Size([10])\n",
      "Numeric Tensor Shape: torch.Size([10, 106])\n",
      "Shape of combined_tensor: torch.Size([2, 10, 256])\n",
      "tensor([[0.2580, 0.0721, 0.1574, 0.2570, 0.0728, 0.1577, 0.2576, 0.0734, 0.1570,\n",
      "         0.2578, 0.0695, 0.1547],\n",
      "        [0.2581, 0.0723, 0.1554, 0.2578, 0.0740, 0.1569, 0.2609, 0.0719, 0.1569,\n",
      "         0.2587, 0.0711, 0.1558]], grad_fn=<CatBackward0>)\n",
      "torch.Size([2, 12])\n",
      "Embeddings Tensor Shape: torch.Size([5, 384])\n",
      "Similarities Tensor Shape: torch.Size([5])\n",
      "Numeric Tensor Shape: torch.Size([5, 106])\n",
      "Embeddings Tensor Shape: torch.Size([5, 384])\n",
      "Similarities Tensor Shape: torch.Size([5])\n",
      "Numeric Tensor Shape: torch.Size([5, 106])\n",
      "Shape of combined_tensor: torch.Size([2, 5, 256])\n",
      "tensor([[0.2512, 0.0767, 0.1566, 0.2542, 0.0777, 0.1567, 0.2487, 0.0761, 0.1556,\n",
      "         0.2529, 0.0741, 0.1564],\n",
      "        [0.2512, 0.0763, 0.1559, 0.2515, 0.0768, 0.1541, 0.2535, 0.0736, 0.1548,\n",
      "         0.2522, 0.0743, 0.1542]], grad_fn=<CatBackward0>)\n",
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
