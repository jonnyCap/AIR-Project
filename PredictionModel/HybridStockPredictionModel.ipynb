{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hybrid Stock Prediction Model\n",
    "\n",
    "This Model, specifically created to make Stock Predictions for upcoming Businesses, means this model predicts the market startup of any new business idea.\n",
    "\n",
    "### Model Architecture\n",
    "To create the most realistic approach possible, we created a hybrid model consisting of the following layers:\n",
    "1. Encodes business ideas using Sentence-BERT.\n",
    "2. Processes static company features using a dense layer.\n",
    "3. Combines both representations in a fusion layer.\n",
    "4. Uses an LSTM to make sequential predictions across a 12-month period.\n",
    "5. Outputs a prediction for each month in the forecast period.\n"
   ],
   "id": "5bfd1cf4c21a9cf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-09T11:59:50.576137Z",
     "start_time": "2024-12-09T11:59:47.388925Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class StockPerformancePredictionModel(nn.Module):\n",
    "    def __init__(self, static_feature_dim, historical_dim, hidden_dim, forecast_steps, num_lstm_layers=2):\n",
    "        super(StockPerformancePredictionModel, self).__init__()\n",
    "\n",
    "        # Text representation layer (Sentence-BERT)\n",
    "        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        # Freeze Sentence-BERT parameters (optional)\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Static feature layers (deep)\n",
    "        self.static_fc = nn.Sequential(\n",
    "            nn.Linear(static_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Historical stock data layers (deep)\n",
    "        self.historical_fc = nn.Sequential(\n",
    "            nn.Linear(historical_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer to combine text, static, and historical embeddings\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(384 + 2 * hidden_dim, hidden_dim),  # 384 is the fixed text embedding dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Text-only layer for inference\n",
    "        self.text_only_fc = nn.Sequential(\n",
    "            nn.Linear(384, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Multi-layer LSTM with residual connection\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_lstm_layers, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Output layer for forecasting\n",
    "        self.output_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # Output single value per timestep\n",
    "        )\n",
    "\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def forward(self, idea, static_features=None, historical_data=None, use_auxiliary_inputs=True, predict_autoregressively=False):\n",
    "        # Ensure device compatibility\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Text embedding\n",
    "        encoded_output = self.text_encoder.encode(idea, convert_to_numpy=True)\n",
    "        text_embedding = torch.from_numpy(encoded_output).float().to(device)\n",
    "\n",
    "        # Add batch dimension if processing a single input\n",
    "        if text_embedding.dim() == 1:\n",
    "            text_embedding = text_embedding.unsqueeze(0)\n",
    "\n",
    "        if use_auxiliary_inputs:\n",
    "            # Static feature embedding\n",
    "            static_embedding = self.static_fc(static_features.to(device))\n",
    "\n",
    "            # Historical stock data embedding\n",
    "            historical_embedding = self.historical_fc(historical_data.to(device))\n",
    "\n",
    "            # Fusion of text + static + historical embeddings\n",
    "            combined_input = torch.cat((text_embedding, static_embedding, historical_embedding), dim=-1)\n",
    "            combined_input = self.fusion_fc(combined_input)\n",
    "        else:\n",
    "            # Text-only input (for inference)\n",
    "            combined_input = self.text_only_fc(text_embedding)\n",
    "\n",
    "        if not predict_autoregressively:\n",
    "            # Repeat for time-series prediction\n",
    "            lstm_input = combined_input.unsqueeze(1).repeat(1, self.forecast_steps, 1)\n",
    "\n",
    "            # Pass through LSTM\n",
    "            lstm_out, _ = self.lstm(lstm_input)\n",
    "\n",
    "            # Apply attention mechanism\n",
    "            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "            # Output predictions\n",
    "            predictions = self.output_fc(attn_out).squeeze(-1)  # Shape: (batch_size, forecast_steps)\n",
    "            return predictions\n",
    "        else:\n",
    "            # Autoregressive prediction\n",
    "            predictions = []\n",
    "            hidden_state = None\n",
    "            input_step = combined_input.unsqueeze(1)\n",
    "\n",
    "            for _ in range(self.forecast_steps):\n",
    "                lstm_out, hidden_state = self.lstm(input_step, hidden_state)\n",
    "                attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "                current_prediction = self.output_fc(attn_out.squeeze(1))\n",
    "                predictions.append(current_prediction)\n",
    "\n",
    "                # Use text-only for subsequent steps\n",
    "                input_step = self.text_only_fc(text_embedding).unsqueeze(1)\n",
    "\n",
    "            predictions = torch.stack(predictions, dim=1)\n",
    "            return predictions\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example usage\n",
    "Here is an example of how to use our newly created model:"
   ],
   "id": "ffb934d9de7941ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:00:05.215619Z",
     "start_time": "2024-12-09T11:59:59.342282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the model - HAVE TO BE ADAPTED TO DATASET (Values are likely correct)\n",
    "static_feature_dim_num = 4    # Number of static features\n",
    "historical_dim_num = 12       # Number of historical stock performance points\n",
    "hidden_dim_num = 128          # Hidden layer size\n",
    "forecast_steps_num = 12       # Predict next 12 months\n",
    "\n",
    "model = StockPerformancePredictionModel(\n",
    "    static_feature_dim=static_feature_dim_num,\n",
    "    historical_dim=historical_dim_num,\n",
    "    hidden_dim=hidden_dim_num,\n",
    "    forecast_steps=forecast_steps_num\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "idea_text = [\"AI-powered e-commerce platform targeting luxury goods.\"]\n",
    "fake_static_features = torch.tensor([[1e9, 500000, 0.25, 10]])  # Example static features (batch size = 1)\n",
    "fake_historical_data = torch.tensor([[0.05, 0.08, 0.06, -0.02, 0.07, 0.03, -0.01, 0.04, 0.02, 0.01, -0.03, 0.05]])  # Example historical data\n",
    "\n",
    "# Move to the same device as the model\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(current_device)\n",
    "fake_static_features = fake_static_features.to(current_device)\n",
    "fake_historical_data = fake_historical_data.to(current_device)\n"
   ],
   "id": "43396d2bee8576ad",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nDocstring can't be built for model BertModel",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1778\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1777\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1778\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:957\u001B[0m\n\u001B[1;32m    903\u001B[0m BERT_INPUTS_DOCSTRING \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;124m    Args:\u001B[39m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;124m        input_ids (`torch.LongTensor` of shape `(\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m)`):\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    949\u001B[0m \u001B[38;5;124m            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\u001B[39m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    953\u001B[0m \u001B[38;5;129;43m@add_start_docstrings\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe bare Bert Model transformer outputting raw hidden-states without any specific head on top.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m    \u001B[49m\u001B[43mBERT_START_DOCSTRING\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m--> 957\u001B[0m \u001B[38;5;28;43;01mclass\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43;01mBertModel\u001B[39;49;00m\u001B[43m(\u001B[49m\u001B[43mBertPreTrainedModel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;250;43m    \u001B[39;49m\u001B[38;5;124;43;03m\"\"\"\u001B[39;49;00m\n\u001B[1;32m    959\u001B[0m \n\u001B[1;32m    960\u001B[0m \u001B[38;5;124;43;03m    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\u001B[39;49;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    967\u001B[0m \u001B[38;5;124;43;03m    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\u001B[39;49;00m\n\u001B[1;32m    968\u001B[0m \u001B[38;5;124;43;03m    \"\"\"\u001B[39;49;00m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1002\u001B[0m, in \u001B[0;36mBertModel\u001B[0;34m()\u001B[0m\n\u001B[1;32m    999\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mlayer[layer]\u001B[38;5;241m.\u001B[39mattention\u001B[38;5;241m.\u001B[39mprune_heads(heads)\n\u001B[1;32m   1001\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;129;43m@add_start_docstrings_to_model_forward\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mBERT_INPUTS_DOCSTRING\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size, sequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m-> 1002\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;129;43m@add_code_sample_docstrings\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_CHECKPOINT_FOR_DOC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBaseModelOutputWithPoolingAndCrossAttentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_CONFIG_FOR_DOC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43mforward\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1013\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mList\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFloatTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mUnion\u001B[49m\u001B[43m[\u001B[49m\u001B[43mTuple\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBaseModelOutputWithPoolingAndCrossAttentions\u001B[49m\u001B[43m]\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;250;43m    \u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43;03m\"\"\"\u001B[39;49;00m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;124;43;03m    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001B[39;49;00m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;124;43;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001B[39;49;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;124;43;03m        `past_key_values`).\u001B[39;49;00m\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;124;43;03m    \"\"\"\u001B[39;49;00m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/utils/doc.py:1137\u001B[0m, in \u001B[0;36madd_code_sample_docstrings.<locals>.docstring_decorator\u001B[0;34m(fn)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocstring can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be built for model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_class\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1139\u001B[0m code_sample \u001B[38;5;241m=\u001B[39m filter_outputs_from_example(\n\u001B[1;32m   1140\u001B[0m     code_sample, expected_output\u001B[38;5;241m=\u001B[39mexpected_output, expected_loss\u001B[38;5;241m=\u001B[39mexpected_loss\n\u001B[1;32m   1141\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Docstring can't be built for model BertModel",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m hidden_dim_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m128\u001B[39m          \u001B[38;5;66;03m# Hidden layer size\u001B[39;00m\n\u001B[1;32m      7\u001B[0m forecast_steps_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m12\u001B[39m       \u001B[38;5;66;03m# Predict next 12 months\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mStockPerformancePredictionModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstatic_feature_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstatic_feature_dim_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistorical_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistorical_dim_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_dim_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforecast_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforecast_steps_num\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Example input data\u001B[39;00m\n\u001B[1;32m     17\u001B[0m idea_text \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAI-powered e-commerce platform targeting luxury goods.\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "Cell \u001B[0;32mIn[1], line 10\u001B[0m, in \u001B[0;36mStockPerformancePredictionModel.__init__\u001B[0;34m(self, static_feature_dim, historical_dim, hidden_dim, forecast_steps, num_lstm_layers)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28msuper\u001B[39m(StockPerformancePredictionModel, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Text representation layer (Sentence-BERT)\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_encoder \u001B[38;5;241m=\u001B[39m \u001B[43mSentenceTransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mall-MiniLM-L6-v2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Freeze Sentence-BERT parameters (optional)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_encoder\u001B[38;5;241m.\u001B[39mparameters():\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:308\u001B[0m, in \u001B[0;36mSentenceTransformer.__init__\u001B[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001B[0m\n\u001B[1;32m    299\u001B[0m         model_name_or_path \u001B[38;5;241m=\u001B[39m __MODEL_HUB_ORGANIZATION__ \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m model_name_or_path\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_sentence_transformer_model(\n\u001B[1;32m    302\u001B[0m     model_name_or_path,\n\u001B[1;32m    303\u001B[0m     token,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    306\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    307\u001B[0m ):\n\u001B[0;32m--> 308\u001B[0m     modules, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_sbert_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    320\u001B[0m     modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_auto_model(\n\u001B[1;32m    321\u001B[0m         model_name_or_path,\n\u001B[1;32m    322\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    329\u001B[0m         config_kwargs\u001B[38;5;241m=\u001B[39mconfig_kwargs,\n\u001B[1;32m    330\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1728\u001B[0m, in \u001B[0;36mSentenceTransformer._load_sbert_model\u001B[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001B[0m\n\u001B[1;32m   1725\u001B[0m \u001B[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001B[39;00m\n\u001B[1;32m   1726\u001B[0m \u001B[38;5;66;03m# Otherwise we fall back to the load method\u001B[39;00m\n\u001B[1;32m   1727\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1728\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[43mmodule_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1729\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   1730\u001B[0m     module \u001B[38;5;241m=\u001B[39m module_class\u001B[38;5;241m.\u001B[39mload(model_name_or_path)\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:78\u001B[0m, in \u001B[0;36mTransformer.__init__\u001B[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001B[0m\n\u001B[1;32m     75\u001B[0m     config_args \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     77\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_seq_length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_max_length\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m tokenizer_args:\n\u001B[1;32m     81\u001B[0m     tokenizer_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_max_length\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m max_seq_length\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:130\u001B[0m, in \u001B[0;36mTransformer._load_model\u001B[0;34m(self, model_name_or_path, config, cache_dir, backend, **model_args)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_args)\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 130\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_args)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monnx\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    560\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    561\u001B[0m     )\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m--> 563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m \u001B[43m_get_model_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    566\u001B[0m     )\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:388\u001B[0m, in \u001B[0;36m_get_model_class\u001B[0;34m(config, model_mapping)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_model_class\u001B[39m(config, model_mapping):\n\u001B[0;32m--> 388\u001B[0m     supported_models \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(supported_models, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    390\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m supported_models\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:763\u001B[0m, in \u001B[0;36m_LazyAutoMapping.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping:\n\u001B[1;32m    762\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping[model_type]\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_attr_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;66;03m# Maybe there was several model types associated with this config.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m model_types \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config_mapping\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;241m==\u001B[39m key\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:777\u001B[0m, in \u001B[0;36m_LazyAutoMapping._load_attr_from_module\u001B[0;34m(self, model_type, attr)\u001B[0m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n\u001B[1;32m    776\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules[module_name] \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers.models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 777\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgetattribute_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:693\u001B[0m, in \u001B[0;36mgetattribute_from_module\u001B[0;34m(module, attr)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attr, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(getattribute_from_module(module, a) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m attr)\n\u001B[0;32m--> 693\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, attr)\n\u001B[1;32m    695\u001B[0m \u001B[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001B[39;00m\n\u001B[1;32m    696\u001B[0m \u001B[38;5;66;03m# object at the top level.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1764\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1766\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1767\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m   1768\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1780\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1778\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1780\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1781\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1783\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nDocstring can't be built for model BertModel"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After setting up the model we can use it like this:",
   "id": "ff716ca228d3b513"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:40:35.881185Z",
     "start_time": "2024-11-22T16:40:35.831581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Forward pass with simultaneous prediction\n",
    "first_predictions = model(\n",
    "    idea=idea_text,\n",
    "    static_features=fake_static_features,\n",
    "    historical_data=fake_historical_data,\n",
    "    use_auxiliary_inputs=True,\n",
    "    predict_autoregressively=False  # Default mode\n",
    ")\n",
    "\n",
    "print(\"Simultaneous Predictions:\", first_predictions)\n",
    "\n",
    "\n",
    "# Forward pass with autoregressive prediction\n",
    "predictions_autoregressive = model(\n",
    "    idea=idea_text,\n",
    "    static_features=fake_static_features,\n",
    "    historical_data=fake_historical_data,\n",
    "    use_auxiliary_inputs=True,\n",
    "    predict_autoregressively=True  # Autoregressive mode\n",
    ")\n",
    "\n",
    "print(\"Autoregressive Predictions:\", predictions_autoregressive)\n",
    "\n",
    "# Forward pass with text-only input\n",
    "predictions_text_only = model(\n",
    "    idea=idea_text,\n",
    "    use_auxiliary_inputs=False,\n",
    "    predict_autoregressively=False  # Simultaneous mode with text-only\n",
    ")\n",
    "\n",
    "print(\"Text-Only Predictions:\", predictions_text_only)\n"
   ],
   "id": "caf2a15dac0aa916",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simultaneous Predictions: tensor([[-0.2989, -0.3566, -0.3654, -0.3666, -0.3668, -0.3668, -0.3668, -0.3668,\n",
      "         -0.3668, -0.3668, -0.3668, -0.3668]], grad_fn=<SqueezeBackward1>)\n",
      "Autoregressive Predictions: tensor([[[-0.2989],\n",
      "         [-0.1090],\n",
      "         [-0.0842],\n",
      "         [-0.0701],\n",
      "         [-0.0647],\n",
      "         [-0.0626],\n",
      "         [-0.0617],\n",
      "         [-0.0611],\n",
      "         [-0.0607],\n",
      "         [-0.0603],\n",
      "         [-0.0600],\n",
      "         [-0.0598]]], grad_fn=<StackBackward0>)\n",
      "Text-Only Predictions: tensor([[-0.0716, -0.0652, -0.0620, -0.0605, -0.0597, -0.0594, -0.0593, -0.0592,\n",
      "         -0.0592, -0.0592, -0.0593, -0.0593]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Training Loop",
   "id": "67ab46e00ad78620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:40:38.222530Z",
     "start_time": "2024-11-22T16:40:35.897451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example data (replace with your actual dataset)\n",
    "idea_texts_test = [\"AI-powered e-commerce platform\", \"Blockchain for supply chain management\"]\n",
    "static_features_test = torch.tensor([[1e6, 0.2, 10, 50], [5e5, 0.1, 5, 25]])  # Shape: (batch_size, static_feature_dim)\n",
    "historical_data_test = torch.tensor([[0.05, 0.08, 0.07, 0.03, 0.04, 0.06, 0.08, 0.09, 0.07, 0.05, 0.02, 0.01],\n",
    "                                [0.10, 0.09, 0.08, 0.06, 0.07, 0.05, 0.04, 0.03, 0.02, 0.01, 0.00, -0.01]])  # Shape: (batch_size, historical_dim)\n",
    "targets = torch.tensor([[0.06, 0.07, 0.08, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01],\n",
    "                        [0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.00, -0.01, -0.02]])  # Shape: (batch_size, forecast_steps)\n",
    "\n",
    "# Define model parameters\n",
    "static_feature_dim_test = 4\n",
    "historical_dim_test = 12\n",
    "hidden_dim_test = 128\n",
    "forecast_steps_test = 12\n",
    "\n",
    "# Initialize the model\n",
    "model = StockPerformancePredictionModel(\n",
    "    static_feature_dim=static_feature_dim_test,\n",
    "    historical_dim=historical_dim_test,\n",
    "    hidden_dim=hidden_dim_test,\n",
    "    forecast_steps=forecast_steps_test\n",
    ")\n",
    "\n",
    "# Move model and data to the same device\n",
    "test_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(test_device)\n",
    "static_features_test = static_features_test.to(test_device)\n",
    "historical_data_test = historical_data_test.to(test_device)\n",
    "targets = targets.to(test_device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    test_predictions = model(\n",
    "        idea=idea_texts_test,\n",
    "        static_features=static_features_test,\n",
    "        historical_data=historical_data_test,\n",
    "        use_auxiliary_inputs=True\n",
    "    )\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(test_predictions, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ],
   "id": "3d3559d051a584c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0540\n",
      "Epoch [2/10], Loss: 0.0412\n",
      "Epoch [3/10], Loss: 0.0302\n",
      "Epoch [4/10], Loss: 0.0211\n",
      "Epoch [5/10], Loss: 0.0139\n",
      "Epoch [6/10], Loss: 0.0084\n",
      "Epoch [7/10], Loss: 0.0046\n",
      "Epoch [8/10], Loss: 0.0023\n",
      "Epoch [9/10], Loss: 0.0014\n",
      "Epoch [10/10], Loss: 0.0015\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now we can test again:",
   "id": "fa68b7168e2a712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:40:40.711911Z",
     "start_time": "2024-11-22T16:40:40.654898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_predictions = model(\n",
    "    idea=idea_text,\n",
    "    static_features=fake_static_features,\n",
    "    historical_data=fake_historical_data,\n",
    "    use_auxiliary_inputs=True,\n",
    "    predict_autoregressively=False  # Default mode\n",
    ")\n",
    "\n",
    "print(\"Simultaneous Predictions:\", first_predictions)\n",
    "\n",
    "\n",
    "# Forward pass with autoregressive prediction\n",
    "predictions_autoregressive = model(\n",
    "    idea=idea_text,\n",
    "    static_features=fake_static_features,\n",
    "    historical_data=fake_historical_data,\n",
    "    use_auxiliary_inputs=True,\n",
    "    predict_autoregressively=True  # Autoregressive mode\n",
    ")\n",
    "\n",
    "print(\"Autoregressive Predictions:\", predictions_autoregressive)\n",
    "\n",
    "# Forward pass with text-only input\n",
    "predictions_text_only = model(\n",
    "    idea=idea_text,\n",
    "    use_auxiliary_inputs=False,\n",
    "    predict_autoregressively=False  # Simultaneous mode with text-only\n",
    ")\n",
    "\n",
    "print(\"Text-Only Predictions:\", predictions_text_only)"
   ],
   "id": "f483a15d1da1a36f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simultaneous Predictions: tensor([[0.0374, 0.0729, 0.0783, 0.0790, 0.0791, 0.0791, 0.0792, 0.0792, 0.0792,\n",
      "         0.0792, 0.0792, 0.0792]], grad_fn=<SqueezeBackward1>)\n",
      "Autoregressive Predictions: tensor([[[ 0.0374],\n",
      "         [ 0.0018],\n",
      "         [-0.0032],\n",
      "         [ 0.0035],\n",
      "         [ 0.0122],\n",
      "         [ 0.0192],\n",
      "         [ 0.0239],\n",
      "         [ 0.0268],\n",
      "         [ 0.0286],\n",
      "         [ 0.0295],\n",
      "         [ 0.0300],\n",
      "         [ 0.0303]]], grad_fn=<StackBackward0>)\n",
      "Text-Only Predictions: tensor([[0.0368, 0.0338, 0.0320, 0.0310, 0.0305, 0.0303, 0.0303, 0.0303, 0.0303,\n",
      "         0.0303, 0.0304, 0.0304]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
