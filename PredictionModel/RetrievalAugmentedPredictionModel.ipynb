{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Prediction Model\n",
    "\n",
    "This Model, specifically created to make Stock Predictions for upcoming Businesses, means this model predicts the market startup of any new business idea.\n"
   ],
   "id": "5bfd1cf4c21a9cf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-03T17:47:59.112350Z",
     "start_time": "2025-01-03T17:47:56.232494Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from RetrievalSystem.RetrievalSystem import RetrievalSystem\n",
    "from PredictionModel.AttentionModel.AttentionModel import AttentionModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PredictionModel.Layers.Layers import SimilarityLayer, IdeaLayer, IdeaStaticLayer, IdeaHistoricalLayer, StaticFeatureLayer, HistoricalFeatureLayer, FirstFusionLayer, SecondFusionLayer, OutputLayer\n",
    "\n",
    "INPUT_PATH = \"../RetrievalSystem/Embeddings/embeddings.csv\"\n",
    "\n",
    "BERT_DIM = 384\n",
    "\n",
    "class RetrievalAugmentedPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, ret_sys: RetrievalSystem = None, static_dim = 34, historical_dim = 72, forecast_steps: int = 6, retrieval_number: int = 16):\n",
    "        super(RetrievalAugmentedPredictionModel, self).__init__()\n",
    "\n",
    "        self.forecast_steps = forecast_steps\n",
    "        self.static_feature_dim = static_dim\n",
    "        self.historical_feature_dim = historical_dim\n",
    "        self.historical_idea_dim = historical_dim - forecast_steps\n",
    "        self.retrieval_number = retrieval_number\n",
    "\n",
    "        # Retrieval Model\n",
    "        if ret_sys:\n",
    "            self.retrieval_system = ret_sys\n",
    "        else:\n",
    "            self.retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number)\n",
    "\n",
    "        # Layers for retrieved documents\n",
    "        self.attention_model = AttentionModel(input_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "        self.similarity_fc = SimilarityLayer(retrieval_number=retrieval_number)\n",
    "        self.static_fc = StaticFeatureLayer(retrieval_number=retrieval_number,hidden_dim=hidden_dim, static_feature_dim=self.static_feature_dim)\n",
    "        self.historical_fc = HistoricalFeatureLayer(retrieval_number=retrieval_number, hidden_dim=hidden_dim, historical_feature_dim=self.historical_feature_dim)\n",
    "\n",
    "        # Layers for new Idea\n",
    "        self.idea_fc = IdeaLayer(bert_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "        self.idea_static_fc = IdeaStaticLayer(static_feature_dim=self.static_feature_dim)\n",
    "        self.idea_historical_fc = IdeaHistoricalLayer(historical_idea_dim=self.historical_idea_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "        # 1.Fusion Layer\n",
    "        self.first_fusion_fc = FirstFusionLayer(bert_dim=BERT_DIM, hidden_dim=hidden_dim, retrieval_number=retrieval_number)\n",
    "        self.fusion_attention = nn.MultiheadAttention(embed_dim=3 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # 2.Fusion Layer\n",
    "        self.second_fusion_fc = SecondFusionLayer(hidden_dim=hidden_dim)\n",
    "        self.second_fusion_attention = nn.MultiheadAttention(embed_dim=2 * hidden_dim, num_heads=2, batch_first=True)\n",
    "\n",
    "        # Multi-layer LSTM with residual connection\n",
    "        self.lstm = nn.LSTM(2 * hidden_dim, 1 * hidden_dim, num_layers=3, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=1 * hidden_dim, num_heads=2, batch_first=True)\n",
    "\n",
    "        # Output layer for forecasting\n",
    "        self.output_fc = OutputLayer(hidden_dim=hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, ideas: list, dataset: pd.DataFrame = None, static_features=None, historical_data=None, use_auxiliary_inputs=True, excluded_tickers=None):\n",
    "        # Ensure device compatibility\n",
    "        if excluded_tickers is None:\n",
    "            excluded_tickers = []\n",
    "        if dataset is None:\n",
    "            print(\"We need a dataset for retrieval\")\n",
    "            return None\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # --- Retrieval Model ---\n",
    "        # Batch retrieve embeddings and documents\n",
    "        retrieval_results = self.retrieval_system.find_similar_entries_for_batch(texts=ideas, top_n=self.retrieval_number, excluded_tickers=excluded_tickers)\n",
    "\n",
    "        # Extract embeddings, similarities, and tickers for the batch\n",
    "        idea_embeddings, retrieved_embeddings, retrieved_similarities, retrieved_tickers = [], [], [], []\n",
    "        for embedding, documents in retrieval_results:\n",
    "            idea_embeddings.append(embedding)\n",
    "            retrieved_embeddings.append(torch.tensor(documents['embedding'].tolist(), dtype=torch.float32).to(device))\n",
    "            retrieved_similarities.append(torch.tensor(documents['similarity'].tolist(), dtype=torch.float32).to(device))\n",
    "            retrieved_tickers.append(documents['tickers'].values)\n",
    "\n",
    "        # Convert to tensors for further processing\n",
    "        idea_embeddings = torch.tensor(idea_embeddings, dtype=torch.float32).to(device)\n",
    "        idea_embeddings = idea_embeddings.squeeze(1)\n",
    "\n",
    "        retrieved_idea_embeddings = torch.stack(retrieved_embeddings).to(device)  # [batch_size, num_retrieved, embedding_dim]\n",
    "        retrieved_similarities = torch.stack(retrieved_similarities).to(device)  # [batch_size, num_retrieved]\n",
    "\n",
    "        # --- Preparing Inputs for Layer ---\n",
    "\n",
    "        print(\"Retrieved tickers: \", retrieved_tickers)\n",
    "        dataset = dataset.set_index(\"tickers\")\n",
    "\n",
    "        # Filter rows from the dataset and extract numeric data\n",
    "        filtered_data = []\n",
    "        for i in range(len(retrieved_tickers)):\n",
    "            tickers = retrieved_tickers[i]\n",
    "            filtered = dataset[dataset.index.isin(tickers)]\n",
    "\n",
    "            # Select numeric columns only\n",
    "            numeric_data = filtered.select_dtypes(include=['number']).apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            filtered_data.append(numeric_data)\n",
    "\n",
    "        # Convert filtered data into a batch tensor with padding\n",
    "        filtered_data = [torch.tensor(row, dtype=torch.float32) for row in filtered_data]\n",
    "        filtered_data = pad_sequence(filtered_data, batch_first=True).to(device)  # [batch_size, padded_length, numeric_dim]\n",
    "        print(\"We have these retrieved documents: \", filtered_data.shape)\n",
    "\n",
    "        # Define static and month columns\n",
    "        static_columns = [\n",
    "            col for col in dataset.columns\n",
    "            if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "        # Extract static and month vectors for each batch\n",
    "        static_vectors = []\n",
    "        month_vectors = []\n",
    "\n",
    "        for i in range(len(retrieved_tickers)):\n",
    "            tickers = retrieved_tickers[i]\n",
    "            filtered = dataset[dataset.index.isin(tickers)]\n",
    "\n",
    "            # Extract static data\n",
    "            static_data = filtered[static_columns].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            static_vectors.append(static_data.flatten())  # Flatten to handle batch processing\n",
    "\n",
    "            # Extract month data\n",
    "            month_data = filtered[month_columns].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            month_vectors.append(month_data.flatten())  # Flatten to handle batch processing\n",
    "\n",
    "        # Convert to tensors\n",
    "        combined_static_tensor = torch.tensor(static_vectors, dtype=torch.float32).to(device)  # [batch_size, static_dim]\n",
    "        combined_historical_tensor = torch.tensor(month_vectors, dtype=torch.float32).to(device)  # [batch_size, historical_dim]\n",
    "\n",
    "        print(f\"Static Tensor Shape: {combined_static_tensor.shape}, Historical Tensor Shape: {combined_historical_tensor.shape}\")\n",
    "\n",
    "        # --- AttentionModel, IdeaInput, 1.FusionLayer ---\n",
    "        # Put retrieved documents into appropriate input layers\n",
    "        weighted_sum, attention_weights = self.attention_model(retrieved_idea_embeddings)\n",
    "        attention_weights = attention_weights.view(attention_weights.size(0), -1, 1)  # Retain batch size\n",
    "        print(f\"Shape of weighted_sum: {weighted_sum.shape}, attention_weights: {attention_weights.shape}\")\n",
    "\n",
    "        similarity_output = self.similarity_fc(retrieved_similarities)  # [batch_size, feature_dim]\n",
    "        combined_static_output = self.static_fc(combined_static_tensor)  # [batch_size, feature_dim]\n",
    "        combined_historical_output = self.historical_fc(combined_historical_tensor)  # [batch_size, feature_dim]\n",
    "\n",
    "        print(f\"Shape of static_output: {combined_static_output.shape}, similarity: {similarity_output.shape}, historical: {combined_historical_output.shape}\")\n",
    "\n",
    "        # Ensure attention_weights matches the batch size\n",
    "        attention_weights = attention_weights.squeeze(-1)  # Remove the last dimension if not needed\n",
    "        print(f\"Shapes: weighted_sum: {weighted_sum.shape}, attention_weights: {attention_weights.shape}, combined_static_output: {combined_static_output.shape}, combined_historical: {combined_historical_output.shape}, similarity: {similarity_output.shape}\")\n",
    "\n",
    "        # Concatenate along the last dimension\n",
    "        combined_retrieval_input = torch.cat((\n",
    "            weighted_sum, attention_weights, combined_static_output, combined_historical_output, similarity_output\n",
    "        ), dim=-1)  # Concatenation along the last dimension\n",
    "        print(f\"Shape of combined_retrieval_input: {combined_retrieval_input.shape}\")\n",
    "\n",
    "        first_fusion_output = self.first_fusion_fc(combined_retrieval_input)\n",
    "\n",
    "        # Attention layer\n",
    "        first_fusion_attention_output, _ = self.fusion_attention(first_fusion_output, first_fusion_output, first_fusion_output)\n",
    "\n",
    "        # Put new ideas data into input layers\n",
    "        idea_output = self.idea_fc(idea_embeddings)\n",
    "\n",
    "        batch_size = idea_embeddings.size(0)\n",
    "        if use_auxiliary_inputs:\n",
    "            static_tensor = static_features.clone().to(torch.float32).to(device)\n",
    "            historical_tensor = historical_data.clone().to(device)\n",
    "        else:\n",
    "            static_tensor = torch.zeros((batch_size, self.static_feature_dim), dtype=torch.float32).to(device)\n",
    "            historical_tensor = torch.zeros((batch_size, self.historical_idea_dim), dtype=torch.float32).to(device)\n",
    "\n",
    "        static_output = self.idea_static_fc(static_tensor) # This wont change within the autoregressiv prediction\n",
    "\n",
    "        # --- Autoregressive prediction ---\n",
    "        predictions = []\n",
    "        for step in range(self.forecast_steps):\n",
    "            historical_output = self.idea_historical_fc(historical_tensor)\n",
    "\n",
    "            # 2. FUSION LAYER - Fuse combined retrieval documents and new idea together\n",
    "            print(f\"Shapes of static_output: {static_output.shape}, historical_output: {historical_output.shape}, idea: {idea_output.shape}, attention_output: {first_fusion_attention_output.shape}\")\n",
    "\n",
    "            combined_idea_input = torch.cat((first_fusion_attention_output, idea_output, static_output, historical_output), dim=1)\n",
    "            second_fusion_output = self.second_fusion_fc(combined_idea_input)\n",
    "\n",
    "            # Attention layer\n",
    "            second_fusion_attention_output, _ = self.second_fusion_attention(second_fusion_output, second_fusion_output, second_fusion_output)\n",
    "\n",
    "            # LSTM\n",
    "            lstm_output, _ = self.lstm(second_fusion_attention_output.unsqueeze(1))  # Add sequence dimension\n",
    "\n",
    "            # Attention\n",
    "            lstm_attention_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "\n",
    "            # OUTPUT\n",
    "            final_prediction = self.output_fc(lstm_attention_output.squeeze(1))  # Remove sequence dimension\n",
    "\n",
    "            # Append to predictions\n",
    "            predictions.append(final_prediction)\n",
    "\n",
    "            # Update historical tensor for next step\n",
    "            historical_tensor = torch.cat((historical_tensor[:, 1:], final_prediction), dim=1)\n",
    "\n",
    "        # Stack predictions into a single tensor\n",
    "        predictions = torch.cat(predictions, dim=1)  # Shape: [1, forecast_steps, 1]\n",
    "        predictions = predictions.to(torch.float32).to(device)  # Retain computational graph\n",
    "        return predictions\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example usage\n",
    "Here is an example of how to use our newly created model:"
   ],
   "id": "ffb934d9de7941ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T17:48:13.879915Z",
     "start_time": "2025-01-03T17:47:59.117499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Initialize the model - HAVE TO BE ADAPTED TO DATASET (Values are likely correct)\n",
    "static_feature_dim_num = 4    # Number of static features\n",
    "historical_dim_num = 12       # Number of historical stock performance points\n",
    "hidden_dim_num = 128          # Hidden layer size\n",
    "forecast_steps_num = 6       # Predict next 12 months\n",
    "\n",
    "batch_size = 9\n",
    "\n",
    "DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=5)\n",
    "\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps_num,\n",
    "    ret_sys = retrieval_system,\n",
    "    retrieval_number=5\n",
    ")\n",
    "\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random entry\n",
    "idea_entries = dataset.iloc[10:10 + batch_size, :]  # Get a batch of rows\n",
    "\n",
    "# Extract business descriptions (ideas)\n",
    "ideas = idea_entries[\"business_description\"].tolist()\n",
    "\n",
    "static_columns = [\n",
    "    col for col in dataset.columns\n",
    "    if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "]\n",
    "month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "# Prepare static and historical data for the batch\n",
    "static_data = idea_entries[static_columns]\n",
    "historical_data = idea_entries[month_columns]\n",
    "\n",
    "# Ensure numeric data and handle missing values\n",
    "static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "# Convert to tensors with batch dimension\n",
    "static_data = torch.tensor(static_data, dtype=torch.float32).to(current_device)  # [batch_size, static_feature_dim_num]\n",
    "historical_data = torch.tensor(historical_data[:, -len(month_columns):-forecast_steps_num], dtype=torch.float32).to(current_device)  # [batch_size, historical_dim_num]\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    ideas=ideas,\n",
    "    dataset=dataset,\n",
    "    static_features=static_data,\n",
    "    historical_data=historical_data,\n",
    "    use_auxiliary_inputs=True\n",
    ")\n",
    "print(prediction)  # Co\n",
    "print(prediction.shape)\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    ideas=ideas,\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "print(prediction)  # Co\n",
    "print(prediction.shape)\n",
    "\n"
   ],
   "id": "43396d2bee8576ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58582/374857662.py:81: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  idea_embeddings = torch.tensor(idea_embeddings, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved tickers:  [array(['AAPL', 'BBY', 'JAMF', 'NKE', 'ZEPP'], dtype=object), array(['AAT', 'SVC', 'SILA', 'CMCT', 'DHC'], dtype=object), array(['AB', 'LPLA', 'WBIG', 'WBIL', 'WBIF'], dtype=object), array(['ABAT', 'LICY', 'AMLI', 'ELBM', 'NVX'], dtype=object), array(['ABBV', 'REGN', 'ALVO', 'CPIX', 'TNXP'], dtype=object), array(['ABCB', 'ASRV', 'FFBC', 'TBBK', 'IROQ'], dtype=object), array(['ABCL', 'ABP', 'ROIV', 'CLDX', 'SMMT'], dtype=object), array(['ABEO', 'ABP', 'ABVC', 'OTLK', 'UBX'], dtype=object), array(['ABEV', 'BUD', 'CCU', 'TAP', 'CCEP'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([9, 5, 106])\n",
      "Static Tensor Shape: torch.Size([9, 170]), Historical Tensor Shape: torch.Size([9, 360])\n",
      "Shape of weighted_sum: torch.Size([9, 384]), attention_weights: torch.Size([9, 5, 1])\n",
      "Shape of static_output: torch.Size([9, 128]), similarity: torch.Size([9, 5]), historical: torch.Size([9, 256])\n",
      "Shapes: weighted_sum: torch.Size([9, 384]), attention_weights: torch.Size([9, 5]), combined_static_output: torch.Size([9, 128]), combined_historical: torch.Size([9, 256]), similarity: torch.Size([9, 5])\n",
      "Shape of combined_retrieval_input: torch.Size([9, 778])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "tensor([[-0.0095, -0.0105, -0.0097, -0.0071, -0.0094, -0.0091],\n",
      "        [-0.0090, -0.0112, -0.0083, -0.0096, -0.0096, -0.0078],\n",
      "        [-0.0075, -0.0079, -0.0093, -0.0107, -0.0086, -0.0087],\n",
      "        [-0.0082, -0.0087, -0.0104, -0.0083, -0.0079, -0.0082],\n",
      "        [-0.0089, -0.0101, -0.0126, -0.0088, -0.0074, -0.0081],\n",
      "        [-0.0094, -0.0100, -0.0084, -0.0092, -0.0095, -0.0102],\n",
      "        [-0.0102, -0.0080, -0.0096, -0.0084, -0.0089, -0.0090],\n",
      "        [-0.0078, -0.0101, -0.0106, -0.0090, -0.0093, -0.0102],\n",
      "        [-0.0082, -0.0094, -0.0080, -0.0087, -0.0087, -0.0094]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([9, 6])\n",
      "Retrieved tickers:  [array(['AAPL', 'BBY', 'JAMF', 'NKE', 'ZEPP'], dtype=object), array(['AAT', 'SVC', 'SILA', 'CMCT', 'DHC'], dtype=object), array(['AB', 'LPLA', 'WBIG', 'WBIL', 'WBIF'], dtype=object), array(['ABAT', 'LICY', 'AMLI', 'ELBM', 'NVX'], dtype=object), array(['ABBV', 'REGN', 'ALVO', 'CPIX', 'TNXP'], dtype=object), array(['ABCB', 'ASRV', 'FFBC', 'TBBK', 'IROQ'], dtype=object), array(['ABCL', 'ABP', 'ROIV', 'CLDX', 'SMMT'], dtype=object), array(['ABEO', 'ABP', 'ABVC', 'OTLK', 'UBX'], dtype=object), array(['ABEV', 'BUD', 'CCU', 'TAP', 'CCEP'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([9, 5, 106])\n",
      "Static Tensor Shape: torch.Size([9, 170]), Historical Tensor Shape: torch.Size([9, 360])\n",
      "Shape of weighted_sum: torch.Size([9, 384]), attention_weights: torch.Size([9, 5, 1])\n",
      "Shape of static_output: torch.Size([9, 128]), similarity: torch.Size([9, 5]), historical: torch.Size([9, 256])\n",
      "Shapes: weighted_sum: torch.Size([9, 384]), attention_weights: torch.Size([9, 5]), combined_static_output: torch.Size([9, 128]), combined_historical: torch.Size([9, 256]), similarity: torch.Size([9, 5])\n",
      "Shape of combined_retrieval_input: torch.Size([9, 778])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "Shapes of static_output: torch.Size([9, 16]), historical_output: torch.Size([9, 32]), idea: torch.Size([9, 128]), attention_output: torch.Size([9, 384])\n",
      "tensor([[-0.0092, -0.0111, -0.0083, -0.0099, -0.0107, -0.0099],\n",
      "        [-0.0082, -0.0098, -0.0096, -0.0112, -0.0105, -0.0095],\n",
      "        [-0.0101, -0.0083, -0.0094, -0.0089, -0.0091, -0.0106],\n",
      "        [-0.0101, -0.0097, -0.0082, -0.0095, -0.0109, -0.0080],\n",
      "        [-0.0108, -0.0086, -0.0072, -0.0090, -0.0099, -0.0092],\n",
      "        [-0.0103, -0.0073, -0.0085, -0.0089, -0.0097, -0.0111],\n",
      "        [-0.0092, -0.0085, -0.0079, -0.0103, -0.0117, -0.0106],\n",
      "        [-0.0080, -0.0081, -0.0105, -0.0067, -0.0089, -0.0098],\n",
      "        [-0.0092, -0.0086, -0.0080, -0.0102, -0.0090, -0.0093]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([9, 6])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Training Loop",
   "id": "67ab46e00ad78620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T17:49:45.859661Z",
     "start_time": "2025-01-03T17:48:13.969436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, dataset, forecast_steps, static_columns, month_columns):\n",
    "        self.dataset = dataset\n",
    "        self.forecast_steps = forecast_steps\n",
    "        self.static_columns = static_columns\n",
    "        self.month_columns = month_columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idea_entry = self.dataset.iloc[idx]\n",
    "\n",
    "        # Extract idea, static data, historical data, and target\n",
    "        idea = idea_entry[\"business_description\"]\n",
    "        ticker = idea_entry[\"tickers\"]\n",
    "\n",
    "        static_data = idea_entry[self.static_columns]\n",
    "        historical_data = idea_entry[self.month_columns]\n",
    "\n",
    "        # Handle missing values\n",
    "        static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "        historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "        # Split target and input\n",
    "        target = torch.tensor(historical_data[-self.forecast_steps:], dtype=torch.float32)\n",
    "        historical_data = torch.tensor(historical_data[:-self.forecast_steps], dtype=torch.float32)\n",
    "\n",
    "        # Return all relevant data\n",
    "        return idea, static_data, historical_data, target, ticker\n",
    "\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "static_columns = [\n",
    "    col for col in dataset.columns\n",
    "    if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "]\n",
    "month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "forecast_steps = 6\n",
    "batch_size = 10\n",
    "retrieval_number = 10\n",
    "\n",
    "stock_dataset = StockDataset(dataset, forecast_steps, static_columns, month_columns)\n",
    "data_loader = DataLoader(stock_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=10)\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps,\n",
    "    ret_sys=retrieval_system,\n",
    "    retrieval_number=10,\n",
    ")\n",
    "model.to(current_device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Initialize storage for loss values\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        ideas, static_data, historical_data, targets, tickers = batch\n",
    "\n",
    "        static_data = static_data.clone().detach().to(current_device)\n",
    "        historical_data = torch.stack([h.clone().detach() for h in historical_data]).to(current_device)\n",
    "        targets = torch.stack([t.clone().detach() for t in targets]).to(current_device)\n",
    "\n",
    "        # Remove tickers in the current batch from the dataset for retrieval\n",
    "        excluded_tickers = list(tickers)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(\n",
    "            ideas=ideas,\n",
    "            dataset=dataset,\n",
    "            static_features=static_data,\n",
    "            historical_data=historical_data,\n",
    "            use_auxiliary_inputs=True,\n",
    "            excluded_tickers=excluded_tickers\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Store epoch loss\n",
    "    losses.append(total_loss)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] completed. Total Loss: {total_loss:.4f}\")\n",
    "\n"
   ],
   "id": "3d3559d051a584c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved tickers:  [array(['PENN', 'GAMB', 'SGHC', 'GMBL', 'EBET', 'CZR', 'GAME', 'GAN',\n",
      "       'CNTY', 'IGT'], dtype=object), array(['CIG', 'ELPC', 'ELP', 'ENIC', 'ELLO', 'AGR', 'CEPU', 'NEE', 'PAM',\n",
      "       'GEV'], dtype=object), array(['SPH', 'GLP', 'BP', 'SUN', 'BWLP', 'CQP', 'MPLX', 'UGI', 'NGS',\n",
      "       'SHEL'], dtype=object), array(['GNTX', 'CIX', 'IEX', 'GTLS', 'NL', 'BWXT', 'WTS', 'KLXE', 'NVT',\n",
      "       'SXI'], dtype=object), array(['LTRN', 'ANL', 'ORIC', 'PDSB', 'EFTR', 'IDYA', 'VSTM', 'JANX',\n",
      "       'RPTX', 'NUVL'], dtype=object), array(['LCID', 'PEV', 'ZK', 'XPEV', 'LOBO', 'THO', 'MULN', 'LOT', 'EH',\n",
      "       'CJET'], dtype=object), array(['MSFT', 'INTA', 'INGM', 'OTEX', 'WCT', 'ARC', 'IBM', 'TWKS',\n",
      "       'NTAP', 'PENG'], dtype=object), array(['SMAR', 'BILL', 'HSTM', 'BL', 'INTJ', 'CWAN', 'WDAY', 'ZUO',\n",
      "       'FORR', 'MCO'], dtype=object), array(['MA', 'GPN', 'ATLCP', 'ATLC', 'USIO', 'SYF', 'RPAY', 'DFS', 'PMTS',\n",
      "       'BFH'], dtype=object), array(['WD', 'CBRE', 'CIGI', 'VEL', 'FFBC', 'PPBI', 'HOUS', 'RMAX',\n",
      "       'LPLA', 'CBNK'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['BHM', 'PKST', 'LXP', 'SEVN', 'LFT', 'OZ', 'ESS', 'SRG', 'KREF',\n",
      "       'VNO'], dtype=object), array(['BFAC', 'PORT', 'AIMAU', 'AIMBU', 'CCIRU', 'LPBBU', 'IXAQ', 'INTE',\n",
      "       'AQUNU', 'AQU'], dtype=object), array(['LECO', 'USAP', 'KALU', 'MT', 'MTRN', 'ESAB', 'X', 'ATI', 'CRS',\n",
      "       'RYI'], dtype=object), array(['FN', 'OPTX', 'IPGP', 'LITE', 'AAOI', 'LPTH', 'TDY', 'OCC', 'GLW',\n",
      "       'LASE'], dtype=object), array(['GCV', 'GGZ', 'GAB', 'GUT', 'GDV', 'GGT', 'GRX', 'GLU', 'BCV',\n",
      "       'ECF'], dtype=object), array(['UAA', 'COLM', 'DECK', 'NKE', 'HBI', 'DBGI', 'ONON', 'VFC', 'NCI',\n",
      "       'SHOO'], dtype=object), array(['SHLS', 'IESC', 'EFXT', 'BEEM', 'WAB', 'ADSE', 'PWR', 'NVT', 'EME',\n",
      "       'MTRX'], dtype=object), array(['KLC', 'BEDU', 'BTSGU', 'BTSG', 'AVAH', 'LRN', 'BV', 'HURN',\n",
      "       'STAF', 'NAVI'], dtype=object), array(['IART', 'CNMD', 'KIDS', 'NVST', 'SIBN', 'ZBH', 'SYK', 'UFPT',\n",
      "       'NVRO', 'DYNT'], dtype=object), array(['BNR', 'PSNL', 'STRO', 'FEMY', 'TECH', 'HOLX', 'AZN', 'DARE',\n",
      "       'NTRA', 'MYGN'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['APT', 'HLN', 'XYLO', 'AXNX', 'OLPX', 'GNTX', 'KOSS', 'PDEX',\n",
      "       'JFBR', 'KN'], dtype=object), array(['AVGO', 'MEI', 'BDC', 'RFIL', 'MTSI', 'TEL', 'VIAV', 'QCOM',\n",
      "       'PLPC', 'LSCC'], dtype=object), array(['NCSM', 'EPM', 'PDS', 'HES', 'GEOS', 'SM', 'BKV', 'BTE', 'LB',\n",
      "       'TPL'], dtype=object), array(['FFWM', 'FHN', 'CVBF', 'LIEN', 'FRST', 'THFF', 'RF', 'UBSI',\n",
      "       'TFSL', 'PRK'], dtype=object), array(['ADCT', 'ADAG', 'APLM', 'CRIS', 'CTMX', 'CASI', 'ANL', 'CGEN',\n",
      "       'STRO', 'MRUS'], dtype=object), array(['APEI', 'ATGE', 'UTI', 'EDTK', 'STRA', 'PRDO', 'LRN', 'ENSG',\n",
      "       'RCMT', 'NVOS'], dtype=object), array(['LMNR', 'AVO', 'JBSS', 'ABVE', 'BGS', 'WILC', 'FDP', 'GENK',\n",
      "       'LVRO', 'PEP'], dtype=object), array(['RFIL', 'PLPC', 'MEI', 'OCC', 'BELFA', 'BELFB', 'APWC', 'TEL',\n",
      "       'HUBB', 'AAOI'], dtype=object), array(['COTY', 'EL', 'RGS', 'ULTA', 'CTCX', 'OXM', 'GO', 'OLPX', 'HCWC',\n",
      "       'NUS'], dtype=object), array(['HBAN', 'HBANM', 'HBANL', 'EQBK', 'WFC', 'CBSH', 'CUBI', 'OVBC',\n",
      "       'FBMS', 'ASB'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['FLO', 'SBH', 'SENEB', 'SENEA', 'GIFT', 'TR', 'HSY', 'ACI', 'FIVE',\n",
      "       'CHSN'], dtype=object), array(['WLYB', 'RSSS', 'BNED', 'LRN', 'CTSH', 'HURN', 'SCNX', 'NYT',\n",
      "       'SKIL', 'MANH'], dtype=object), array(['FVAL', 'FQAL', 'FDMO', 'IYM', 'IYG', 'JHMM', 'JHML', 'SPYG',\n",
      "       'SLYV', 'SLYG'], dtype=object), array(['IRIX', 'LNSR', 'SGHT', 'POCI', 'STAA', 'EYEN', 'BLCO', 'EYPT',\n",
      "       'OPT', 'VUZI'], dtype=object), array(['ETR', 'AQN', 'TAC', 'SO', 'AEP', 'WEC', 'ALE', 'AVA', 'SUUN', 'D'],\n",
      "      dtype=object), array(['UP', 'SABR', 'GBTG', 'DSGX', 'LYFT', 'ASLE', 'YOU', 'MMYT',\n",
      "       'VSEC', 'LSPD'], dtype=object), array(['CHY', 'CHI', 'CHW', 'CGO', 'CSQ', 'AVK', 'PDI', 'NCV', 'NCZ',\n",
      "       'NBB'], dtype=object), array(['MQY', 'MYN', 'MQT', 'MYI', 'MYD', 'MHN', 'MVF', 'MHD', 'MPA',\n",
      "       'MUC'], dtype=object), array(['HAP', 'REMX', 'ERTH', 'NLR', 'POWA', 'IDX', 'OIH', 'FLTR', 'CRAK',\n",
      "       'PRNT'], dtype=object), array(['SNYR', 'NUS', 'HIMS', 'UL', 'NHTC', 'SKIN', 'CHD', 'PBH', 'JFBR',\n",
      "       'PG'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['SEDG', 'EOSEW', 'EOSE', 'SHLS', 'ENPH', 'ZEO', 'DFLI', 'NVVE',\n",
      "       'CSIQ', 'NEOV'], dtype=object), array(['ECL', 'IEX', 'WTTR', 'NVRI', 'WAT', 'CLH', 'CWAN', 'MMM', 'SOLV',\n",
      "       'MATW'], dtype=object), array(['TEF', 'TEO', 'TLK', 'AMX', 'ORAN', 'TIMB', 'TU', 'SKM', 'TDS',\n",
      "       'VOD'], dtype=object), array(['PWUPU', 'IGTAU', 'IGTA', 'PRLH', 'ALSA', 'LPBBU', 'CMCA', 'NVAC',\n",
      "       'AQUNU', 'AQU'], dtype=object), array(['PGZ', 'GMOM', 'AWP', 'DTRE', 'PW', 'RLTY', 'REET', 'FGB', 'PSR',\n",
      "       'RBLD'], dtype=object), array(['XPER', 'IAS', 'MOBQ', 'SONY', 'GPUS', 'LVO', 'LBRDA', 'LBRDK',\n",
      "       'LBRDP', 'APP'], dtype=object), array(['SLP', 'NOTV', 'COR', 'LAB', 'CRL', 'EVO', 'GILD', 'COEP', 'CTKB',\n",
      "       'XBIO'], dtype=object), array(['AMLI', 'LAC', 'SLI', 'ATLX', 'SGML', 'LAAC', 'PLG', 'SVM', 'TMQ',\n",
      "       'HL'], dtype=object), array(['CRM', 'HHS', 'LSPD', 'RMNI', 'ICLK', 'EGAN', 'PRGS', 'INFY',\n",
      "       'NOW', 'PAR'], dtype=object), array(['SPAI', 'SPWH', 'AOUT', 'AREB', 'CLAR', 'ZEPP', 'VRT', 'MOBBW',\n",
      "       'MOB', 'PERF'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['BSL', 'BGB', 'BGX', 'MCI', 'OBDC', 'BTZ', 'CCAP', 'FCRX', 'TSLX',\n",
      "       'BIT'], dtype=object), array(['AIG', 'AFG', 'CNA', 'RZB', 'RGA', 'ACIC', 'TRV', 'GBLI', 'PRA',\n",
      "       'KNSL'], dtype=object), array(['IVDA', 'VIAV', 'SPAI', 'HUBC', 'MTEK', 'MSI', 'WKEY', 'RVSN',\n",
      "       'SMTC', 'WYY'], dtype=object), array(['BY', 'FFBC', 'IROQ', 'BOH', 'FINW', 'FBNC', 'FNWD', 'EFSCP',\n",
      "       'EFSC', 'USB'], dtype=object), array(['SMP', 'UCTT', 'ST', 'IR', 'IEX', 'GPC', 'HLIO', 'GTLS', 'ITT',\n",
      "       'AIT'], dtype=object), array(['FCT', 'BRW', 'ERC', 'EVF', 'NAD', 'DMA', 'FFA', 'FMY', 'PGZ',\n",
      "       'EFT'], dtype=object), array(['AEM', 'KGC', 'GLDG', 'HMY', 'NGD', 'PAAS', 'CGAU', 'USAS', 'MUX',\n",
      "       'GORO'], dtype=object), array(['BNR', 'URGN', 'PRE', 'BDSX', 'TLX', 'EXAS', 'PSNL', 'LH', 'NVS',\n",
      "       'FLGT'], dtype=object), array(['IHRT', 'TZUP', 'SGA', 'LBRDP', 'LBRDK', 'LBRDA', 'LEE', 'UONE',\n",
      "       'UONEK', 'LVO'], dtype=object), array(['BGC', 'GBLI', 'RS', 'IPG', 'MAX', 'UVE', 'AC', 'WTM', 'GLLI',\n",
      "       'ESGR'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['SDHC', 'MTH', 'MHO', 'HOV', 'LGIH', 'ZG', 'Z', 'TMHC', 'UHG',\n",
      "       'TPH'], dtype=object), array(['FRBA', 'TOWN', 'PFS', 'WTFCP', 'WTFC', 'WTFCM', 'BUSE', 'VLY',\n",
      "       'VLYPO', 'VLYPN'], dtype=object), array(['MLPA', 'EINC', 'FCG', 'MLPX', 'ERTH', 'AMZA', 'TPYP', 'RDIV',\n",
      "       'PBD', 'KNCT'], dtype=object), array(['IESC', 'PWR', 'TDS', 'PAR', 'AIOT', 'VIEW', 'NOW', 'TYL', 'SNCR',\n",
      "       'ARBB'], dtype=object), array(['SNPS', 'DELL', 'MCHP', 'SMCI', 'PENG', 'CDNS', 'PRGS', 'LTRX',\n",
      "       'QCOM', 'QUIK'], dtype=object), array(['NEWTH', 'EGBN', 'FFBC', 'BY', 'OPHC', 'WF', 'FRST', 'CNOBP',\n",
      "       'CNOB', 'FBNC'], dtype=object), array(['HURN', 'EGAN', 'NOW', 'RCMT', 'MNDR', 'TWKS', 'SSNC', 'PINC',\n",
      "       'CNXC', 'AEI'], dtype=object), array(['DBI', 'DBGI', 'LEVI', 'BURL', 'WBA', 'M', 'JWN', 'WEYS', 'TGT',\n",
      "       'GCO'], dtype=object), array(['VYGR', 'PRTA', 'ALEC', 'SILO', 'DNLI', 'NRSN', 'SPRC', 'NERV',\n",
      "       'CPRX', 'ATHE'], dtype=object), array(['TETE', 'DIST', 'CNDA', 'ATEK', 'BUJA', 'TMTC', 'INTE', 'KACL',\n",
      "       'AACT', 'BRAC'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['ADM', 'INGR', 'ALTO', 'BGS', 'ABVE', 'GIS', 'POST', 'SEB', 'BG',\n",
      "       'THS'], dtype=object), array(['ALK', 'SAVE', 'UAL', 'AAL', 'ATSG', 'CPA', 'MESA', 'AIRTP',\n",
      "       'AIRT', 'JBLU'], dtype=object), array(['EWBC', 'WABC', 'EBTC', 'FNWB', 'INBK', 'TBBK', 'WASH', 'FINW',\n",
      "       'WNEB', 'OVLY'], dtype=object), array(['SARO', 'VTOL', 'JTAI', 'BLDE', 'ATRO', 'AIRT', 'AIRTP', 'SKYH',\n",
      "       'EVTL', 'UAL'], dtype=object), array(['ISRG', 'LYRA', 'MGRX', 'IRMD', 'TLIS', 'NUS', 'ENOV', 'XYLO',\n",
      "       'CYTO', 'BIOR'], dtype=object), array(['CBOE', 'MRX', 'TW', 'MKTX', 'IBKR', 'TOP', 'FAF', 'STT', 'XTKG',\n",
      "       'INTR'], dtype=object), array(['BOND', 'PAXS', 'PDI', 'RAVI', 'LDUR', 'WIP', 'VNLA', 'MLN',\n",
      "       'SPTL', 'SMB'], dtype=object), array(['BFK', 'BLE', 'BYM', 'BTA', 'BNY', 'BBN', 'BHV', 'BTT', 'MQY',\n",
      "       'BIT'], dtype=object), array(['QSR', 'ARKR', 'BTBD', 'MCD', 'USFD', 'WEN', 'PFGC', 'YUM',\n",
      "       'FATBB', 'FATBP'], dtype=object), array(['PDSB', 'TPST', 'LYEL', 'ADAP', 'TCRX', 'PPBT', 'SNSE', 'IBRX',\n",
      "       'CUE', 'MRUS'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['FXNC', 'FFBC', 'CIZN', 'FULTP', 'FULT', 'HWC', 'BOH', 'BYFC',\n",
      "       'ACNB', 'EGBN'], dtype=object), array(['FINW', 'FBNC', 'FFBC', 'COF', 'BFIN', 'BY', 'PFBC', 'TBBK',\n",
      "       'FVCB', 'STEL'], dtype=object), array(['AMSF', 'AMRC', 'AMBO', 'FLNC', 'HYLN', 'INVE', 'WDS', 'EMLP',\n",
      "       'AMBI', 'SIDU'], dtype=object), array(['USPH', 'SEM', 'EHC', 'ATI', 'NVOS', 'ENSG', 'OPCH', 'DYNT',\n",
      "       'AMED', 'AMN'], dtype=object), array(['EXR', 'PSA', 'NSA', 'CUBE', 'OLP', 'ELS', 'AHR', 'FVR', 'ONL',\n",
      "       'SITC'], dtype=object), array(['UVXY', 'VIXY', 'ASHS', 'EMGF', 'CWEB', 'SPDN', 'SMLF', 'REET',\n",
      "       'TLTE', 'OEUR'], dtype=object), array(['GPK', 'MATV', 'SW', 'SON', 'MGIH', 'SENEA', 'AMCR', 'PACK', 'MMM',\n",
      "       'IP'], dtype=object), array(['CARA', 'SPRC', 'DMAC', 'ARDX', 'MLYS', 'INZY', 'CYCN', 'AZN',\n",
      "       'RDY', 'REVB'], dtype=object), array(['WTTR', 'CLWT', 'PNR', 'WTS', 'ZWS', 'CECO', 'IEX', 'MWA', 'FSI',\n",
      "       'MG'], dtype=object), array(['SENEA', 'DIT', 'LANC', 'BRID', 'PFGC', 'PTVE', 'THS', 'GO', 'SYY',\n",
      "       'FLO'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['WST', 'LIVN', 'BLFS', 'CTLT', 'SRDX', 'UTHR', 'RVNC', 'ISRG',\n",
      "       'RDY', 'NVNO'], dtype=object), array(['EEM', 'EEMV', 'HEWU', 'EEMS', 'ACWV', 'IEMG', 'HEWJ', 'FILL',\n",
      "       'HEWC', 'SLVP'], dtype=object), array(['BY', 'FRST', 'BFIN', 'PBHC', 'ACNB', 'COLB', 'ASB', 'SPFI', 'PRK',\n",
      "       'TFC'], dtype=object), array(['BGY', 'BDJ', 'BTZ', 'BST', 'CII', 'HYT', 'BIT', 'BGT', 'BLW',\n",
      "       'FFA'], dtype=object), array(['SEDG', 'SMXT', 'CSLR', 'JKS', 'NEOV', 'SUUN', 'MAXN', 'RUN',\n",
      "       'HLGN', 'CSIQ'], dtype=object), array(['FRO', 'HAFN', 'NAT', 'PARR', 'GOGL', 'SFL', 'DHT', 'SDRL', 'NGL',\n",
      "       'DKL'], dtype=object), array(['MWA', 'WTS', 'AOS', 'WMS', 'WTTR', 'CWT', 'BMI', 'VLTO', 'MAS',\n",
      "       'IEX'], dtype=object), array(['BSV', 'BLV', 'IGLB', 'VTEB', 'GVI', 'AGG', 'CORP', 'ILTB', 'IGHG',\n",
      "       'SCHO'], dtype=object), array(['POWW', 'SWBI', 'RGR', 'BYRN', 'OLN', 'AS', 'AREB', 'VTSI', 'SPWH',\n",
      "       'ASYS'], dtype=object), array(['NAT', 'HAFN', 'GOGL', 'PXS', 'SDRL', 'FRO', 'PANL', 'CTRM', 'SFL',\n",
      "       'DHT'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['AIT', 'IEX', 'SXI', 'CR', 'FET', 'INTT', 'LECO', 'MEI', 'PKOH',\n",
      "       'MTRX'], dtype=object), array(['EBAY', 'ELA', 'ICE', 'GLBE', 'WT', 'SHOP', 'FVRR', 'MYSZ', 'LSPD',\n",
      "       'BYON'], dtype=object), array(['NOVV', 'KVAC', 'KVACU', 'FIAC', 'TETE', 'CDAQ', 'RCFA', 'PRLH',\n",
      "       'CMCA', 'FTII'], dtype=object), array(['AMG', 'ALTI', 'LAZ', 'VCTR', 'EQH', 'LPLA', 'FOA', 'ADX', 'NTRSO',\n",
      "       'NTRS'], dtype=object), array(['USRT', 'RDOG', 'FRI', 'DTRE', 'PSR', 'ICF', 'IPOS', 'DIV', 'MORT',\n",
      "       'REET'], dtype=object), array(['BOH', 'COF', 'FFBC', 'USB', 'RF', 'GS', 'HWC', 'WFC', 'TRMK',\n",
      "       'MS'], dtype=object), array(['EQBK', 'FBMS', 'CBSH', 'ASB', 'FIBK', 'SFBS', 'BFST', 'OVBC',\n",
      "       'EFSCP', 'EFSC'], dtype=object), array(['FWONA', 'FWONK', 'GM', 'PCAR', 'F', 'CARS', 'UXIN', 'BWMX', 'KMX',\n",
      "       'VFC'], dtype=object), array(['STRM', 'CCLDP', 'CCLD', 'CCLDO', 'AMWL', 'HURN', 'NXPL', 'EXLS',\n",
      "       'MNDR', 'IQV'], dtype=object), array(['CYBN', 'ATAI', 'PBM', 'CMND', 'VTGN', 'ENVB', 'SILO', 'ATNF',\n",
      "       'PXMD', 'EXAI'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['WBIL', 'WBIG', 'AIVL', 'FM', 'DEEF', 'KLDW', 'FILL', 'RIGS',\n",
      "       'VAMO', 'HDGE'], dtype=object), array(['TWO', 'RITM', 'TRMK', 'EARN', 'IVR', 'NYMTM', 'NYMTZ', 'NYMTN',\n",
      "       'NYMT', 'NYMTI'], dtype=object), array(['BRID', 'YUM', 'DRI', 'YUMC', 'BH', 'USFD', 'JACK', 'RAVE', 'QSR',\n",
      "       'PBPB'], dtype=object), array(['FXL', 'FXU', 'FXG', 'FXD', 'FXN', 'FXO', 'FXH', 'FGD', 'BMVP',\n",
      "       'FFTY'], dtype=object), array(['DON', 'DES', 'DHS', 'DNL', 'DLS', 'DFE', 'DGS', 'EES', 'EZM',\n",
      "       'DTH'], dtype=object), array(['INTC', 'NXU', 'LNW', 'PENG', 'VIAV', 'CIEN', 'CALX', 'NTAP',\n",
      "       'NTWK', 'DOCN'], dtype=object), array(['SDRL', 'PDS', 'NE', 'DTI', 'OIS', 'OII', 'PTEN', 'HES', 'TDW',\n",
      "       'VAL'], dtype=object), array(['IJR', 'IJH', 'IAI', 'IHF', 'ITA', 'MXI', 'IJJ', 'IVE', 'IVW',\n",
      "       'OEF'], dtype=object), array(['MSM', 'MRC', 'AIT', 'DXPE', 'B', 'MMM', 'ZKH', 'EML', 'RYI',\n",
      "       'GTEC'], dtype=object), array(['WKEY', 'MSB', 'MATH', 'BCAN', 'CNET', 'SEMR', 'STCN', 'ALTI',\n",
      "       'SCWX', 'CHKP'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['PRT', 'PR', 'TPL', 'PVL', 'SJT', 'MTR', 'PED', 'BPT', 'STR',\n",
      "       'SBR'], dtype=object), array(['CERT', 'NOTV', 'MLAB', 'CRL', 'CTLT', 'ICLR', 'BDX', 'RNLX',\n",
      "       'UTHR', 'NSTG'], dtype=object), array(['CAMT', 'ASYS', 'MCHP', 'TSM', 'AEHR', 'INTT', 'AME', 'FORM',\n",
      "       'LRCX', 'DIOD'], dtype=object), array(['RBA', 'ARCB', 'CAR', 'JBHT', 'LQDT', 'TSLA', 'SNA', 'AIR', 'PRO',\n",
      "       'KAR'], dtype=object), array(['CYTK', 'BEAM', 'SLN', 'GBIO', 'WVE', 'RNA', 'KRRO', 'UTHR',\n",
      "       'DSGN', 'FULC'], dtype=object), array(['BIGC', 'BYON', 'NEGG', 'NXTT', 'SHOP', 'IBM', 'INGM', 'LSPD',\n",
      "       'SPSC', 'GTAC'], dtype=object), array(['INLX', 'MITK', 'SPCB', 'WKEY', 'OSPN', 'STER', 'GLOB', 'TKC',\n",
      "       'INTA', 'DFIN'], dtype=object), array(['AXR', 'VTR', 'BHM', 'MHO', 'HPP', 'DEI', 'HOUS', 'WPC', 'LEN',\n",
      "       'IVT'], dtype=object), array(['SWKS', 'ATRO', 'ALNT', 'HWM', 'XYLO', 'HOVR', 'RTX', 'IVDA',\n",
      "       'MYNA', 'APTV'], dtype=object), array(['ISRG', 'INSP', 'TMDX', 'ESTA', 'AEYE', 'CYTO', 'IART', 'SOGP',\n",
      "       'VSAC', 'VTAK'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['MDYV', 'SLYV', 'SLYG', 'SPVU', 'RPV', 'SPYG', 'XMVM', 'XRLV',\n",
      "       'XSVM', 'INTF'], dtype=object), array(['IART', 'CNMD', 'ALGN', 'ENOV', 'MDT', 'ATEC', 'XRAY', 'FTV',\n",
      "       'SYK', 'NDSN'], dtype=object), array(['TIP', 'TLT', 'TLH', 'TFLO', 'LTPZ', 'GOVT', 'GBF', 'STPZ', 'IGLB',\n",
      "       'CPI'], dtype=object), array(['GMED', 'BBLG', 'BBLGW', 'IART', 'ATEC', 'MBOT', 'SIBN', 'ESTA',\n",
      "       'TRSG', 'CLGN'], dtype=object), array(['NOAH', 'BSIG', 'AGBA', 'SEIC', 'FBIZ', 'DB', 'ARES', 'TOP', 'FOA',\n",
      "       'RF'], dtype=object), array(['WFC', 'MS', 'UBS', 'FITBO', 'FITBP', 'FITB', 'FITBI', 'CMA',\n",
      "       'CBU', 'BUSE'], dtype=object), array(['CNTX', 'CASI', 'AVBP', 'LTRN', 'LEGN', 'SNDX', 'LPTX', 'CGEM',\n",
      "       'BOLT', 'CKPT'], dtype=object), array(['ZONE', 'RAY', 'IRBT', 'SPB', 'HBB', 'JFBR', 'SEE', 'WHR', 'PMEC',\n",
      "       'EPC'], dtype=object), array(['FOUR', 'MANH', 'SIFY', 'LSPD', 'AIOT', 'TDS', 'SCSC', 'OTEX',\n",
      "       'CHKP', 'PSN'], dtype=object), array(['VNDA', 'ROIV', 'TNXP', 'BHVN', 'UTHR', 'CPIX', 'ABBV', 'CRVS',\n",
      "       'TEVA', 'NNVC'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['JMM', 'NMI', 'JRI', 'NMZ', 'JRS', 'NAD', 'NXP', 'NMS', 'NUV',\n",
      "       'NZF'], dtype=object), array(['TGTX', 'INCY', 'BPMC', 'APTO', 'SNGX', 'RIGL', 'CRIS', 'CCCC',\n",
      "       'VINC', 'PRLD'], dtype=object), array(['VRNA', 'INSM', 'INVA', 'TRVI', 'XAIR', 'PXMD', 'MNOV', 'PULM',\n",
      "       'GPCR', 'UTHR'], dtype=object), array(['AKAM', 'IQ', 'DOYU', 'RTC', 'GLBE', 'AUUD', 'ANGI', 'HUYA',\n",
      "       'BIDU', 'IHRT'], dtype=object), array(['DOYU', 'HRYU', 'CHR', 'IQ', 'VERB', 'HUYA', 'MOMO', 'YY', 'FUBO',\n",
      "       'NXTT'], dtype=object), array(['INKM', 'ULST', 'RLY', 'SURE', 'GCC', 'CYB', 'EDC', 'EBND', 'SPAB',\n",
      "       'FTLS'], dtype=object), array(['AONC', 'CYH', 'ARDT', 'THC', 'ISRG', 'UHS', 'JYNT', 'AMWL', 'CI',\n",
      "       'AMN'], dtype=object), array(['TCBX', 'HWBK', 'FUSB', 'FBMS', 'FCNCP', 'FCNCA', 'FCNCO', 'TCBC',\n",
      "       'TCBS', 'IBOC'], dtype=object), array(['MGIH', 'GPK', 'SLVM', 'KRT', 'SW', 'CLW', 'CMPR', 'EDUC', 'IP',\n",
      "       'ITP'], dtype=object), array(['EYPT', 'DSGN', 'MGTX', 'BEAM', 'WVE', 'GBIO', 'FDMT', 'ADVM',\n",
      "       'VIGL', 'APLS'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Retrieved tickers:  [array(['ERAS', 'SWTX', 'BDTX', 'LPTX', 'LTRN', 'ALLR', 'CLRB', 'RLAY',\n",
      "       'BDRX', 'APTO'], dtype=object), array(['CIEN', 'AVT', 'PDFS', 'VRT', 'UIS', 'SIFY', 'SMTC', 'CSCO',\n",
      "       'CHKP', 'ROP'], dtype=object), array(['HUBB', 'CSWI', 'IESC', 'DXPE', 'MTRX', 'AIT', 'ULS', 'VPG', 'PWR',\n",
      "       'R'], dtype=object), array(['IYW', 'IYG', 'IYF', 'IDU', 'IYK', 'IYM', 'IYJ', 'EQAL', 'IYT',\n",
      "       'IYH'], dtype=object), array(['LRCX', 'ICHR', 'ONTO', 'ZONE', 'ASYS', 'UMC', 'TSM', 'NPO',\n",
      "       'AEIS', 'AEHR'], dtype=object), array(['DXCM', 'TNDM', 'OTRK', 'BTTX', 'AMWL', 'FTLF', 'RSLS', 'MOVE',\n",
      "       'RDY', 'SHLT'], dtype=object), array(['SPGM', 'SPEM', 'GMF', 'SPDW', 'EWX', 'RWX', 'GWX', 'EDIV', 'RWO',\n",
      "       'DWX'], dtype=object), array(['OR', 'KGC', 'GORO', 'GLDG', 'GOLD', 'EGO', 'AG', 'TGB', 'NVA',\n",
      "       'ORLA'], dtype=object), array(['PRK', 'NKSH', 'BOTJ', 'NBHC', 'INDB', 'FFBC', 'VABK', 'FRST',\n",
      "       'FRAF', 'BFIN'], dtype=object), array(['UTHR', 'CAPR', 'DNLI', 'ZVRA', 'PTCT', 'ELVN', 'EWTX', 'OVID',\n",
      "       'TAK', 'BHVN'], dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([10, 10, 106])\n",
      "Static Tensor Shape: torch.Size([10, 340]), Historical Tensor Shape: torch.Size([10, 720])\n",
      "Shape of weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10, 1])\n",
      "Shape of static_output: torch.Size([10, 128]), similarity: torch.Size([10, 10]), historical: torch.Size([10, 256])\n",
      "Shapes: weighted_sum: torch.Size([10, 384]), attention_weights: torch.Size([10, 10]), combined_static_output: torch.Size([10, 128]), combined_historical: torch.Size([10, 256]), similarity: torch.Size([10, 10])\n",
      "Shape of combined_retrieval_input: torch.Size([10, 788])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n",
      "Shapes of static_output: torch.Size([10, 16]), historical_output: torch.Size([10, 32]), idea: torch.Size([10, 128]), attention_output: torch.Size([10, 384])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 87\u001B[0m\n\u001B[1;32m     84\u001B[0m excluded_tickers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(tickers)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43mideas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mideas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstatic_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstatic_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistorical_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistorical_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auxiliary_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexcluded_tickers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexcluded_tickers\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[1;32m     97\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(predictions, targets)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[1], line 70\u001B[0m, in \u001B[0;36mRetrievalAugmentedPredictionModel.forward\u001B[0;34m(self, ideas, dataset, static_features, historical_data, use_auxiliary_inputs, excluded_tickers)\u001B[0m\n\u001B[1;32m     66\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters())\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# --- Retrieval Model ---\u001B[39;00m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Batch retrieve embeddings and documents\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m retrieval_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieval_system\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_similar_entries_for_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mideas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieval_number\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexcluded_tickers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexcluded_tickers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Extract embeddings, similarities, and tickers for the batch\u001B[39;00m\n\u001B[1;32m     73\u001B[0m idea_embeddings, retrieved_embeddings, retrieved_similarities, retrieved_tickers \u001B[38;5;241m=\u001B[39m [], [], [], []\n",
      "File \u001B[0;32m~/Documents/AIR-Project/RetrievalSystem/RetrievalSystem.py:75\u001B[0m, in \u001B[0;36mRetrievalSystem.find_similar_entries_for_batch\u001B[0;34m(self, texts, top_n, excluded_tickers)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Convert embeddings column to lists if necessary\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(copied_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 75\u001B[0m     copied_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mcopied_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43membedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43meval\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m copied_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     78\u001B[0m dataset_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(embeddings, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m<string>:0\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "46300e763218c4cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), losses, marker='o', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Example post-training prediction\n",
    "example_idea = \"I want to create a coffee shop that uses digital cups to analyze what's in your coffee and its impact on you.\"\n",
    "prediction = model(\n",
    "    ideas=[example_idea],\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "print(\"Prediction after training:\", prediction)\n",
    "\n",
    "# Plot Predictions vs. Targets\n",
    "targets_numpy = targets.cpu().detach().numpy()\n",
    "predictions_numpy = predictions.cpu().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(predictions_numpy.shape[0]):  # Loop over batch size\n",
    "    plt.plot(targets_numpy[i], label=f\"Target {i+1}\", linestyle='--')\n",
    "    plt.plot(predictions_numpy[i], label=f\"Prediction {i+1}\")\n",
    "plt.xlabel(\"Forecast Step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Predictions vs. Targets\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Example post-training prediction\n",
    "example_idea = \"I want to create a coffee shop that uses digital cups to analyze what's in your coffee and its impact on you.\"\n",
    "prediction = model(\n",
    "    ideas=[example_idea],\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "print(\"Prediction after training:\", prediction)\n"
   ],
   "id": "123c917b2c48f56f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
