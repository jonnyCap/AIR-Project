{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Prediction Model\n",
    "\n",
    "This Model, specifically created to make Stock Predictions for upcoming Businesses, means this model predicts the market startup of any new business idea.\n"
   ],
   "id": "5bfd1cf4c21a9cf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T07:19:44.686040Z",
     "start_time": "2024-12-14T07:19:44.668487Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "from RetrievalSystem.RetrievalSystem import RetrievalSystem\n",
    "from PredictionModel.AttentionModel.AttentionModel import AttentionModel\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = \"../RetrievalSystem/Embeddings/embeddings.csv\"\n",
    "\n",
    "BERT_DIM = 768\n",
    "\n",
    "class RetrievalAugmentedPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, ret_sys: RetrievalSystem = None, static_dim = 34, historical_dim = 72, forecast_steps: int = 6, retrieval_number: int = 16):\n",
    "        super(RetrievalAugmentedPredictionModel, self).__init__()\n",
    "\n",
    "        self.static_feature_dim = static_dim\n",
    "        self.historical_feature_dim = historical_dim\n",
    "\n",
    "        if ret_sys:\n",
    "            self.retrieval_system = ret_sys\n",
    "        else:\n",
    "            self.retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number)\n",
    "\n",
    "        # 16 * 768 -> 768 + 16\n",
    "        self.attention_model = AttentionModel(input_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "\n",
    "        # 16 -> 32\n",
    "        self.similarity_fc = nn.Sequential(\n",
    "            nn.Linear(retrieval_number, 2 * retrieval_number),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * retrieval_number, 2 * retrieval_number),\n",
    "            nn.LayerNorm(2 * retrieval_number),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Use Same bert model as for original embeddings\n",
    "        # 768 -> 4 * 128 -> 128\n",
    "        self.idea_fc = nn.Sequential(\n",
    "            nn.Linear(BERT_DIM, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Static feature layers (deep)\n",
    "        # 34 * 16 -> 34 * 8 -> 256\n",
    "        self.static_fc = nn.Sequential(\n",
    "            nn.Linear(self.static_feature_dim * retrieval_number, self.static_feature_dim * (retrieval_number // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.static_feature_dim * (retrieval_number // 2), 2 * hidden_dim),\n",
    "            nn.LayerNorm(2 * hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Historical stock data layers (deep)\n",
    "        # 72 * 16-> 72 * 8 -> 72 * 8 -> 512\n",
    "        self.historical_fc = nn.Sequential(\n",
    "            nn.Linear(self.historical_feature_dim * retrieval_number, self.historical_feature_dim * (retrieval_number // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.historical_feature_dim * (retrieval_number // 2), self.historical_feature_dim * (retrieval_number // 2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.historical_feature_dim * (retrieval_number // 2), 4 * hidden_dim),\n",
    "            nn.LayerNorm(4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 34 -> 32\n",
    "        self.idea_static_fc = nn.Linear(self.static_feature_dim, 32)\n",
    "        # 72 -> 64\n",
    "        self.idea_historical_fc = nn.Linear(self.historical_feature_dim, hidden_dim//2)\n",
    "\n",
    "        # First Fustion Layer, combines:\n",
    "        # 1. AttentionModel Output -> 768\n",
    "        # 1.a Attention Scores -> retrievel_numbre (16)\n",
    "        # 2. Combined Static Layer Output -> 256\n",
    "        # 2. Combined Static Layer Output -> 512\n",
    "        # 4. Cosine Simularity Layer -> 32\n",
    "        # combined = 1184 -> 1024 -> 512\n",
    "        self._first_fusion_fc = nn.Sequential(\n",
    "            nn.Linear(BERT_DIM + retrieval_number + 2 * hidden_dim + 4 * hidden_dim + 2 * retrieval_number, 8 * hidden_dim),  # 384 is the fixed text embedding dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8 * hidden_dim,  4 * hidden_dim),\n",
    "            nn.LayerNorm(4 * hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Attention layer after first fusion\n",
    "        self.fusion_attention = nn.MultiheadAttention(embed_dim=4 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Second Fusion Layer, combines:\n",
    "        # 1. Previous Fusion Layer Output: 512\n",
    "        # 2. Idea Embedding: 256 (ouput of idea layer)\n",
    "        # 3. Idea Static: 32\n",
    "        # 4. Idea Historical: 64\n",
    "        # combined = 992 -> 1024\n",
    "        self._second_fusion_fc = nn.Sequential(\n",
    "            nn.Linear(4 * hidden_dim + 2 * hidden_dim + 32 + 64, 8 * hidden_dim),  # 384 is the fixed text embedding dimension\n",
    "            nn.GELU(),\n",
    "            nn.Linear(8 * hidden_dim, 7 * hidden_dim),\n",
    "            nn.LayerNorm(7 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(7 * hidden_dim, 5 * hidden_dim),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(5 * hidden_dim, 4 * hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Second fusion\n",
    "        self.second_fusion_attention = nn.MultiheadAttention(embed_dim=4 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Multi-layer LSTM with residual connection\n",
    "        self.lstm = nn.LSTM(4 * hidden_dim, 2 * hidden_dim, num_layers=10, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=2 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Output layer for forecasting\n",
    "        self.output_fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.Hardswish(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim //2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1), # Final Output\n",
    "        )\n",
    "\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def forward(self, idea, dataset: pd.DataFrame = None, static_features=None, historical_data=None, use_auxiliary_inputs=True):\n",
    "        # Ensure device compatibility\n",
    "        if dataset is None:\n",
    "            print(\"We need a dataset for retrieval\")\n",
    "            return None\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Get Idea embedding and similar documents\n",
    "        idea_embedding, retrieved_documents = self.retrieval_system.find_similar_entries(idea)\n",
    "        idea_embedding = torch.tensor(idea_embedding, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Extract embeddings and tickers from retrieved documents\n",
    "        retrieved_idea_embeddings = retrieved_documents.loc[:, [\"embedding\"]].values\n",
    "        print(\"Combined embeddings shape: \", retrieved_idea_embeddings.shape)\n",
    "        retrieved_idea_embeddings = torch.tensor(retrieved_idea_embeddings.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "        retrieved_similarities = retrieved_documents.loc[:, [\"similarity\"]].values.flatten()\n",
    "        retrieved_similarities = torch.tensor(retrieved_similarities.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "        # Filter rows from the dataset where the index is in retrieved tickers\n",
    "        retrieved_tickers = retrieved_documents.loc[:, [\"tickers\"]].values.flatten()  # Flatten to get a 1D array of tickers\n",
    "        print(\"Retrieved tickers: \", retrieved_tickers)\n",
    "        dataset = dataset.set_index(\"tickers\")\n",
    "        filtered_data = dataset[dataset.index.isin(retrieved_tickers)]\n",
    "        print(\"We have these retrieved documents: \", filtered_data.shape)\n",
    "\n",
    "        # Create a vector with all columns that are not \"ticker\", \"business_description\", or starting with \"month\"\n",
    "        static_columns = [\n",
    "            col for col in filtered_data.columns\n",
    "            if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        static_vector = filtered_data[static_columns].values.flatten()  # Convert to NumPy array\n",
    "\n",
    "        # Create a second vector with all columns starting with \"month\"\n",
    "        month_columns = [col for col in filtered_data.columns if col.startswith(\"month\")]\n",
    "        month_vector = filtered_data[month_columns].values.flatten()  # Convert to NumPy array\n",
    "\n",
    "        print(f\"Shape of static vector: {static_vector.shape}, Shape of month vector: {month_vector.shape}\")\n",
    "\n",
    "        # Convert vectors to tensors and move to the appropriate device\n",
    "        combined_static_tensor = torch.tensor(static_vector, dtype=torch.float32).to(device)\n",
    "        combined_historical_tensor = torch.tensor(month_vector, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Initialize the historical data tensor (to handle shifting)\n",
    "        if historical_data is not None and use_auxiliary_inputs:\n",
    "            historical_tensor = historical_data.clone().to(device)\n",
    "        else:\n",
    "            historical_tensor = torch.zeros((1, self.historical_feature_dim), dtype=torch.float32).to(device)\n",
    "\n",
    "        predictions = []\n",
    "        for step in range(self.forecast_steps):\n",
    "            # Put retrieved documents into appropriate input layers\n",
    "            weighted_sum, attention_weights = self.attention_model(retrieved_idea_embeddings)\n",
    "            attention_weights = attention_weights.view(1, -1)\n",
    "            print(f\"Shape of weighted_sum: {weighted_sum.shape}, attention_weights: {attention_weights.shape}\")\n",
    "\n",
    "            similarity_output = self.similarity_fc(retrieved_similarities).unsqueeze(0)\n",
    "            combined_static_output = self.static_fc(combined_static_tensor).unsqueeze(0)\n",
    "            combined_historical_output = self.historical_fc(combined_historical_tensor).unsqueeze(0)\n",
    "            print(f\"Shape of static_output: {combined_static_output.shape}, similarity: {similarity_output.shape}, historical: {combined_historical_output.shape}\")\n",
    "\n",
    "            # 1. FUSION LAYER - Fuse retrieval layers together\n",
    "            combined_retrieval_input = torch.cat((weighted_sum, attention_weights, combined_static_output, combined_historical_output, similarity_output), dim=1)\n",
    "            first_fusion_output = self._first_fusion_fc(combined_retrieval_input)\n",
    "\n",
    "            # Attention layer\n",
    "            first_fusion_attention_output, _ = self.fusion_attention(first_fusion_output, first_fusion_output, first_fusion_output)\n",
    "\n",
    "            # Put new ideas data into input layers\n",
    "            idea_output = self.idea_fc(idea_embedding)\n",
    "            if use_auxiliary_inputs:\n",
    "                static_output = self.idea_static_fc(static_features.to(device))\n",
    "                historical_output = self.idea_historical_fc(historical_tensor)\n",
    "            else:\n",
    "                static_input = torch.zeros((1, self.static_feature_dim), dtype=torch.float32).to(device)\n",
    "                static_output = self.idea_static_fc(static_input)\n",
    "                historical_input = torch.zeros((1, self.historical_feature_dim), dtype=torch.float32).to(device)\n",
    "                historical_output = self.idea_historical_fc(historical_input)\n",
    "\n",
    "            # 2. FUSION LAYER - Fuse combined retrieval documents and new idea together\n",
    "            print(f\"Shapes of static_output: {static_output.shape}, historical_output: {historical_output.shape}, idea: {idea_output.shape}, attention_output: {first_fusion_attention_output.shape}\")\n",
    "            combined_idea_input = torch.cat((first_fusion_attention_output, idea_output, static_output, historical_output), dim=1)\n",
    "            second_fusion_output = self._second_fusion_fc(combined_idea_input)\n",
    "\n",
    "            # Attention layer\n",
    "            second_fusion_attention_output, _ = self.second_fusion_attention(second_fusion_output, second_fusion_output, second_fusion_output)\n",
    "\n",
    "            # LSTM\n",
    "            lstm_output, _ = self.lstm(second_fusion_attention_output.unsqueeze(1))  # Add sequence dimension\n",
    "\n",
    "            # Attention\n",
    "            lstm_attention_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "\n",
    "            # OUTPUT\n",
    "            final_prediction = self.output_fc(lstm_attention_output.squeeze(1))  # Remove sequence dimension\n",
    "\n",
    "            # Append to predictions\n",
    "            predictions.append(final_prediction)\n",
    "\n",
    "            # Update historical tensor for next step\n",
    "            print(f\"Final prediction: {final_prediction.shape}, historical tensor: {historical_tensor.shape}\")\n",
    "            historical_tensor = torch.cat((historical_tensor[:, 1:], final_prediction), dim=1)\n",
    "            print(f\"Resulting historical tensor shape: {historical_tensor.shape}\")\n",
    "\n",
    "        # Stack predictions into a single tensor\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example usage\n",
    "Here is an example of how to use our newly created model:"
   ],
   "id": "ffb934d9de7941ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T07:19:58.273325Z",
     "start_time": "2024-12-14T07:19:44.727670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Initialize the model - HAVE TO BE ADAPTED TO DATASET (Values are likely correct)\n",
    "static_feature_dim_num = 4    # Number of static features\n",
    "historical_dim_num = 12       # Number of historical stock performance points\n",
    "hidden_dim_num = 128          # Hidden layer size\n",
    "forecast_steps_num = 12       # Predict next 12 months\n",
    "\n",
    "\n",
    "DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=16)\n",
    "\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps_num,\n",
    "    ret_sys = retrieval_system,\n",
    "    retrieval_number=16\n",
    ")\n",
    "\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random entry\n",
    "idea_entry = dataset.iloc[4000, :]  # Get a random row (Series)\n",
    "idea = idea_entry[\"business_description\"]\n",
    "\n",
    "static_columns = [\n",
    "    col for col in dataset.columns\n",
    "    if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "]\n",
    "month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "static_data = idea_entry[static_columns]\n",
    "historical_data = idea_entry[month_columns]\n",
    "\n",
    "# Ensure numeric data and handle missing values\n",
    "static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "# Convert to tensors\n",
    "static_data = torch.tensor(static_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "historical_data = torch.tensor(historical_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    idea=idea,\n",
    "    dataset=dataset,\n",
    "    static_features=static_data,\n",
    "    historical_data=historical_data,\n",
    "    use_auxiliary_inputs=True\n",
    ")\n",
    "print(prediction.detach().cpu().numpy())  # Co\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    idea=idea,\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "print(prediction.detach().cpu().numpy())  # Co\n",
    "\n"
   ],
   "id": "43396d2bee8576ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['OTTR' 'XEL' 'LNT' 'PCG' 'RRX' 'AGX' 'CNP' 'VMI' 'DTE' 'AEE' 'POWL' 'SO'\n",
      " 'CETY' 'FELE' 'PKX' 'NRG']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "[[[0.08797872]\n",
      "  [0.09033579]\n",
      "  [0.08845487]\n",
      "  [0.08862357]\n",
      "  [0.08724919]\n",
      "  [0.08809322]\n",
      "  [0.0871031 ]\n",
      "  [0.0885502 ]\n",
      "  [0.08975917]\n",
      "  [0.08866005]\n",
      "  [0.08752359]\n",
      "  [0.08857803]]]\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['OTTR' 'XEL' 'LNT' 'PCG' 'RRX' 'AGX' 'CNP' 'VMI' 'DTE' 'AEE' 'POWL' 'SO'\n",
      " 'CETY' 'FELE' 'PKX' 'NRG']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 72])\n",
      "Resulting historical tensor shape: torch.Size([1, 72])\n",
      "[[[0.08775145]\n",
      "  [0.08765166]\n",
      "  [0.08915459]\n",
      "  [0.08804768]\n",
      "  [0.09005649]\n",
      "  [0.08958396]\n",
      "  [0.09004724]\n",
      "  [0.08999524]\n",
      "  [0.08721818]\n",
      "  [0.08660123]\n",
      "  [0.088     ]\n",
      "  [0.08878514]]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Training Loop",
   "id": "67ab46e00ad78620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T07:19:58.304073Z",
     "start_time": "2024-12-14T07:19:58.288020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Move model and data to the same device\n",
    "test_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(test_device)\n",
    "targets = targets.to(test_device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    test_predictions = model(\n",
    "        idea=idea_texts_test,\n",
    "        static_features=static_features_test,\n",
    "        historical_data=historical_data_test,\n",
    "        use_auxiliary_inputs=True\n",
    "    )\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(test_predictions, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ],
   "id": "3d3559d051a584c6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StockPerformancePredictionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m forecast_steps_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m12\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Initialize the model\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mStockPerformancePredictionModel\u001B[49m(\n\u001B[1;32m     20\u001B[0m     static_feature_dim\u001B[38;5;241m=\u001B[39mstatic_feature_dim_test,\n\u001B[1;32m     21\u001B[0m     historical_dim\u001B[38;5;241m=\u001B[39mhistorical_dim_test,\n\u001B[1;32m     22\u001B[0m     hidden_dim\u001B[38;5;241m=\u001B[39mhidden_dim_test,\n\u001B[1;32m     23\u001B[0m     forecast_steps\u001B[38;5;241m=\u001B[39mforecast_steps_test\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Move model and data to the same device\u001B[39;00m\n\u001B[1;32m     27\u001B[0m test_device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'StockPerformancePredictionModel' is not defined"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
