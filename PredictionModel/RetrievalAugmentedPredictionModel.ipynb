{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Prediction Model\n",
    "\n",
    "This Model, specifically created to make Stock Predictions for upcoming Businesses, means this model predicts the market startup of any new business idea.\n"
   ],
   "id": "5bfd1cf4c21a9cf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-03T09:45:43.105796Z",
     "start_time": "2025-01-03T09:45:43.082857Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from RetrievalSystem.RetrievalSystem import RetrievalSystem\n",
    "from PredictionModel.AttentionModel.AttentionModel import AttentionModel\n",
    "import pandas as pd\n",
    "from PredictionModel.Layers.Layers import SimilarityLayer, IdeaLayer, StaticFeatureLayer, HistoricalFeatureLayer, FirstFusionLayer, SecondFusionLayer, OutputLayer\n",
    "\n",
    "INPUT_PATH = \"../RetrievalSystem/Embeddings/embeddings.csv\"\n",
    "\n",
    "BERT_DIM = 768\n",
    "\n",
    "class RetrievalAugmentedPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, ret_sys: RetrievalSystem = None, static_dim = 34, historical_dim = 72, forecast_steps: int = 6, retrieval_number: int = 16):\n",
    "        super(RetrievalAugmentedPredictionModel, self).__init__()\n",
    "\n",
    "        self.forecast_steps = forecast_steps\n",
    "        self.static_feature_dim = static_dim\n",
    "        self.historical_feature_dim = historical_dim\n",
    "        self.historical_idea_dim = historical_dim - forecast_steps\n",
    "        self.retrieval_number = retrieval_number\n",
    "\n",
    "        if ret_sys:\n",
    "            self.retrieval_system = ret_sys\n",
    "        else:\n",
    "            self.retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number)\n",
    "\n",
    "        # 16 * 768 -> 768 + 16\n",
    "        self.attention_model = AttentionModel(input_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "        # 16 -> 32\n",
    "        self.similarity_fc = SimilarityLayer(retrieval_number=retrieval_number)\n",
    "        # Use Same bert model as for original embeddings\n",
    "        # 768 -> 4 * 128 -> 128\n",
    "        self.idea_fc = IdeaLayer(bert_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "\n",
    "        # Static feature layers (deep)\n",
    "        # 34 * 16 -> 34 * 8 -> 256\n",
    "        self.static_fc = StaticFeatureLayer(retrieval_number=retrieval_number,hidden_dim=hidden_dim, static_feature_dim=self.static_feature_dim)\n",
    "\n",
    "        # Historical stock data layers (deep)\n",
    "        # 72 * 16-> 72 * 8 -> 72 * 8 -> 512\n",
    "        self.historical_fc = HistoricalFeatureLayer(retrieval_number=retrieval_number, hidden_dim=hidden_dim, historical_feature_dim=self.historical_feature_dim)\n",
    "\n",
    "        # 34 -> 32\n",
    "        self.idea_static_fc = nn.Linear(self.static_feature_dim, 32)\n",
    "        # 72 -> 64\n",
    "        self.idea_historical_fc = nn.Linear(self.historical_idea_dim, hidden_dim//2)\n",
    "\n",
    "        # First Fustion Layer, combines:\n",
    "        # 1. AttentionModel Output -> 768\n",
    "        # 1.a Attention Scores -> retrievel_numbre (16)\n",
    "        # 2. Combined Static Layer Output -> 256\n",
    "        # 2. Combined Static Layer Output -> 512\n",
    "        # 4. Cosine Simularity Layer -> 32\n",
    "        # combined = 1184 -> 1024 -> 512\n",
    "        self.first_fusion_fc = FirstFusionLayer(bert_dim=BERT_DIM, hidden_dim=hidden_dim, retrieval_number=retrieval_number)\n",
    "\n",
    "        # Attention layer after first fusion\n",
    "        self.fusion_attention = nn.MultiheadAttention(embed_dim=4 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Second Fusion Layer, combines:\n",
    "        # 1. Previous Fusion Layer Output: 512\n",
    "        # 2. Idea Embedding: 256 (ouput of idea layer)\n",
    "        # 3. Idea Static: 32\n",
    "        # 4. Idea Historical: 64\n",
    "        # combined = 992 -> 1024\n",
    "        self.second_fusion_fc = SecondFusionLayer(hidden_dim=hidden_dim)\n",
    "\n",
    "        # Second fusion\n",
    "        self.second_fusion_attention = nn.MultiheadAttention(embed_dim=4 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Multi-layer LSTM with residual connection\n",
    "        self.lstm = nn.LSTM(4 * hidden_dim, 2 * hidden_dim, num_layers=10, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=2 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Output layer for forecasting\n",
    "        self.output_fc = OutputLayer(hidden_dim=hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, ideas: list, dataset: pd.DataFrame = None, static_features=None, historical_data=None, use_auxiliary_inputs=True, excluded_tickers=None):\n",
    "        # Ensure device compatibility\n",
    "        if excluded_tickers is None:\n",
    "            excluded_tickers = []\n",
    "        if dataset is None:\n",
    "            print(\"We need a dataset for retrieval\")\n",
    "            return None\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # --- Retrieval Model ---\n",
    "        idea_embeddings = []\n",
    "        retrieved_embeddings = []\n",
    "        retrieved_similarities = []\n",
    "        retrieved_tickers = []\n",
    "        for text in ideas:  # Iterate over the batch\n",
    "            embedding, documents = self.retrieval_system.find_similar_entries(\n",
    "                text=text, top_n=self.retrieval_number, excluded_tickers=excluded_tickers\n",
    "            )\n",
    "            idea_embeddings.append(embedding)\n",
    "            retrieved_embeddings.append(torch.tensor(documents['embedding'].tolist(), dtype=torch.float32))\n",
    "            retrieved_similarities.append(torch.tensor(documents['similarity'].tolist(), dtype=torch.float32))\n",
    "            retrieved_tickers.append(documents['tickers'].values)\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        idea_embeddings = torch.tensor(idea_embeddings, dtype=torch.float32).to(device)  # [batch_size, embedding_dim]\n",
    "        idea_embeddings = idea_embeddings.squeeze(1)\n",
    "\n",
    "        retrieved_idea_embeddings = torch.stack(retrieved_embeddings).to(device)  # [batch_size, num_retrieved, embedding_dim]\n",
    "        retrieved_similarities = torch.stack(retrieved_similarities).to(device)  # [batch_size, num_retrieved]\n",
    "\n",
    "        # --- Preparing Inputs for Layer ---\n",
    "\n",
    "        print(\"Retrieved tickers: \", retrieved_tickers)\n",
    "        dataset = dataset.set_index(\"tickers\")\n",
    "\n",
    "        # Filter rows from the dataset and extract numeric data\n",
    "        filtered_data = []\n",
    "        for i in range(len(retrieved_tickers)):\n",
    "            tickers = retrieved_tickers[i]\n",
    "            filtered = dataset[dataset.index.isin(tickers)]\n",
    "\n",
    "            # Select numeric columns only\n",
    "            numeric_data = filtered.select_dtypes(include=['number']).apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            filtered_data.append(numeric_data)\n",
    "\n",
    "        # Convert filtered data into a batch tensor with padding\n",
    "        filtered_data = [torch.tensor(row, dtype=torch.float32) for row in filtered_data]\n",
    "        filtered_data = pad_sequence(filtered_data, batch_first=True).to(device)  # [batch_size, padded_length, numeric_dim]\n",
    "        print(\"We have these retrieved documents: \", filtered_data.shape)\n",
    "\n",
    "        # Define static and month columns\n",
    "        static_columns = [\n",
    "            col for col in dataset.columns\n",
    "            if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "        # Extract static and month vectors for each batch\n",
    "        static_vectors = []\n",
    "        month_vectors = []\n",
    "\n",
    "        for i in range(len(retrieved_tickers)):\n",
    "            tickers = retrieved_tickers[i]\n",
    "            filtered = dataset[dataset.index.isin(tickers)]\n",
    "\n",
    "            # Extract static data\n",
    "            static_data = filtered[static_columns].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            static_vectors.append(static_data.flatten())  # Flatten to handle batch processing\n",
    "\n",
    "            # Extract month data\n",
    "            month_data = filtered[month_columns].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "            month_vectors.append(month_data.flatten())  # Flatten to handle batch processing\n",
    "\n",
    "        # Convert to tensors\n",
    "        combined_static_tensor = torch.tensor(static_vectors, dtype=torch.float32).to(device)  # [batch_size, static_dim]\n",
    "        combined_historical_tensor = torch.tensor(month_vectors, dtype=torch.float32).to(device)  # [batch_size, historical_dim]\n",
    "\n",
    "        print(f\"Static Tensor Shape: {combined_static_tensor.shape}, Historical Tensor Shape: {combined_historical_tensor.shape}\")\n",
    "\n",
    "        # --- AttentionModel, IdeaInput, 1.FusionLayer ---\n",
    "        # Put retrieved documents into appropriate input layers\n",
    "        weighted_sum, attention_weights = self.attention_model(retrieved_idea_embeddings)\n",
    "        attention_weights = attention_weights.view(attention_weights.size(0), -1, 1)  # Retain batch size\n",
    "        print(f\"Shape of weighted_sum: {weighted_sum.shape}, attention_weights: {attention_weights.shape}\")\n",
    "\n",
    "        similarity_output = self.similarity_fc(retrieved_similarities)  # [batch_size, feature_dim]\n",
    "        combined_static_output = self.static_fc(combined_static_tensor)  # [batch_size, feature_dim]\n",
    "        combined_historical_output = self.historical_fc(combined_historical_tensor)  # [batch_size, feature_dim]\n",
    "\n",
    "        print(f\"Shape of static_output: {combined_static_output.shape}, similarity: {similarity_output.shape}, historical: {combined_historical_output.shape}\")\n",
    "\n",
    "        # Ensure attention_weights matches the batch size\n",
    "        attention_weights = attention_weights.squeeze(-1)  # Remove the last dimension if not needed\n",
    "        print(f\"Shapes: weighted_sum: {weighted_sum.shape}, attention_weights: {attention_weights.shape}, combined_static_output: {combined_static_output.shape}, combined_historical: {combined_historical_output.shape}, similarity: {similarity_output.shape}\")\n",
    "\n",
    "        # Concatenate along the last dimension\n",
    "        combined_retrieval_input = torch.cat((\n",
    "            weighted_sum, attention_weights, combined_static_output, combined_historical_output, similarity_output\n",
    "        ), dim=-1)  # Concatenation along the last dimension\n",
    "        print(f\"Shape of combined_retrieval_input: {combined_retrieval_input.shape}\")\n",
    "\n",
    "        first_fusion_output = self.first_fusion_fc(combined_retrieval_input)\n",
    "\n",
    "        # Attention layer\n",
    "        first_fusion_attention_output, _ = self.fusion_attention(first_fusion_output, first_fusion_output, first_fusion_output)\n",
    "\n",
    "        # Put new ideas data into input layers\n",
    "        idea_output = self.idea_fc(idea_embeddings)\n",
    "\n",
    "        batch_size = idea_embeddings.size(0)\n",
    "        if use_auxiliary_inputs:\n",
    "            print(\"Using auxiliary inputs\")\n",
    "            static_tensor = static_features.clone().to(device)\n",
    "            historical_tensor = historical_data.clone().to(device)\n",
    "        else:\n",
    "            print(\"Not using auxiliary inputs\")\n",
    "            static_tensor = torch.zeros((batch_size, self.static_feature_dim), dtype=torch.float32).to(device)\n",
    "            historical_tensor = torch.zeros((batch_size, self.historical_idea_dim), dtype=torch.float32).to(device)\n",
    "\n",
    "        static_output = self.idea_static_fc(static_tensor) # This wont change within the autoregressiv prediction\n",
    "\n",
    "        # --- Autoregressive prediction ---\n",
    "        predictions = []\n",
    "        for step in range(self.forecast_steps):\n",
    "            historical_output = self.idea_historical_fc(historical_tensor)\n",
    "\n",
    "            # 2. FUSION LAYER - Fuse combined retrieval documents and new idea together\n",
    "            print(f\"Shapes of static_output: {static_output.shape}, historical_output: {historical_output.shape}, idea: {idea_output.shape}, attention_output: {first_fusion_attention_output.shape}\")\n",
    "\n",
    "            combined_idea_input = torch.cat((first_fusion_attention_output, idea_output, static_output, historical_output), dim=1)\n",
    "            second_fusion_output = self.second_fusion_fc(combined_idea_input)\n",
    "\n",
    "            # Attention layer\n",
    "            second_fusion_attention_output, _ = self.second_fusion_attention(second_fusion_output, second_fusion_output, second_fusion_output)\n",
    "\n",
    "            # LSTM\n",
    "            lstm_output, _ = self.lstm(second_fusion_attention_output.unsqueeze(1))  # Add sequence dimension\n",
    "\n",
    "            # Attention\n",
    "            lstm_attention_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "\n",
    "            # OUTPUT\n",
    "            final_prediction = self.output_fc(lstm_attention_output.squeeze(1))  # Remove sequence dimension\n",
    "            print(f\"Final prediction: {final_prediction} and shape: {final_prediction.shape}\")\n",
    "\n",
    "            # Append to predictions\n",
    "            predictions.append(final_prediction)\n",
    "\n",
    "            # Update historical tensor for next step\n",
    "            print(f\"Final prediction: {final_prediction.shape}, historical tensor: {historical_tensor.shape}\")\n",
    "            historical_tensor = torch.cat((historical_tensor[:, 1:], final_prediction), dim=1)\n",
    "            print(f\"Resulting historical tensor shape: {historical_tensor.shape}\")\n",
    "\n",
    "        # Stack predictions into a single tensor\n",
    "        print(f\"Prediction before doing operations: {predictions}\")\n",
    "        predictions = torch.cat(predictions, dim=1)  # Shape: [1, forecast_steps, 1]\n",
    "        predictions = predictions.squeeze().detach().cpu().numpy()  # Remove the last dimension, Shape: [1, forecast_steps]\n",
    "        return predictions\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example usage\n",
    "Here is an example of how to use our newly created model:"
   ],
   "id": "ffb934d9de7941ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:48:25.004452Z",
     "start_time": "2025-01-03T09:45:43.129409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Initialize the model - HAVE TO BE ADAPTED TO DATASET (Values are likely correct)\n",
    "static_feature_dim_num = 4    # Number of static features\n",
    "historical_dim_num = 12       # Number of historical stock performance points\n",
    "hidden_dim_num = 128          # Hidden layer size\n",
    "forecast_steps_num = 12       # Predict next 12 months\n",
    "\n",
    "batch_size = 9\n",
    "\n",
    "DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=16)\n",
    "\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps_num,\n",
    "    ret_sys = retrieval_system,\n",
    "    retrieval_number=16\n",
    ")\n",
    "\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random entry\n",
    "idea_entries = dataset.iloc[4000:4000 + batch_size, :]  # Get a batch of rows\n",
    "\n",
    "# Extract business descriptions (ideas)\n",
    "ideas = idea_entries[\"business_description\"].tolist()\n",
    "\n",
    "static_columns = [\n",
    "    col for col in dataset.columns\n",
    "    if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "]\n",
    "month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "# Prepare static and historical data for the batch\n",
    "static_data = idea_entries[static_columns]\n",
    "historical_data = idea_entries[month_columns]\n",
    "\n",
    "# Ensure numeric data and handle missing values\n",
    "static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "# Convert to tensors with batch dimension\n",
    "static_data = torch.tensor(static_data, dtype=torch.float32).to(current_device)  # [batch_size, static_feature_dim_num]\n",
    "historical_data = torch.tensor(historical_data[:, -len(month_columns):-forecast_steps_num], dtype=torch.float32).to(current_device)  # [batch_size, historical_dim_num]\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    ideas=ideas,\n",
    "    dataset=dataset,\n",
    "    static_features=static_data,\n",
    "    historical_data=historical_data,\n",
    "    use_auxiliary_inputs=True\n",
    ")\n",
    "print(prediction)  # Co\n",
    "print(prediction.shape)\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    ideas=ideas,\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "print(prediction)  # Co\n",
    "print(prediction.shape)\n",
    "\n"
   ],
   "id": "43396d2bee8576ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved tickers:  [array(['OTTR', 'XEL', 'LNT', 'PCG', 'RRX', 'AGX', 'CNP', 'VMI', 'DTE',\n",
      "       'AEE', 'POWL', 'SO', 'CETY', 'FELE', 'PKX', 'NRG'], dtype=object), array(['OUST', 'ROK', 'INVZ', 'LTRX', 'AEVA', 'STM', 'INDI', 'ADI', 'AVY',\n",
      "       'BSY', 'AMBA', 'APTV', 'ALGM', 'VC', 'KARO', 'MVIS'], dtype=object), array(['OUT', 'CCO', 'KD', 'APPS', 'EXTR', 'EH', 'ILLR', 'BNZI', 'ICLK',\n",
      "       'APP', 'SPHR', 'IPG', 'OB', 'DAVA', 'ITI', 'TTGT'], dtype=object), array(['OVBC', 'CARE', 'PFS', 'FULT', 'FULTP', 'UBSI', 'RRBI', 'FBMS',\n",
      "       'SHBI', 'TOWN', 'BOTJ', 'VABK', 'PNC', 'BRBS', 'BPRN', 'FBNC'],\n",
      "      dtype=object), array(['OVID', 'PRAX', 'PTCT', 'APLT', 'MRNS', 'BMRN', 'ANNX', 'XFOR',\n",
      "       'NBIX', 'CMRX', 'TGTX', 'APLS', 'MIRM', 'SEEL', 'PHAR', 'IONS'],\n",
      "      dtype=object), array(['OVLY', 'TCBX', 'SSBI', 'UNTY', 'STEL', 'PCB', 'HTLF', 'HTLFP',\n",
      "       'FBNC', 'FRST', 'VBFC', 'PNBK', 'EWBC', 'HTBK', 'WTBA', 'VBTX'],\n",
      "      dtype=object), array(['OVV', 'EPSN', 'VRN', 'BTE', 'COP', 'MUR', 'DWSN', 'OBE', 'CVE',\n",
      "       'PFIE', 'TXO', 'PAGP', 'VET', 'USEG', 'CNQ', 'WES'], dtype=object), array(['OWL', 'ACRE', 'HLI', 'APO', 'BX', 'EVR', 'RNP', 'PWP', 'JEF',\n",
      "       'UBS', 'BN', 'GS', 'PSEC', 'PJT', 'RILYL', 'RILY'], dtype=object), array(['OWLT', 'ZEPP', 'BBIG', 'HNST', 'CTRN', 'LFWD', 'PRPL', 'BFLY',\n",
      "       'SNBR', 'CRI', 'NRXS', 'EJH', 'HRMY', 'DTSS', 'CBLL', 'YJ'],\n",
      "      dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([9, 16, 106])\n",
      "Static Tensor Shape: torch.Size([9, 544]), Historical Tensor Shape: torch.Size([9, 1152])\n",
      "Shape of weighted_sum: torch.Size([9, 768]), attention_weights: torch.Size([9, 16, 1])\n",
      "Shape of static_output: torch.Size([9, 256]), similarity: torch.Size([9, 32]), historical: torch.Size([9, 512])\n",
      "Shapes: weighted_sum: torch.Size([9, 768]), attention_weights: torch.Size([9, 16]), combined_static_output: torch.Size([9, 256]), combined_historical: torch.Size([9, 512]), similarity: torch.Size([9, 32])\n",
      "Shape of combined_retrieval_input: torch.Size([9, 1584])\n",
      "Using auxiliary inputs\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1398],\n",
      "        [-0.1400],\n",
      "        [-0.1398],\n",
      "        [-0.1381],\n",
      "        [-0.1395],\n",
      "        [-0.1401],\n",
      "        [-0.1422],\n",
      "        [-0.1418],\n",
      "        [-0.1410]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1390],\n",
      "        [-0.1399],\n",
      "        [-0.1397],\n",
      "        [-0.1396],\n",
      "        [-0.1384],\n",
      "        [-0.1416],\n",
      "        [-0.1419],\n",
      "        [-0.1395],\n",
      "        [-0.1402]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1414],\n",
      "        [-0.1426],\n",
      "        [-0.1429],\n",
      "        [-0.1419],\n",
      "        [-0.1422],\n",
      "        [-0.1410],\n",
      "        [-0.1430],\n",
      "        [-0.1393],\n",
      "        [-0.1401]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1390],\n",
      "        [-0.1397],\n",
      "        [-0.1391],\n",
      "        [-0.1410],\n",
      "        [-0.1412],\n",
      "        [-0.1419],\n",
      "        [-0.1402],\n",
      "        [-0.1402],\n",
      "        [-0.1398]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1402],\n",
      "        [-0.1409],\n",
      "        [-0.1393],\n",
      "        [-0.1397],\n",
      "        [-0.1404],\n",
      "        [-0.1402],\n",
      "        [-0.1392],\n",
      "        [-0.1416],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1393],\n",
      "        [-0.1403],\n",
      "        [-0.1417],\n",
      "        [-0.1420],\n",
      "        [-0.1406],\n",
      "        [-0.1413],\n",
      "        [-0.1397],\n",
      "        [-0.1405],\n",
      "        [-0.1406]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1411],\n",
      "        [-0.1416],\n",
      "        [-0.1390],\n",
      "        [-0.1387],\n",
      "        [-0.1395],\n",
      "        [-0.1414],\n",
      "        [-0.1392],\n",
      "        [-0.1417],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1398],\n",
      "        [-0.1405],\n",
      "        [-0.1406],\n",
      "        [-0.1394],\n",
      "        [-0.1387],\n",
      "        [-0.1395],\n",
      "        [-0.1411],\n",
      "        [-0.1408],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1404],\n",
      "        [-0.1413],\n",
      "        [-0.1401],\n",
      "        [-0.1390],\n",
      "        [-0.1411],\n",
      "        [-0.1397],\n",
      "        [-0.1414],\n",
      "        [-0.1419],\n",
      "        [-0.1400]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1407],\n",
      "        [-0.1401],\n",
      "        [-0.1410],\n",
      "        [-0.1400],\n",
      "        [-0.1402],\n",
      "        [-0.1391],\n",
      "        [-0.1387],\n",
      "        [-0.1391],\n",
      "        [-0.1391]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1419],\n",
      "        [-0.1424],\n",
      "        [-0.1406],\n",
      "        [-0.1411],\n",
      "        [-0.1388],\n",
      "        [-0.1414],\n",
      "        [-0.1409],\n",
      "        [-0.1403],\n",
      "        [-0.1393]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1393],\n",
      "        [-0.1421],\n",
      "        [-0.1410],\n",
      "        [-0.1393],\n",
      "        [-0.1385],\n",
      "        [-0.1400],\n",
      "        [-0.1403],\n",
      "        [-0.1414],\n",
      "        [-0.1396]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Prediction before doing operations: [tensor([[-0.1398],\n",
      "        [-0.1400],\n",
      "        [-0.1398],\n",
      "        [-0.1381],\n",
      "        [-0.1395],\n",
      "        [-0.1401],\n",
      "        [-0.1422],\n",
      "        [-0.1418],\n",
      "        [-0.1410]], grad_fn=<AddmmBackward0>), tensor([[-0.1390],\n",
      "        [-0.1399],\n",
      "        [-0.1397],\n",
      "        [-0.1396],\n",
      "        [-0.1384],\n",
      "        [-0.1416],\n",
      "        [-0.1419],\n",
      "        [-0.1395],\n",
      "        [-0.1402]], grad_fn=<AddmmBackward0>), tensor([[-0.1414],\n",
      "        [-0.1426],\n",
      "        [-0.1429],\n",
      "        [-0.1419],\n",
      "        [-0.1422],\n",
      "        [-0.1410],\n",
      "        [-0.1430],\n",
      "        [-0.1393],\n",
      "        [-0.1401]], grad_fn=<AddmmBackward0>), tensor([[-0.1390],\n",
      "        [-0.1397],\n",
      "        [-0.1391],\n",
      "        [-0.1410],\n",
      "        [-0.1412],\n",
      "        [-0.1419],\n",
      "        [-0.1402],\n",
      "        [-0.1402],\n",
      "        [-0.1398]], grad_fn=<AddmmBackward0>), tensor([[-0.1402],\n",
      "        [-0.1409],\n",
      "        [-0.1393],\n",
      "        [-0.1397],\n",
      "        [-0.1404],\n",
      "        [-0.1402],\n",
      "        [-0.1392],\n",
      "        [-0.1416],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>), tensor([[-0.1393],\n",
      "        [-0.1403],\n",
      "        [-0.1417],\n",
      "        [-0.1420],\n",
      "        [-0.1406],\n",
      "        [-0.1413],\n",
      "        [-0.1397],\n",
      "        [-0.1405],\n",
      "        [-0.1406]], grad_fn=<AddmmBackward0>), tensor([[-0.1411],\n",
      "        [-0.1416],\n",
      "        [-0.1390],\n",
      "        [-0.1387],\n",
      "        [-0.1395],\n",
      "        [-0.1414],\n",
      "        [-0.1392],\n",
      "        [-0.1417],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>), tensor([[-0.1398],\n",
      "        [-0.1405],\n",
      "        [-0.1406],\n",
      "        [-0.1394],\n",
      "        [-0.1387],\n",
      "        [-0.1395],\n",
      "        [-0.1411],\n",
      "        [-0.1408],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>), tensor([[-0.1404],\n",
      "        [-0.1413],\n",
      "        [-0.1401],\n",
      "        [-0.1390],\n",
      "        [-0.1411],\n",
      "        [-0.1397],\n",
      "        [-0.1414],\n",
      "        [-0.1419],\n",
      "        [-0.1400]], grad_fn=<AddmmBackward0>), tensor([[-0.1407],\n",
      "        [-0.1401],\n",
      "        [-0.1410],\n",
      "        [-0.1400],\n",
      "        [-0.1402],\n",
      "        [-0.1391],\n",
      "        [-0.1387],\n",
      "        [-0.1391],\n",
      "        [-0.1391]], grad_fn=<AddmmBackward0>), tensor([[-0.1419],\n",
      "        [-0.1424],\n",
      "        [-0.1406],\n",
      "        [-0.1411],\n",
      "        [-0.1388],\n",
      "        [-0.1414],\n",
      "        [-0.1409],\n",
      "        [-0.1403],\n",
      "        [-0.1393]], grad_fn=<AddmmBackward0>), tensor([[-0.1393],\n",
      "        [-0.1421],\n",
      "        [-0.1410],\n",
      "        [-0.1393],\n",
      "        [-0.1385],\n",
      "        [-0.1400],\n",
      "        [-0.1403],\n",
      "        [-0.1414],\n",
      "        [-0.1396]], grad_fn=<AddmmBackward0>)]\n",
      "[[-0.13975291 -0.13899489 -0.14141016 -0.13900377 -0.14024194 -0.13934019\n",
      "  -0.141063   -0.13977425 -0.14040697 -0.14069341 -0.14189386 -0.13929592]\n",
      " [-0.1400397  -0.13987681 -0.14255734 -0.13967457 -0.14094721 -0.14032759\n",
      "  -0.14164376 -0.14052275 -0.14134358 -0.14012055 -0.14244212 -0.14205734]\n",
      " [-0.1397947  -0.13972314 -0.14285766 -0.13906573 -0.13926812 -0.14166296\n",
      "  -0.1390013  -0.14056008 -0.14013477 -0.14103253 -0.1405877  -0.14102283]\n",
      " [-0.13812315 -0.13963827 -0.14186785 -0.14100505 -0.13974133 -0.1420184\n",
      "  -0.13866222 -0.13942091 -0.13900737 -0.1399907  -0.1411089  -0.13926929]\n",
      " [-0.13945131 -0.13841777 -0.1421669  -0.14123754 -0.14037694 -0.14063396\n",
      "  -0.13949151 -0.13871607 -0.14108755 -0.14015964 -0.13875689 -0.13846043]\n",
      " [-0.14005782 -0.14155136 -0.14101027 -0.1418797  -0.14016584 -0.14129905\n",
      "  -0.14143865 -0.13949554 -0.1396729  -0.1391329  -0.14142485 -0.139999  ]\n",
      " [-0.14221452 -0.14193675 -0.14298224 -0.14019544 -0.13921572 -0.139698\n",
      "  -0.13923019 -0.14114733 -0.14139177 -0.13870747 -0.14086066 -0.14032562]\n",
      " [-0.1417682  -0.13948286 -0.13930626 -0.14023347 -0.1416043  -0.14051278\n",
      "  -0.14172037 -0.14079843 -0.1418889  -0.13905998 -0.1403321  -0.14142664]\n",
      " [-0.14104639 -0.1402023  -0.14008807 -0.13978522 -0.13974734 -0.1405612\n",
      "  -0.13969307 -0.13968736 -0.13995267 -0.13911523 -0.13927256 -0.13961829]]\n",
      "(9, 12)\n",
      "Retrieved tickers:  [array(['OTTR', 'XEL', 'LNT', 'PCG', 'RRX', 'AGX', 'CNP', 'VMI', 'DTE',\n",
      "       'AEE', 'POWL', 'SO', 'CETY', 'FELE', 'PKX', 'NRG'], dtype=object), array(['OUST', 'ROK', 'INVZ', 'LTRX', 'AEVA', 'STM', 'INDI', 'ADI', 'AVY',\n",
      "       'BSY', 'AMBA', 'APTV', 'ALGM', 'VC', 'KARO', 'MVIS'], dtype=object), array(['OUT', 'CCO', 'KD', 'APPS', 'EXTR', 'EH', 'ILLR', 'BNZI', 'ICLK',\n",
      "       'APP', 'SPHR', 'IPG', 'OB', 'DAVA', 'ITI', 'TTGT'], dtype=object), array(['OVBC', 'CARE', 'PFS', 'FULT', 'FULTP', 'UBSI', 'RRBI', 'FBMS',\n",
      "       'SHBI', 'TOWN', 'BOTJ', 'VABK', 'PNC', 'BRBS', 'BPRN', 'FBNC'],\n",
      "      dtype=object), array(['OVID', 'PRAX', 'PTCT', 'APLT', 'MRNS', 'BMRN', 'ANNX', 'XFOR',\n",
      "       'NBIX', 'CMRX', 'TGTX', 'APLS', 'MIRM', 'SEEL', 'PHAR', 'IONS'],\n",
      "      dtype=object), array(['OVLY', 'TCBX', 'SSBI', 'UNTY', 'STEL', 'PCB', 'HTLF', 'HTLFP',\n",
      "       'FBNC', 'FRST', 'VBFC', 'PNBK', 'EWBC', 'HTBK', 'WTBA', 'VBTX'],\n",
      "      dtype=object), array(['OVV', 'EPSN', 'VRN', 'BTE', 'COP', 'MUR', 'DWSN', 'OBE', 'CVE',\n",
      "       'PFIE', 'TXO', 'PAGP', 'VET', 'USEG', 'CNQ', 'WES'], dtype=object), array(['OWL', 'ACRE', 'HLI', 'APO', 'BX', 'EVR', 'RNP', 'PWP', 'JEF',\n",
      "       'UBS', 'BN', 'GS', 'PSEC', 'PJT', 'RILYL', 'RILY'], dtype=object), array(['OWLT', 'ZEPP', 'BBIG', 'HNST', 'CTRN', 'LFWD', 'PRPL', 'BFLY',\n",
      "       'SNBR', 'CRI', 'NRXS', 'EJH', 'HRMY', 'DTSS', 'CBLL', 'YJ'],\n",
      "      dtype=object)]\n",
      "We have these retrieved documents:  torch.Size([9, 16, 106])\n",
      "Static Tensor Shape: torch.Size([9, 544]), Historical Tensor Shape: torch.Size([9, 1152])\n",
      "Shape of weighted_sum: torch.Size([9, 768]), attention_weights: torch.Size([9, 16, 1])\n",
      "Shape of static_output: torch.Size([9, 256]), similarity: torch.Size([9, 32]), historical: torch.Size([9, 512])\n",
      "Shapes: weighted_sum: torch.Size([9, 768]), attention_weights: torch.Size([9, 16]), combined_static_output: torch.Size([9, 256]), combined_historical: torch.Size([9, 512]), similarity: torch.Size([9, 32])\n",
      "Shape of combined_retrieval_input: torch.Size([9, 1584])\n",
      "Not using auxiliary inputs\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1406],\n",
      "        [-0.1405],\n",
      "        [-0.1406],\n",
      "        [-0.1404],\n",
      "        [-0.1410],\n",
      "        [-0.1386],\n",
      "        [-0.1405],\n",
      "        [-0.1415],\n",
      "        [-0.1425]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1396],\n",
      "        [-0.1408],\n",
      "        [-0.1420],\n",
      "        [-0.1400],\n",
      "        [-0.1414],\n",
      "        [-0.1413],\n",
      "        [-0.1408],\n",
      "        [-0.1413],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1381],\n",
      "        [-0.1399],\n",
      "        [-0.1404],\n",
      "        [-0.1411],\n",
      "        [-0.1401],\n",
      "        [-0.1406],\n",
      "        [-0.1394],\n",
      "        [-0.1393],\n",
      "        [-0.1411]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1392],\n",
      "        [-0.1409],\n",
      "        [-0.1398],\n",
      "        [-0.1415],\n",
      "        [-0.1406],\n",
      "        [-0.1418],\n",
      "        [-0.1405],\n",
      "        [-0.1412],\n",
      "        [-0.1402]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1409],\n",
      "        [-0.1403],\n",
      "        [-0.1403],\n",
      "        [-0.1425],\n",
      "        [-0.1412],\n",
      "        [-0.1406],\n",
      "        [-0.1405],\n",
      "        [-0.1390],\n",
      "        [-0.1388]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1388],\n",
      "        [-0.1386],\n",
      "        [-0.1393],\n",
      "        [-0.1416],\n",
      "        [-0.1399],\n",
      "        [-0.1417],\n",
      "        [-0.1408],\n",
      "        [-0.1414],\n",
      "        [-0.1401]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1418],\n",
      "        [-0.1394],\n",
      "        [-0.1405],\n",
      "        [-0.1412],\n",
      "        [-0.1412],\n",
      "        [-0.1403],\n",
      "        [-0.1409],\n",
      "        [-0.1398],\n",
      "        [-0.1412]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1408],\n",
      "        [-0.1404],\n",
      "        [-0.1398],\n",
      "        [-0.1417],\n",
      "        [-0.1427],\n",
      "        [-0.1409],\n",
      "        [-0.1386],\n",
      "        [-0.1412],\n",
      "        [-0.1404]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1413],\n",
      "        [-0.1394],\n",
      "        [-0.1399],\n",
      "        [-0.1408],\n",
      "        [-0.1414],\n",
      "        [-0.1407],\n",
      "        [-0.1409],\n",
      "        [-0.1402],\n",
      "        [-0.1404]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1391],\n",
      "        [-0.1406],\n",
      "        [-0.1394],\n",
      "        [-0.1418],\n",
      "        [-0.1434],\n",
      "        [-0.1412],\n",
      "        [-0.1407],\n",
      "        [-0.1392],\n",
      "        [-0.1405]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1412],\n",
      "        [-0.1407],\n",
      "        [-0.1402],\n",
      "        [-0.1398],\n",
      "        [-0.1399],\n",
      "        [-0.1403],\n",
      "        [-0.1406],\n",
      "        [-0.1398],\n",
      "        [-0.1389]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Shapes of static_output: torch.Size([9, 32]), historical_output: torch.Size([9, 64]), idea: torch.Size([9, 256]), attention_output: torch.Size([9, 512])\n",
      "Final prediction: tensor([[-0.1405],\n",
      "        [-0.1406],\n",
      "        [-0.1403],\n",
      "        [-0.1420],\n",
      "        [-0.1407],\n",
      "        [-0.1399],\n",
      "        [-0.1401],\n",
      "        [-0.1422],\n",
      "        [-0.1405]], grad_fn=<AddmmBackward0>) and shape: torch.Size([9, 1])\n",
      "Final prediction: torch.Size([9, 1]), historical tensor: torch.Size([9, 60])\n",
      "Resulting historical tensor shape: torch.Size([9, 60])\n",
      "Prediction before doing operations: [tensor([[-0.1406],\n",
      "        [-0.1405],\n",
      "        [-0.1406],\n",
      "        [-0.1404],\n",
      "        [-0.1410],\n",
      "        [-0.1386],\n",
      "        [-0.1405],\n",
      "        [-0.1415],\n",
      "        [-0.1425]], grad_fn=<AddmmBackward0>), tensor([[-0.1396],\n",
      "        [-0.1408],\n",
      "        [-0.1420],\n",
      "        [-0.1400],\n",
      "        [-0.1414],\n",
      "        [-0.1413],\n",
      "        [-0.1408],\n",
      "        [-0.1413],\n",
      "        [-0.1397]], grad_fn=<AddmmBackward0>), tensor([[-0.1381],\n",
      "        [-0.1399],\n",
      "        [-0.1404],\n",
      "        [-0.1411],\n",
      "        [-0.1401],\n",
      "        [-0.1406],\n",
      "        [-0.1394],\n",
      "        [-0.1393],\n",
      "        [-0.1411]], grad_fn=<AddmmBackward0>), tensor([[-0.1392],\n",
      "        [-0.1409],\n",
      "        [-0.1398],\n",
      "        [-0.1415],\n",
      "        [-0.1406],\n",
      "        [-0.1418],\n",
      "        [-0.1405],\n",
      "        [-0.1412],\n",
      "        [-0.1402]], grad_fn=<AddmmBackward0>), tensor([[-0.1409],\n",
      "        [-0.1403],\n",
      "        [-0.1403],\n",
      "        [-0.1425],\n",
      "        [-0.1412],\n",
      "        [-0.1406],\n",
      "        [-0.1405],\n",
      "        [-0.1390],\n",
      "        [-0.1388]], grad_fn=<AddmmBackward0>), tensor([[-0.1388],\n",
      "        [-0.1386],\n",
      "        [-0.1393],\n",
      "        [-0.1416],\n",
      "        [-0.1399],\n",
      "        [-0.1417],\n",
      "        [-0.1408],\n",
      "        [-0.1414],\n",
      "        [-0.1401]], grad_fn=<AddmmBackward0>), tensor([[-0.1418],\n",
      "        [-0.1394],\n",
      "        [-0.1405],\n",
      "        [-0.1412],\n",
      "        [-0.1412],\n",
      "        [-0.1403],\n",
      "        [-0.1409],\n",
      "        [-0.1398],\n",
      "        [-0.1412]], grad_fn=<AddmmBackward0>), tensor([[-0.1408],\n",
      "        [-0.1404],\n",
      "        [-0.1398],\n",
      "        [-0.1417],\n",
      "        [-0.1427],\n",
      "        [-0.1409],\n",
      "        [-0.1386],\n",
      "        [-0.1412],\n",
      "        [-0.1404]], grad_fn=<AddmmBackward0>), tensor([[-0.1413],\n",
      "        [-0.1394],\n",
      "        [-0.1399],\n",
      "        [-0.1408],\n",
      "        [-0.1414],\n",
      "        [-0.1407],\n",
      "        [-0.1409],\n",
      "        [-0.1402],\n",
      "        [-0.1404]], grad_fn=<AddmmBackward0>), tensor([[-0.1391],\n",
      "        [-0.1406],\n",
      "        [-0.1394],\n",
      "        [-0.1418],\n",
      "        [-0.1434],\n",
      "        [-0.1412],\n",
      "        [-0.1407],\n",
      "        [-0.1392],\n",
      "        [-0.1405]], grad_fn=<AddmmBackward0>), tensor([[-0.1412],\n",
      "        [-0.1407],\n",
      "        [-0.1402],\n",
      "        [-0.1398],\n",
      "        [-0.1399],\n",
      "        [-0.1403],\n",
      "        [-0.1406],\n",
      "        [-0.1398],\n",
      "        [-0.1389]], grad_fn=<AddmmBackward0>), tensor([[-0.1405],\n",
      "        [-0.1406],\n",
      "        [-0.1403],\n",
      "        [-0.1420],\n",
      "        [-0.1407],\n",
      "        [-0.1399],\n",
      "        [-0.1401],\n",
      "        [-0.1422],\n",
      "        [-0.1405]], grad_fn=<AddmmBackward0>)]\n",
      "[[-0.14060704 -0.13957341 -0.13814278 -0.1392018  -0.14093876 -0.13876069\n",
      "  -0.14178155 -0.140849   -0.14130792 -0.13909581 -0.1411919  -0.140546  ]\n",
      " [-0.14045207 -0.14075325 -0.13985768 -0.14094077 -0.14033411 -0.13860486\n",
      "  -0.13940473 -0.14043087 -0.13936286 -0.1405845  -0.14067565 -0.1406123 ]\n",
      " [-0.14064042 -0.14202707 -0.14043887 -0.13978381 -0.14030418 -0.13931978\n",
      "  -0.14052884 -0.13975278 -0.13985075 -0.13944091 -0.1402136  -0.14034662]\n",
      " [-0.140364   -0.13995698 -0.14106475 -0.14149679 -0.1424592  -0.14164798\n",
      "  -0.14115141 -0.14165542 -0.14080322 -0.14180191 -0.1398143  -0.14201644]\n",
      " [-0.14103976 -0.14137565 -0.14009209 -0.14058805 -0.14117621 -0.13986896\n",
      "  -0.14123218 -0.14269856 -0.14140518 -0.14342509 -0.13987881 -0.14066221]\n",
      " [-0.13863918 -0.14128584 -0.14056076 -0.14180464 -0.14055978 -0.14171332\n",
      "  -0.14034481 -0.14087284 -0.14072229 -0.14116663 -0.1402552  -0.13990603]\n",
      " [-0.14053789 -0.14075764 -0.1393755  -0.14052832 -0.14048366 -0.1408039\n",
      "  -0.14088035 -0.13859998 -0.14086424 -0.14065166 -0.14055441 -0.14011042]\n",
      " [-0.14154007 -0.1413403  -0.13932447 -0.1412196  -0.13896917 -0.14135976\n",
      "  -0.13982035 -0.14124717 -0.14018275 -0.13916643 -0.13981839 -0.14224279]\n",
      " [-0.14253244 -0.13971435 -0.1411017  -0.14017761 -0.13883817 -0.14008377\n",
      "  -0.14122392 -0.1403936  -0.14037526 -0.14046305 -0.13887073 -0.14046732]]\n",
      "(9, 12)\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Training Loop",
   "id": "67ab46e00ad78620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:48:28.165572Z",
     "start_time": "2025-01-03T09:48:25.072739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Initialize the retrieval system and model\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=16)\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps_num,\n",
    "    ret_sys=retrieval_system,\n",
    "    static_dim=34,\n",
    "    historical_dim=72, # THIS IS IMPORTANT\n",
    "    retrieval_number=16\n",
    ")\n",
    "model.to(current_device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        if i > 10:\n",
    "            break\n",
    "        # Get the current entry\n",
    "        idea_entry = dataset.iloc[i, :]\n",
    "        idea = idea_entry[\"business_description\"]\n",
    "\n",
    "        ticker = idea_entry[\"tickers\"]\n",
    "        print(f\"Currently testing with ticker {ticker}\")\n",
    "\n",
    "        # Prepare static and historical data\n",
    "        static_columns = [\n",
    "            col for col in dataset.columns\n",
    "            if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "        static_data = idea_entry[static_columns]\n",
    "        historical_data = idea_entry[month_columns]\n",
    "\n",
    "        # Ensure numeric data and handle missing values\n",
    "        static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "        historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "        # Separate the target and input historical data\n",
    "        target = torch.tensor(historical_data[-forecast_steps_num:], dtype=torch.float32).unsqueeze(0).to(current_device)  # Target: last forecast_steps_num\n",
    "        historical_data = historical_data[:-forecast_steps_num]  # Input: all but last forecast_steps_num\n",
    "\n",
    "        # Convert to tensors\n",
    "        static_data = torch.tensor(static_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "        historical_data = torch.tensor(historical_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "        print(f\"Shapes in training loop: static: {static_data.shape}, historical data shape: {historical_data.shape}\")\n",
    "\n",
    "        # Remove the current entry from the dataset for retrieval\n",
    "        filtered_dataset = dataset.drop(index=i)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction = model(\n",
    "            idea=idea,\n",
    "            dataset=filtered_dataset,\n",
    "            static_features=static_data,\n",
    "            historical_data=historical_data,\n",
    "            use_auxiliary_inputs=True,\n",
    "            excluded_tickers=[ticker]\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(prediction, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i}/{len(dataset)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] completed. Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "prediction = model(\n",
    "    idea=\"I want to create a coffeshop that uses digital cups that analyze exactly whats in your coffe and how it is going to impact you.\",\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "\n",
    "print(\"Prediction after Training: \", prediction)\n"
   ],
   "id": "3d3559d051a584c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing with ticker A\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RetrievalAugmentedPredictionModel.forward() got an unexpected keyword argument 'idea'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[86], line 68\u001B[0m\n\u001B[1;32m     65\u001B[0m filtered_dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mdrop(index\u001B[38;5;241m=\u001B[39mi)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43midea\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midea\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiltered_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstatic_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstatic_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistorical_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistorical_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auxiliary_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexcluded_tickers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mticker\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[1;32m     78\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(prediction, target)\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[0;31mTypeError\u001B[0m: RetrievalAugmentedPredictionModel.forward() got an unexpected keyword argument 'idea'"
     ]
    }
   ],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
