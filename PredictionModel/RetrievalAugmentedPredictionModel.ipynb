{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Prediction Model\n",
    "\n",
    "This Model, specifically created to make Stock Predictions for upcoming Businesses, means this model predicts the market startup of any new business idea.\n"
   ],
   "id": "5bfd1cf4c21a9cf9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-02T18:41:52.207280Z",
     "start_time": "2025-01-02T18:41:52.193998Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "from RetrievalSystem.RetrievalSystem import RetrievalSystem\n",
    "from PredictionModel.AttentionModel.AttentionModel import AttentionModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "INPUT_PATH = \"../RetrievalSystem/Embeddings/embeddings.csv\"\n",
    "\n",
    "BERT_DIM = 768\n",
    "\n",
    "class RetrievalAugmentedPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 128, ret_sys: RetrievalSystem = None, static_dim = 34, historical_dim = 72, forecast_steps: int = 6, retrieval_number: int = 16):\n",
    "        super(RetrievalAugmentedPredictionModel, self).__init__()\n",
    "\n",
    "        self.static_feature_dim = static_dim\n",
    "        self.historical_feature_dim = historical_dim\n",
    "        self.historical_idea_dim = historical_dim - forecast_steps\n",
    "        self.retrieval_number = retrieval_number\n",
    "\n",
    "        if ret_sys:\n",
    "            self.retrieval_system = ret_sys\n",
    "        else:\n",
    "            self.retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number)\n",
    "\n",
    "        # 16 * 768 -> 768 + 16\n",
    "        self.attention_model = AttentionModel(input_dim=BERT_DIM, hidden_dim=hidden_dim)\n",
    "\n",
    "        # 16 -> 32\n",
    "        self.similarity_fc = nn.Sequential(\n",
    "            nn.Linear(retrieval_number, 2 * retrieval_number),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * retrieval_number, 2 * retrieval_number),\n",
    "            nn.LayerNorm(2 * retrieval_number),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Use Same bert model as for original embeddings\n",
    "        # 768 -> 4 * 128 -> 128\n",
    "        self.idea_fc = nn.Sequential(\n",
    "            nn.Linear(BERT_DIM, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Static feature layers (deep)\n",
    "        # 34 * 16 -> 34 * 8 -> 256\n",
    "        self.static_fc = nn.Sequential(\n",
    "            nn.Linear(self.static_feature_dim * retrieval_number, self.static_feature_dim * (retrieval_number // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.static_feature_dim * (retrieval_number // 2), 2 * hidden_dim),\n",
    "            nn.LayerNorm(2 * hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Historical stock data layers (deep)\n",
    "        # 72 * 16-> 72 * 8 -> 72 * 8 -> 512\n",
    "        self.historical_fc = nn.Sequential(\n",
    "            nn.Linear(self.historical_feature_dim * retrieval_number, self.historical_feature_dim * (retrieval_number // 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.historical_feature_dim * (retrieval_number // 2), self.historical_feature_dim * (retrieval_number // 2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.historical_feature_dim * (retrieval_number // 2), 4 * hidden_dim),\n",
    "            nn.LayerNorm(4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 34 -> 32\n",
    "        self.idea_static_fc = nn.Linear(self.static_feature_dim, 32)\n",
    "        # 72 -> 64\n",
    "        self.idea_historical_fc = nn.Linear(self.historical_idea_dim, hidden_dim//2)\n",
    "\n",
    "        # First Fustion Layer, combines:\n",
    "        # 1. AttentionModel Output -> 768\n",
    "        # 1.a Attention Scores -> retrievel_numbre (16)\n",
    "        # 2. Combined Static Layer Output -> 256\n",
    "        # 2. Combined Static Layer Output -> 512\n",
    "        # 4. Cosine Simularity Layer -> 32\n",
    "        # combined = 1184 -> 1024 -> 512\n",
    "        self._first_fusion_fc = nn.Sequential(\n",
    "            nn.Linear(BERT_DIM + retrieval_number + 2 * hidden_dim + 4 * hidden_dim + 2 * retrieval_number, 8 * hidden_dim),  # 384 is the fixed text embedding dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8 * hidden_dim,  4 * hidden_dim),\n",
    "            nn.LayerNorm(4 * hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Attention layer after first fusion\n",
    "        self.fusion_attention = nn.MultiheadAttention(embed_dim=4 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Second Fusion Layer, combines:\n",
    "        # 1. Previous Fusion Layer Output: 512\n",
    "        # 2. Idea Embedding: 256 (ouput of idea layer)\n",
    "        # 3. Idea Static: 32\n",
    "        # 4. Idea Historical: 64\n",
    "        # combined = 992 -> 1024\n",
    "        self._second_fusion_fc = nn.Sequential(\n",
    "            nn.Linear(4 * hidden_dim + 2 * hidden_dim + 32 + 64, 8 * hidden_dim),  # 384 is the fixed text embedding dimension\n",
    "            nn.GELU(),\n",
    "            nn.Linear(8 * hidden_dim, 7 * hidden_dim),\n",
    "            nn.LayerNorm(7 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(7 * hidden_dim, 5 * hidden_dim),\n",
    "            nn.Hardswish(),\n",
    "            nn.Linear(5 * hidden_dim, 4 * hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Second fusion\n",
    "        self.second_fusion_attention = nn.MultiheadAttention(embed_dim=4 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Multi-layer LSTM with residual connection\n",
    "        self.lstm = nn.LSTM(4 * hidden_dim, 2 * hidden_dim, num_layers=10, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=2 * hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        # Output layer for forecasting\n",
    "        self.output_fc = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.Hardswish(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim //2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1), # Final Output\n",
    "        )\n",
    "\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def forward(self, idea, dataset: pd.DataFrame = None, static_features=None, historical_data=None, use_auxiliary_inputs=True, excluded_tickers=None):\n",
    "        # Ensure device compatibility\n",
    "        if excluded_tickers is None:\n",
    "            excluded_tickers = []\n",
    "        if dataset is None:\n",
    "            print(\"We need a dataset for retrieval\")\n",
    "            return None\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Get Idea embedding and similar documents\n",
    "        idea_embedding, retrieved_documents = self.retrieval_system.find_similar_entries(text=idea, top_n=self.retrieval_number, excluded_tickers=excluded_tickers)\n",
    "        idea_embedding = torch.tensor(idea_embedding, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Extract embeddings and tickers from retrieved documents\n",
    "        retrieved_idea_embeddings = retrieved_documents.loc[:, [\"embedding\"]].values\n",
    "        print(\"Combined embeddings shape: \", retrieved_idea_embeddings.shape)\n",
    "        retrieved_idea_embeddings = torch.tensor(retrieved_idea_embeddings.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "        retrieved_similarities = retrieved_documents.loc[:, [\"similarity\"]].values.flatten()\n",
    "        retrieved_similarities = torch.tensor(retrieved_similarities.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "        # Filter rows from the dataset where the index is in retrieved tickers\n",
    "        retrieved_tickers = retrieved_documents.loc[:, [\"tickers\"]].values.flatten()  # Flatten to get a 1D array of tickers\n",
    "        print(\"Retrieved tickers: \", retrieved_tickers)\n",
    "        dataset = dataset.set_index(\"tickers\")\n",
    "        filtered_data = dataset[dataset.index.isin(retrieved_tickers)]\n",
    "        print(\"We have these retrieved documents: \", filtered_data.shape)\n",
    "\n",
    "        # Create a vector with all columns that are not \"ticker\", \"business_description\", or starting with \"month\"\n",
    "        static_columns = [\n",
    "            col for col in filtered_data.columns\n",
    "            if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        static_vector = filtered_data[static_columns].values.flatten()  # Convert to NumPy array\n",
    "\n",
    "        # Create a second vector with all columns starting with \"month\"\n",
    "        month_columns = [col for col in filtered_data.columns if col.startswith(\"month\")]\n",
    "        month_vector = filtered_data[month_columns].values.flatten()  # Convert to NumPy array\n",
    "\n",
    "        print(f\"Shape of static vector: {static_vector.shape}, Shape of month vector: {month_vector.shape}\")\n",
    "\n",
    "        # Convert vectors to tensors and move to the appropriate device\n",
    "        combined_static_tensor = torch.tensor(static_vector, dtype=torch.float32).to(device)\n",
    "        combined_historical_tensor = torch.tensor(month_vector, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Initialize the historical data tensor (to handle shifting)\n",
    "        if historical_data is not None and use_auxiliary_inputs:\n",
    "            historical_tensor = historical_data.clone().to(device)\n",
    "        else:\n",
    "            historical_tensor = torch.zeros((1, self.historical_idea_dim), dtype=torch.float32).to(device)\n",
    "\n",
    "        predictions = []\n",
    "        for step in range(self.forecast_steps):\n",
    "            # Put retrieved documents into appropriate input layers\n",
    "            weighted_sum, attention_weights = self.attention_model(retrieved_idea_embeddings)\n",
    "            attention_weights = attention_weights.view(1, -1)\n",
    "            print(f\"Shape of weighted_sum: {weighted_sum.shape}, attention_weights: {attention_weights.shape}\")\n",
    "\n",
    "            similarity_output = self.similarity_fc(retrieved_similarities).unsqueeze(0)\n",
    "            combined_static_output = self.static_fc(combined_static_tensor).unsqueeze(0)\n",
    "            combined_historical_output = self.historical_fc(combined_historical_tensor).unsqueeze(0)\n",
    "            print(f\"Shape of static_output: {combined_static_output.shape}, similarity: {similarity_output.shape}, historical: {combined_historical_output.shape}\")\n",
    "\n",
    "            # 1. FUSION LAYER - Fuse retrieval layers together\n",
    "            combined_retrieval_input = torch.cat((weighted_sum, attention_weights, combined_static_output, combined_historical_output, similarity_output), dim=1)\n",
    "            first_fusion_output = self._first_fusion_fc(combined_retrieval_input)\n",
    "\n",
    "            # Attention layer\n",
    "            first_fusion_attention_output, _ = self.fusion_attention(first_fusion_output, first_fusion_output, first_fusion_output)\n",
    "\n",
    "            # Put new ideas data into input layers\n",
    "            idea_output = self.idea_fc(idea_embedding)\n",
    "            if use_auxiliary_inputs:\n",
    "                static_output = self.idea_static_fc(static_features.to(device))\n",
    "                historical_output = self.idea_historical_fc(historical_tensor)\n",
    "            else:\n",
    "                static_input = torch.zeros((1, self.static_feature_dim), dtype=torch.float32).to(device)\n",
    "                static_output = self.idea_static_fc(static_input)\n",
    "                historical_input = torch.zeros((1, self.historical_idea_dim), dtype=torch.float32).to(device)\n",
    "                historical_output = self.idea_historical_fc(historical_input)\n",
    "\n",
    "            # 2. FUSION LAYER - Fuse combined retrieval documents and new idea together\n",
    "            print(f\"Shapes of static_output: {static_output.shape}, historical_output: {historical_output.shape}, idea: {idea_output.shape}, attention_output: {first_fusion_attention_output.shape}\")\n",
    "            combined_idea_input = torch.cat((first_fusion_attention_output, idea_output, static_output, historical_output), dim=1)\n",
    "            second_fusion_output = self._second_fusion_fc(combined_idea_input)\n",
    "\n",
    "            # Attention layer\n",
    "            second_fusion_attention_output, _ = self.second_fusion_attention(second_fusion_output, second_fusion_output, second_fusion_output)\n",
    "\n",
    "            # LSTM\n",
    "            lstm_output, _ = self.lstm(second_fusion_attention_output.unsqueeze(1))  # Add sequence dimension\n",
    "\n",
    "            # Attention\n",
    "            lstm_attention_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "\n",
    "            # OUTPUT\n",
    "            final_prediction = self.output_fc(lstm_attention_output.squeeze(1))  # Remove sequence dimension\n",
    "\n",
    "            # Append to predictions\n",
    "            predictions.append(final_prediction)\n",
    "\n",
    "            # Update historical tensor for next step\n",
    "            print(f\"Final prediction: {final_prediction.shape}, historical tensor: {historical_tensor.shape}\")\n",
    "            historical_tensor = torch.cat((historical_tensor[:, 1:], final_prediction), dim=1)\n",
    "            print(f\"Resulting historical tensor shape: {historical_tensor.shape}\")\n",
    "\n",
    "        # Stack predictions into a single tensor\n",
    "        predictions = torch.stack(predictions, dim=1)  # Shape: [1, forecast_steps, 1]\n",
    "        predictions = predictions.squeeze(-1)  # Remove the last dimension, Shape: [1, forecast_steps]\n",
    "        return predictions\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example usage\n",
    "Here is an example of how to use our newly created model:"
   ],
   "id": "ffb934d9de7941ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:42:16.402171Z",
     "start_time": "2025-01-02T18:41:52.218071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Initialize the model - HAVE TO BE ADAPTED TO DATASET (Values are likely correct)\n",
    "static_feature_dim_num = 4    # Number of static features\n",
    "historical_dim_num = 12       # Number of historical stock performance points\n",
    "hidden_dim_num = 128          # Hidden layer size\n",
    "forecast_steps_num = 12       # Predict next 12 months\n",
    "\n",
    "\n",
    "DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=16)\n",
    "\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps_num,\n",
    "    ret_sys = retrieval_system,\n",
    "    retrieval_number=16\n",
    ")\n",
    "\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random entry\n",
    "idea_entry = dataset.iloc[4000, :]  # Get a random row (Series)\n",
    "idea = idea_entry[\"business_description\"]\n",
    "\n",
    "static_columns = [\n",
    "    col for col in dataset.columns\n",
    "    if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "]\n",
    "month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "static_data = idea_entry[static_columns]\n",
    "historical_data = idea_entry[month_columns]\n",
    "\n",
    "# Ensure numeric data and handle missing values\n",
    "static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "# Convert to tensors\n",
    "static_data = torch.tensor(static_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "historical_data = torch.tensor(historical_data[forecast_steps_num:], dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    idea=idea,\n",
    "    dataset=dataset,\n",
    "    static_features=static_data,\n",
    "    historical_data=historical_data,\n",
    "    use_auxiliary_inputs=True\n",
    ")\n",
    "print(prediction.detach().cpu().numpy())  # Co\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model(\n",
    "    idea=idea,\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "print(prediction.detach().cpu().numpy())  # Co\n",
    "\n"
   ],
   "id": "43396d2bee8576ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['OTTR' 'XEL' 'LNT' 'PCG' 'RRX' 'AGX' 'CNP' 'VMI' 'DTE' 'AEE' 'POWL' 'SO'\n",
      " 'CETY' 'FELE' 'PKX' 'NRG']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "[[0.02991007 0.03150583 0.03161616 0.03136112 0.03260419 0.03320973\n",
      "  0.03305061 0.03108346 0.02946009 0.03386216 0.03390808 0.03340594]]\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['OTTR' 'XEL' 'LNT' 'PCG' 'RRX' 'AGX' 'CNP' 'VMI' 'DTE' 'AEE' 'POWL' 'SO'\n",
      " 'CETY' 'FELE' 'PKX' 'NRG']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "[[0.03294794 0.03154049 0.03281134 0.03168665 0.03316632 0.03173842\n",
      "  0.03362747 0.0322229  0.03418783 0.03475092 0.03308937 0.03175249]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Training Loop",
   "id": "67ab46e00ad78620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:43:35.900819Z",
     "start_time": "2025-01-02T18:42:16.422266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "DATASET_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Initialize the retrieval system and model\n",
    "retrieval_system = RetrievalSystem(INPUT_PATH, retrieval_number=16)\n",
    "model = RetrievalAugmentedPredictionModel(\n",
    "    forecast_steps=forecast_steps_num,\n",
    "    ret_sys=retrieval_system,\n",
    "    static_dim=34,\n",
    "    historical_dim=72, # THIS IS IMPORTANT\n",
    "    retrieval_number=16\n",
    ")\n",
    "model.to(current_device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        if i > 10:\n",
    "            break\n",
    "        # Get the current entry\n",
    "        idea_entry = dataset.iloc[i, :]\n",
    "        idea = idea_entry[\"business_description\"]\n",
    "\n",
    "        ticker = idea_entry[\"tickers\"]\n",
    "        print(f\"Currently testing with ticker {ticker}\")\n",
    "\n",
    "        # Prepare static and historical data\n",
    "        static_columns = [\n",
    "            col for col in dataset.columns\n",
    "            if col not in [\"tickers\", \"business_description\"] and not col.startswith(\"month\")\n",
    "        ]\n",
    "        month_columns = [col for col in dataset.columns if col.startswith(\"month\")]\n",
    "\n",
    "        static_data = idea_entry[static_columns]\n",
    "        historical_data = idea_entry[month_columns]\n",
    "\n",
    "        # Ensure numeric data and handle missing values\n",
    "        static_data = static_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "        historical_data = historical_data.apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(float)\n",
    "\n",
    "        # Separate the target and input historical data\n",
    "        target = torch.tensor(historical_data[-forecast_steps_num:], dtype=torch.float32).unsqueeze(0).to(current_device)  # Target: last forecast_steps_num\n",
    "        historical_data = historical_data[:-forecast_steps_num]  # Input: all but last forecast_steps_num\n",
    "\n",
    "        # Convert to tensors\n",
    "        static_data = torch.tensor(static_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "        historical_data = torch.tensor(historical_data, dtype=torch.float32).unsqueeze(0).to(current_device)\n",
    "        print(f\"Shapes in training loop: static: {static_data.shape}, historical data shape: {historical_data.shape}\")\n",
    "\n",
    "        # Remove the current entry from the dataset for retrieval\n",
    "        filtered_dataset = dataset.drop(index=i)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction = model(\n",
    "            idea=idea,\n",
    "            dataset=filtered_dataset,\n",
    "            static_features=static_data,\n",
    "            historical_data=historical_data,\n",
    "            use_auxiliary_inputs=True,\n",
    "            excluded_tickers=[ticker]\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(prediction, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i}/{len(dataset)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] completed. Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "prediction = model(\n",
    "    idea=\"I want to create a coffeshop that uses digital cups that analyze exactly whats in your coffe and how it is going to impact you.\",\n",
    "    dataset=dataset,\n",
    "    use_auxiliary_inputs=False\n",
    ")\n",
    "\n",
    "print(\"Prediction after Training: \", prediction)\n"
   ],
   "id": "3d3559d051a584c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing with ticker A\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['IR' 'WAT' 'BRKR' 'ICHR' 'AME' 'CHX' 'PH' 'AMAT' 'LUNA' 'UCTT' 'E' 'NPO'\n",
      " 'DOV' 'ACMR' 'HBIO' 'LRCX']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Epoch [1/1], Step [0/7089], Loss: 0.0047\n",
      "Currently testing with ticker AA\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['HUDI' 'CMC' 'MT' 'MSM' 'FAST' 'RYI' 'TX' 'KALU' 'SIM' 'KWR' 'AP' 'GWAV'\n",
      " 'LECO' 'RIOT' 'ASTL' 'GATX']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Currently testing with ticker AACG\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['GSUN' 'STG' 'AMBO' 'COE' 'DAO' 'JDZG' 'EEIQ' 'GV' 'EDU' 'LRN' 'NRDY'\n",
      " 'TCTM' 'CHGG' 'CLEU' 'YXT' 'TAL']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Currently testing with ticker AACT\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['RENEW' 'RENE' 'ATMC' 'SKGR' 'SKGRW' 'GODN' 'IPXXU' 'IPXX' 'CNDA' 'BHAC'\n",
      " 'ONYX' 'BAYA' 'ATEK' 'HSPTU' 'TMTC' 'AIB']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Currently testing with ticker AADI\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['RNAZ' 'VSTM' 'PDSB' 'FHTX' 'YMAB' 'DAWN' 'IMNM' 'MNPR' 'GLYC' 'THAR'\n",
      " 'IMAB' 'ZLAB' 'KRON' 'IDYA' 'VCNX' 'CELC']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Currently testing with ticker AAL\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['DAL' 'UAL' 'AL' 'FTAIN' 'FTAIO' 'FTAI' 'FTAIM' 'LTM' 'VLRS' 'MOND'\n",
      " 'SKYW' 'CPA' 'SAVE' 'CAAP' 'JBLU' 'XTIA']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Currently testing with ticker AAME\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n",
      "Combined embeddings shape:  (16, 1)\n",
      "Retrieved tickers:  ['AFG' 'KMPR' 'KNSL' 'HMN' 'CIA' 'HIG' 'WRB' 'AIFU' 'FANH' 'SIGI' 'SIGIP'\n",
      " 'AFL' 'AIG' 'WTM' 'PRA' 'ACGLN']\n",
      "We have these retrieved documents:  (16, 107)\n",
      "Shape of static vector: (544,), Shape of month vector: (1152,)\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Shape of weighted_sum: torch.Size([1, 768]), attention_weights: torch.Size([1, 16])\n",
      "Shape of static_output: torch.Size([1, 256]), similarity: torch.Size([1, 32]), historical: torch.Size([1, 512])\n",
      "Shapes of static_output: torch.Size([1, 32]), historical_output: torch.Size([1, 64]), idea: torch.Size([1, 256]), attention_output: torch.Size([1, 512])\n",
      "Final prediction: torch.Size([1, 1]), historical tensor: torch.Size([1, 60])\n",
      "Resulting historical tensor shape: torch.Size([1, 60])\n",
      "Currently testing with ticker AAOI\n",
      "Shapes in training loop: static: torch.Size([1, 34]), historical data shape: torch.Size([1, 60])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 68\u001B[0m\n\u001B[1;32m     65\u001B[0m filtered_dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mdrop(index\u001B[38;5;241m=\u001B[39mi)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43midea\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midea\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiltered_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstatic_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstatic_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistorical_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistorical_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auxiliary_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexcluded_tickers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mticker\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[1;32m     78\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(prediction, target)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[8], line 143\u001B[0m, in \u001B[0;36mRetrievalAugmentedPredictionModel.forward\u001B[0;34m(self, idea, dataset, static_features, historical_data, use_auxiliary_inputs, excluded_tickers)\u001B[0m\n\u001B[1;32m    140\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters())\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# Get Idea embedding and similar documents\u001B[39;00m\n\u001B[0;32m--> 143\u001B[0m idea_embedding, retrieved_documents \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieval_system\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_similar_entries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midea\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieval_number\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexcluded_tickers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexcluded_tickers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m idea_embedding \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(idea_embedding, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# Extract embeddings and tickers from retrieved documents\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/RetrievalSystem/RetrievalSystem.py:76\u001B[0m, in \u001B[0;36mRetrievalSystem.find_similar_entries\u001B[0;34m(self, text, top_n, excluded_tickers)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Convert strings to lists only if they are strings\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(copied_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 76\u001B[0m     copied_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mcopied_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43membedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43meval\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m copied_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# Compute cosine similarities\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m<string>:0\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
