{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hybrid Stock Prediction Model Training\n",
    "In the \"HybridStockPredictionModel\" notebook we created our model that can be used to make efficient stock prediction for new business ideas.\n",
    "\n",
    "At first we create our Dataset class that will be used to train the model:"
   ],
   "id": "9e59e4e80ee01ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T08:34:25.780685Z",
     "start_time": "2024-11-12T08:34:25.770493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from Model import HybridStockPredictionModel\n",
    "\n",
    "\n",
    "class StockPerformanceDataset(Dataset):\n",
    "    def __init__(self, data, text_encoder, forecast_steps=12):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: A list of dictionaries where each dictionary contains:\n",
    "                  - 'idea_text': A string of the business idea.\n",
    "                  - 'static_features': A list or array of static features.\n",
    "                  - 'targets': A list or array of target stock performance values.\n",
    "            text_encoder: The Sentence-BERT model for text encoding.\n",
    "            forecast_steps: Number of months to forecast.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.text_encoder = text_encoder\n",
    "        self.forecast_steps = forecast_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        idea_text = item['idea_text']\n",
    "        static_features = torch.tensor(item['static_features'], dtype=torch.float32)\n",
    "\n",
    "        # Encode the text description using Sentence-BERT\n",
    "        text_embedding = torch.tensor(self.text_encoder.encode(idea_text), dtype=torch.float32)\n",
    "\n",
    "        # Targets (12-month forecast sequence)\n",
    "        targets = torch.tensor(item['targets'][:self.forecast_steps], dtype=torch.float32)\n",
    "\n",
    "        return text_embedding, static_features, targets\n"
   ],
   "id": "7dec583cb6041997",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training the model\n",
    "Here we import the model and set it up for training"
   ],
   "id": "9557115e5f9083c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-12T08:34:28.610440Z",
     "start_time": "2024-11-12T08:34:25.799591Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from Model.HybridStockPredictionModel import StockPerformancePredictionModel\n",
    " \n",
    "train_data = [] # import from csv\n",
    "model = StockPerformancePredictionModel(text_embedding_dim=384, static_feature_dim=10, hidden_dim=128, forecast_steps=12)\n",
    "\n",
    "# Assuming 'train_data' is a list of dictionaries with the required keys\n",
    "dataset = StockPerformanceDataset(train_data, model.text_encoder)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Assuming 'train_data' is a list of dictionaries with the required keys\u001B[39;00m\n\u001B[1;32m      8\u001B[0m dataset \u001B[38;5;241m=\u001B[39m StockPerformanceDataset(train_data, model\u001B[38;5;241m.\u001B[39mtext_encoder)\n\u001B[0;32m----> 9\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m \u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/Dataset/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:376\u001B[0m, in \u001B[0;36mDataLoader.__init__\u001B[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# map-style\u001B[39;00m\n\u001B[1;32m    375\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shuffle:\n\u001B[0;32m--> 376\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m \u001B[43mRandomSampler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    378\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m SequentialSampler(dataset)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/AIR/AIR-Project/Dataset/env/lib/python3.11/site-packages/torch/utils/data/sampler.py:164\u001B[0m, in \u001B[0;36mRandomSampler.__init__\u001B[0;34m(self, data_source, replacement, num_samples, generator)\u001B[0m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreplacement should be a boolean value, but got replacement=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplacement\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    161\u001B[0m     )\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    166\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### We can also create a a custom loss function and an optimizer",
   "id": "cc8c8d4c3dfd6e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "id": "c4f1bc95622e999e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also possible create custom loss functions:",
   "id": "d00b397e0d5f09c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example for mixed loss\n",
    "regression_loss_fn = nn.MSELoss()\n",
    "classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Suppose the last indicator is categorical\n",
    "def custom_loss(predictions, targets):\n",
    "    # Separate out indicators\n",
    "    reg_targets, class_targets = targets[:, :, :-1], targets[:, :, -1].long()\n",
    "    reg_preds, class_preds = predictions[:, :, :-1], predictions[:, :, -1]\n",
    "\n",
    "    # Compute losses for each part\n",
    "    reg_loss = regression_loss_fn(reg_preds, reg_targets)\n",
    "    class_loss = classification_loss_fn(class_preds.view(-1, class_preds.size(-1)), class_targets.view(-1))\n",
    "\n",
    "    # Combine losses\n",
    "    return reg_loss + class_loss\n"
   ],
   "id": "81795ae578e9baec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Start of training\n",
    "\n",
    "Here we provide a custom training loop function:"
   ],
   "id": "6a536c54deb3fb20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for text_embedding, static_features, targets in train_loader:\n",
    "        # Move data to device (CPU or GPU)\n",
    "        text_embedding = text_embedding.to(device)\n",
    "        static_features = static_features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(text_embedding, static_features)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss for monitoring\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    average_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_epoch_loss:.4f}')\n"
   ],
   "id": "490af956f3c83c5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "After training we are going to evaluate our model:"
   ],
   "id": "5998d18e19fbe2a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'val_data' is formatted like 'train_data'\n",
    "val_dataset = StockPerformanceDataset(val_data, model.text_encoder)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for text_embedding, static_features, targets in val_loader:\n",
    "            text_embedding = text_embedding.to(device)\n",
    "            static_features = static_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            predictions = model(text_embedding, static_features)\n",
    "            loss = criterion(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# Add early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step (from above)\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for text_embedding, static_features, targets in train_loader:\n",
    "        text_embedding, static_features, targets = (\n",
    "            text_embedding.to(device),\n",
    "            static_features.to(device),\n",
    "            targets.to(device)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text_embedding, static_features)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation step\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping condition\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0  # Reset patience counter\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n"
   ],
   "id": "b65b87305950eae5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
