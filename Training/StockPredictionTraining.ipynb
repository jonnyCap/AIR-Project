{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 10006457,
     "sourceType": "datasetVersion",
     "datasetId": 6159769
    },
    {
     "sourceId": 10006507,
     "sourceType": "datasetVersion",
     "datasetId": 6159808
    },
    {
     "sourceId": 10008951,
     "sourceType": "datasetVersion",
     "datasetId": 6161623
    },
    {
     "sourceId": 10039467,
     "sourceType": "datasetVersion",
     "datasetId": 6184272
    },
    {
     "sourceId": 10040718,
     "sourceType": "datasetVersion",
     "datasetId": 6185176
    },
    {
     "sourceId": 185555,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 155090,
     "modelId": 177567
    }
   ],
   "dockerImageVersionId": 30787,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "9e59e4e80ee01ffd",
   "cell_type": "markdown",
   "source": "# Hybrid Stock Prediction Model Training\n\nIn the \"HybridStockPredictionModel\" notebook we created our model that can be used to make efficient stock prediction for new business ideas.\n\n\n\nAt first we create our Dataset class that will be used to train the model:\n\n\n\n\n\n# Load Libraries and Set Up Dependencies",
   "metadata": {}
  },
  {
   "id": "51ffd50e792a2102",
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "tqdm._instances.clear()  # Clear any existing progress bars\n",
    "tqdm.pandas(disable=True)  # Disable tqdm globally\n",
    "tqdm.disable = True\n",
    "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
    "import sys\n",
    "\n",
    "# For Kaggle:\n",
    "# sys.path.append('/kaggle/input/stockpredictionmodel/pytorch/default/7')\n",
    "# For IDE:\n",
    "sys.path.append('/home/kai/Documents/AIR-Project/Model')\n",
    "from HybridStockPredictionModel import StockPerformancePredictionModel\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:50:50.664664Z",
     "iopub.execute_input": "2024-12-02T09:50:50.665091Z",
     "iopub.status.idle": "2024-12-02T09:50:50.672621Z",
     "shell.execute_reply.started": "2024-12-02T09:50:50.665044Z",
     "shell.execute_reply": "2024-12-02T09:50:50.671611Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "4b42d729-a352-4029-bbab-790258cae14a",
   "cell_type": "code",
   "source": "# import joblib\n\n# hidden_dim = 128  # Example hidden size\n# historical_scaler = joblib.load(\"/kaggle/input/scaler/historical_scaler.pkl\")\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model_save_path = \"/kaggle/input/trainedmodel2/best_model.pth\"\n# model = StockPerformancePredictionModel(3, 12, hidden_dim, 12)\n\n# # Load the model on the device where you want to use it\n# model.load_state_dict(torch.load(model_save_path, map_location=device))\n# model.to(device)\n\n# model.eval()\n\n# with torch.no_grad():\n#     predictions = model(\n#         idea=[\"Agilent Technologies, Inc. provides application focused solutions to the life sciences, diagnostics, and applied chemical markets worldwide. The company operates in three segments: Life Sciences and Applied Markets, Diagnostics and Genomics, and Agilent CrossLab. The Life Sciences and Applied Markets segment offers liquid chromatography systems and components; liquid chromatography mass spectrometry systems; gas chromatography systems and components; gas chromatography mass spectrometry systems; inductively coupled plasma mass spectrometry instruments; atomic absorption instruments; microwave plasma-atomic emission spectrometry instruments; inductively coupled plasma optical emission spectrometry instruments; raman spectroscopy; cell analysis plate based assays; flow cytometer; real-time cell analyzer; cell imaging systems; microplate reader; laboratory software; information management and analytics; laboratory automation and robotic systems; dissolution testing; and vacuum pumps, and measurement technologies. The Diagnostics and Genomics segment focuses on genomics, nucleic acid contract manufacturing and research and development, pathology, companion diagnostics, reagent partnership, and biomolecular analysis businesses. The Agilent CrossLab segment provides GC and LC columns, sample preparation products, custom chemistries, and laboratory instrument supplies; and offers services portfolio, including repairs, parts, maintenance, installations, training, compliance support, software as a service, asset management, and consulting services. The company markets its products through direct sales, distributors, resellers, manufacturer's representatives, and electronic commerce. Agilent Technologies, Inc. was incorporated in 1999 and is headquartered in Santa Clara, California.\"],\n#         use_auxiliary_inputs=False\n#     )\n\n# print(predictions)\n# nan_tensor = torch.full((1, 12), float('nan'), device=predictions.device) # Expand zeros_tensor to 2D (1, 12)\n# right = [0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962,0.0002258687415962]\n# right_tensor = torch.tensor(right, device=predictions.device)\n# right_tensor = right_tensor.unsqueeze(0)\n\n# # Now concatenate along dim=1\n# stock_market_prediction = torch.cat((right_tensor, predictions), dim=1)\n\n# # Ensure the tensor is detached from the computation graph before converting to numpy\n# predictions_np = stock_market_prediction.detach().cpu().numpy()\n# prediction_historical = historical_scaler.inverse_transform(predictions_np)\n# print(prediction_historical[0])\n\n# # Assuming target_series and pred_series are created here...\n# target_series = pd.Series(prediction_historical[0])\n\n# sns.set_style(\"darkgrid\")\n# fig = plt.figure(figsize=(10, 6))  # Add figure size for clarity\n\n# plt.subplot(1, 2, 1)\n# ax = sns.lineplot(x=target_series.index, y=target_series.values, label=\"Data\", color='royalblue')\n# ax.set_title('Stock price', size=14, fontweight='bold')\n# ax.set_xlabel(\"Days\", size=14)\n# ax.set_ylabel(\"Cost (USD)\", size=14)\n\n# plt.show()  # Ensure the plot appears\n\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:50:50.674209Z",
     "iopub.execute_input": "2024-12-02T09:50:50.674478Z",
     "iopub.status.idle": "2024-12-02T09:50:50.688279Z",
     "shell.execute_reply.started": "2024-12-02T09:50:50.674454Z",
     "shell.execute_reply": "2024-12-02T09:50:50.687532Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "794309d3bc005e40",
   "cell_type": "markdown",
   "source": "# Define the PyTorch Dataset",
   "metadata": {}
  },
  {
   "id": "994f632eb01d453c",
   "cell_type": "code",
   "source": "class StockDataset(Dataset):\n    def __init__(self, ideas, static_features, historical_data, targets):\n        self.ideas = ideas\n        self.static_features = torch.tensor(static_features, dtype=torch.float32)\n        self.historical_data = torch.tensor(historical_data, dtype=torch.float32)\n        self.targets = torch.tensor(targets, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        # Return the idea (text), static features, historical data, and the target\n        return self.ideas[idx], self.static_features[idx], self.historical_data[idx], self.targets[idx]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:50:50.689508Z",
     "iopub.execute_input": "2024-12-02T09:50:50.689772Z",
     "iopub.status.idle": "2024-12-02T09:50:50.703272Z",
     "shell.execute_reply.started": "2024-12-02T09:50:50.689748Z",
     "shell.execute_reply": "2024-12-02T09:50:50.702575Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "bc548ced91f569bf",
   "cell_type": "markdown",
   "source": "# Load and Preprocess the Dataset",
   "metadata": {}
  },
  {
   "id": "7dec583cb6041997",
   "cell_type": "code",
   "source": "# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/realstockdataset/real_company_stock_dataset.csv\")\n\n# Define columns\nidea_column = \"business_description\"\nstatic_feature_columns = [\"market_size\", \"investment\", \"team_strength\"]\nhistorical_columns = [col for col in df.columns if col.startswith(\"month_\")]\n\n# Prepare your features and target\nideas = df[idea_column].iloc[327:].reset_index(drop=True).values\nstatic_features = df[static_feature_columns].iloc[327:,:].reset_index(drop=True).values \n\n# Work with historical columns as DataFrame first\nhistorical_df = df[historical_columns]\n\n# Split the data: all rows but split columns\ntrain_data = historical_df.iloc[327:, :12].reset_index(drop=True).values \ntest_data = historical_df.iloc[327:, 12:24].reset_index(drop=True).values \n\n# Initialize new scalers\nstatic_scaler = StandardScaler()  # or MinMaxScaler()\nhistorical_scaler = MinMaxScaler(feature_range=(0, 1))\n\n# Fit scalers on the entire dataset\ntrain_data = historical_scaler.fit_transform(train_data)  # All rows, first 12 columns\ntest_data = historical_scaler.transform(test_data)  # All rows, first 12 columns\nstatic_features = static_scaler.fit_transform(static_features)  # All rows, first 12 columns\n\n# Convert Nan to Zero\ntrain_data[np.isnan(train_data)] = 0\ntest_data[np.isnan(test_data)] = 0\nstatic_features[np.isnan(static_features)] = 0\n\n# Train-test split\nideas_train, ideas_test, static_train, static_test, hist_train, hist_test, target_train, target_test = train_test_split(\n    ideas, static_features, train_data, test_data, test_size=0.2, random_state=42\n)\n\n\n# Denormalize the values similarly\npredictions_historical = historical_scaler.inverse_transform(test_data)\ntargets_historical = historical_scaler.inverse_transform(train_data)\n\nplt.figure(figsize=(10, 6))\n\n# Loop through predictions and targets\nfor i in range(len(predictions_historical)):\n    plt.plot(targets_historical[i], linestyle='--', marker='x', color='red')  # Plot targets\n\n# Add labels, title, and legend\nplt.xlabel('Time Steps')  # Or whatever your x-axis represents\nplt.ylabel('Value')  # Or whatever your y-axis represents\nplt.title('First 12 Months Training-Data')\nplt.legend()  # Add legend to differentiate between predictions and targets\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\nplt.figure(figsize=(10, 6))\n# Loop through predictions and targets\nfor i in range(len(predictions_historical)):\n    plt.plot(predictions_historical[i], linestyle='-', marker='o', color='blue')  # Plot predictions\n\n# Add labels, title, and legend\nplt.xlabel('Time Steps')  # Or whatever your x-axis represents\nplt.ylabel('Value')  # Or whatever your y-axis represents\nplt.title('12 to 24 Months Testing-Data')\nplt.legend()  # Add legend to differentiate between predictions and targets\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n# Combine all inputs for the training dataset\n# Assuming each row of historical data matches with static features and idea embeddings\ntrain_dataset = StockDataset(\n    ideas=ideas_train,\n    static_features=static_train,\n    historical_data=hist_train,\n    targets=target_train)\n\ntest_dataset = StockDataset(\n    historical_data=hist_test,                    # Historical data\n    ideas=ideas_test,                             # Idea embeddings\n    static_features=static_test,                  # Static features\n    targets=target_test                           # Targets\n)\n\nprint(\"Dataset updated successfully!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:50:50.772753Z",
     "iopub.execute_input": "2024-12-02T09:50:50.773124Z",
     "iopub.status.idle": "2024-12-02T09:51:09.419311Z",
     "shell.execute_reply.started": "2024-12-02T09:50:50.773085Z",
     "shell.execute_reply": "2024-12-02T09:51:09.418418Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "9557115e5f9083c4",
   "cell_type": "markdown",
   "source": "### Training the model\n\nHere we import the model and set it up for training",
   "metadata": {}
  },
  {
   "id": "initial_id",
   "cell_type": "code",
   "source": "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=100):\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    writer = SummaryWriter(log_dir=\"logs\")  # TensorBoard writer\n    train_loss_statistics = []\n    test_loss_statistics = []\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        \n        all_predictions = []\n        all_targets = []\n        \n        for ideas, static_features, historical_data, targets in train_loader:\n            \n            static_features, historical_data, targets = (\n                static_features.to(device),\n                historical_data.to(device),\n                targets.to(device),\n            )\n            \n            optimizer.zero_grad()\n            \n            predictions = model(\n                idea=ideas,\n                static_features=static_features,\n                historical_data=historical_data,\n                use_auxiliary_inputs=True,\n                predict_autoregressively=False,\n            )\n            \n            loss = criterion(predictions, targets)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # More lenient gradient clipping\n            optimizer.step()\n            train_loss += loss.item()\n\n            all_predictions.append(predictions.cpu().detach().numpy())\n            all_targets.append(targets.cpu().detach().numpy())\n\n        # Flatten the lists to make evaluation easier\n        all_predictions = np.concatenate(all_predictions, axis=0)\n        all_targets = np.concatenate(all_targets, axis=0)\n    \n        # Denormalize the values similarly\n        predictions_historical = historical_scaler.inverse_transform(all_predictions)\n        targets_historical = historical_scaler.inverse_transform(all_targets)\n    \n        plt.figure(figsize=(10, 6))\n    \n        # Loop through predictions and targets\n        for i in range(len(predictions_historical)):\n            plt.plot(predictions_historical[i], linestyle='-', marker='o', color='blue')  # Plot predictions\n            plt.plot(targets_historical[i], linestyle='--', marker='x', color='red')  # Plot targets\n        \n        # Add labels, title, and legend\n        plt.xlabel('Time Steps')  # Or whatever your x-axis represents\n        plt.ylabel('Value')  # Or whatever your y-axis represents\n        plt.title('Predictions vs Targets')\n        plt.legend()  # Add legend to differentiate between predictions and targets\n        plt.grid(True)\n        \n        # Show the plot\n        plt.show()\n        \n\n        # Validation phase\n        val_loss, _, _, _ = evaluate(model, val_loader, device, criterion)\n        scheduler.step(val_loss)\n\n        # Logging\n        train_loss /= len(train_loader)\n        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n        train_loss_statistics.append(train_loss)\n        test_loss_statistics.append(val_loss)\n        writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss}, epoch)\n\n    writer.close()\n\n    \n    sns.set_style(\"darkgrid\")\n    fig = plt.figure(figsize=(16, 6))  # Set figure size here\n    ax = plt.subplot(1, 2, 2)\n    \n    # Create x-axis for epochs\n    epochs = range(1, len(train_loss_statistics) + 1)\n    \n    # Plot training loss\n    sns.lineplot(x=epochs, y=train_loss_statistics, label=\"Train Loss\", color='royalblue', ax=ax)\n    \n    # Plot testing loss\n    sns.lineplot(x=epochs, y=test_loss_statistics, label=\"Test Loss\", color='tomato', ax=ax)\n    \n    # Customize labels and title\n    ax.set_xlabel(\"Epoch\", size=14)\n    ax.set_ylabel(\"Loss\", size=14)\n    ax.set_title(\"Training Loss / Testing Loss\", size=14, fontweight='bold')\n    \n    plt.tight_layout()  # Adjust layout to prevent overlap\n    plt.show()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:51:09.421194Z",
     "iopub.execute_input": "2024-12-02T09:51:09.421481Z",
     "iopub.status.idle": "2024-12-02T09:51:09.434678Z",
     "shell.execute_reply.started": "2024-12-02T09:51:09.421453Z",
     "shell.execute_reply": "2024-12-02T09:51:09.433785Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "cc8c8d4c3dfd6e4",
   "cell_type": "markdown",
   "source": "#### We can also create a a custom loss function and an optimizer",
   "metadata": {}
  },
  {
   "id": "d00b397e0d5f09c6",
   "cell_type": "markdown",
   "source": "You can also possible create custom loss functions:",
   "metadata": {}
  },
  {
   "id": "81795ae578e9baec",
   "cell_type": "code",
   "source": "class TemporalLoss(nn.Module):\n    def __init__(self, mse_weight=0.6, diversity_weight=0.1, lambda_smooth=0.2, lambda_penalty=0.1):\n        super(TemporalLoss, self).__init__()\n        self.mse_weight = mse_weight\n        self.diversity_weight = diversity_weight\n        self.lambda_smooth = lambda_smooth\n        self.lambda_penalty = lambda_penalty\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, predictions, targets):\n        # Ensure dimensions match\n        if predictions.dim() == 2:\n            predictions = predictions.unsqueeze(-1)\n        if targets.dim() == 2:\n            targets = targets.unsqueeze(-1)\n\n        # Base MSE loss\n        mse = self.mse_loss(predictions, targets)\n\n        # Diversity encouragement\n        diversity_penalty = -torch.std(predictions)\n\n        # Temporal smoothness loss\n        temporal_diff = (predictions[:, 1:, :] - predictions[:, :-1, :]) / (torch.abs(predictions[:, :-1, :]) + 1e-6)\n        smoothness_loss = torch.mean(temporal_diff**2)\n\n        # Negative prediction penalty\n        negative_penalty = torch.sum(torch.clamp(-predictions, min=0))\n\n        # Combine all loss components\n        combined_loss = (\n            self.mse_weight * mse +\n            self.diversity_weight * diversity_penalty +\n            self.lambda_smooth * smoothness_loss +\n            self.lambda_penalty * negative_penalty\n        )\n\n        return combined_loss\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:51:09.435699Z",
     "iopub.execute_input": "2024-12-02T09:51:09.436060Z",
     "iopub.status.idle": "2024-12-02T09:51:09.449146Z",
     "shell.execute_reply.started": "2024-12-02T09:51:09.436020Z",
     "shell.execute_reply": "2024-12-02T09:51:09.448164Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "5998d18e19fbe2a2",
   "cell_type": "markdown",
   "source": "### Evaluation\n\nAfter training we are going to evaluate our model:",
   "metadata": {}
  },
  {
   "id": "b65b87305950eae5",
   "cell_type": "code",
   "source": "import joblib\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport seaborn as sns\n\ndef evaluate(model, val_loader, device, criterion):\n    model.eval()  # Set the model to evaluation mode\n\n    val_loss = 0.0\n    all_predictions = []\n    all_targets = []\n    correct = 0\n    total = 0\n\n    with torch.no_grad():  # No need to track gradients during evaluation\n        for ideas_batch, static_batch, historical_batch, target_batch in val_loader:\n            # Move data to device\n            static_batch = static_batch.to(device)\n            historical_batch = historical_batch.to(device)\n            target_batch = target_batch.to(device)\n    \n            # Get predictions\n            predictions = model(idea=ideas_batch, use_auxiliary_inputs=False)\n\n            # print(\"Training predictions Shape:\", predictions.shape)\n            # print(\"Testing target_batch Shape:\", target_batch.shape) \n    \n            # Compute temporal loss\n            loss = criterion(predictions, target_batch)\n            val_loss += loss.item()\n            \n            all_predictions.append(predictions.cpu().numpy())\n            all_targets.append(target_batch.cpu().numpy())\n\n                \n    # Average validation loss\n    val_loss /= len(val_loader)\n    print(f\"Validation Loss: {val_loss}\")\n\n    # Flatten the lists to make evaluation easier\n    all_predictions = np.concatenate(all_predictions, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n\n    # Denormalize the values similarly\n    predictions_historical = historical_scaler.inverse_transform(all_predictions)\n    targets_historical = historical_scaler.inverse_transform(all_targets)\n\n    plt.figure(figsize=(10, 6))\n\n    # Loop through predictions and targets\n    for i in range(len(predictions_historical)):\n        plt.plot(predictions_historical[i], linestyle='-', marker='o', color='blue')  # Plot predictions\n        plt.plot(targets_historical[i], linestyle='--', marker='x', color='red')  # Plot targets\n    \n    # Add labels, title, and legend\n    plt.xlabel('Time Steps')  # Or whatever your x-axis represents\n    plt.ylabel('Value')  # Or whatever your y-axis represents\n    plt.title('Predictions vs Targets')\n    plt.legend()  # Add legend to differentiate between predictions and targets\n    plt.grid(True)\n    \n    # Show the plot\n    plt.show()\n\n    # Calculate MSE and MAE for historical features\n    mse_historical = mean_squared_error(predictions_historical, targets_historical)\n    mae_historical = mean_absolute_error(targets_historical, predictions_historical)\n    r2_historical = r2_score(targets_historical, predictions_historical)\n\n    # Print metrics for each section\n    print(f\"MSE for historical features: {mse_historical}\")\n    print(f\"MAE for historical features: {mae_historical}\")\n    print(f\"RÂ² for historical features: {r2_historical}\")\n\n    # import matplotlib.pyplot as plt\n\n    # plt.scatter(all_targets, all_predictions, alpha=0.5)\n    # plt.xlabel(\"Actual Values\")\n    # plt.ylabel(\"Predicted Values\")\n    # plt.title(\"Actual vs. Predicted\")\n    # plt.show()\n    \n    # residuals = all_targets - all_predictions\n    # plt.hist(residuals, bins=30, edgecolor='k')\n    # plt.xlabel(\"Residual\")\n    # plt.ylabel(\"Frequency\")\n    # plt.title(\"Residual Distribution\")\n    # plt.show()\n\n\n    return val_loss, mse_historical, mae_historical, r2_historical\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:51:09.450865Z",
     "iopub.execute_input": "2024-12-02T09:51:09.451222Z",
     "iopub.status.idle": "2024-12-02T09:51:09.464524Z",
     "shell.execute_reply.started": "2024-12-02T09:51:09.451197Z",
     "shell.execute_reply": "2024-12-02T09:51:09.463603Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "2aafad460b392c3",
   "cell_type": "markdown",
   "source": "# Integration with Training Loop",
   "metadata": {}
  },
  {
   "id": "3b5c679b090c90a8",
   "cell_type": "code",
   "source": "# Run Training and Evaluation\n# Model initialization\nstatic_feature_dim = static_features.shape[1]\nhistorical_dim = 12\nhidden_dim = 128\nforecast_steps = 12\nmodel_save_path = \"/kaggle/input/trainedmodel1/best_model.pth\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = xm.xla_device()\n\nmodel = StockPerformancePredictionModel(static_feature_dim, historical_dim, hidden_dim, forecast_steps).to(device)\n# model = StockPerformancePredictionModel(static_feature_dim, historical_dim, hidden_dim, forecast_steps)\n# model.load_state_dict(torch.load(model_save_path, weights_only=True))\n# model.to(device)\n\n# Create DataLoader for training and validation\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Loss and optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\ntemporal_loss = TemporalLoss(lambda_smooth=0.1)\n\n# import warnings\n# warnings.filterwarnings(\"ignore\", message=\"Loading widget...\")\n\nprint(\"Start Training\")\ntrain_model(model, train_loader, test_loader, temporal_loss, optimizer, device, epochs=50)\n\nos.makedirs(\"models\", exist_ok=True)\ntorch.save(model.state_dict(), \"models/best_model.pth\")\n\nprint(\"Saved Model in Outputs/Models\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T09:51:09.465683Z",
     "iopub.execute_input": "2024-12-02T09:51:09.466035Z",
     "iopub.status.idle": "2024-12-02T10:04:17.722425Z",
     "shell.execute_reply.started": "2024-12-02T09:51:09.465999Z",
     "shell.execute_reply": "2024-12-02T10:04:17.720966Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
