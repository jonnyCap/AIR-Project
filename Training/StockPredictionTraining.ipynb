{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hybrid Stock Prediction Model Training\n",
    "In the \"HybridStockPredictionModel\" notebook we created our model that can be used to make efficient stock prediction for new business ideas.\n",
    "\n",
    "At first we create our Dataset class that will be used to train the model:\n",
    "\n",
    "\n",
    "# Load Libraries and Set Up Dependencies"
   ],
   "id": "9e59e4e80ee01ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:54:45.913319Z",
     "start_time": "2024-11-24T16:54:42.698716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ],
   "id": "51ffd50e792a2102",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and Preprocess the Dataset",
   "id": "bc548ced91f569bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:57:30.518779Z",
     "start_time": "2024-11-24T16:54:45.923217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../Dataset/Data/normalized_real_company_stock_dataset.csv\")\n",
    "\n",
    "# Define columns\n",
    "idea_column = \"business_description\"\n",
    "static_feature_columns = [\"market_size\", \"investment\", \"team_strength\"]\n",
    "historical_columns = [col for col in df.columns if col.startswith(\"month_\")]\n",
    "\n",
    "# Define the target as stock performance for the next 6 months\n",
    "target_columns = historical_columns[-6:]  # Last 6 months of performance\n",
    "forecast_steps = 6\n",
    "\n",
    "# Prepare your features and target\n",
    "ideas = df[idea_column].values\n",
    "static_features = df[static_feature_columns].values\n",
    "historical_data = df[historical_columns].values\n",
    "targets = df[target_columns].values\n",
    "\n",
    "# Scale static features\n",
    "scaler_static = StandardScaler()\n",
    "static_features = scaler_static.fit_transform(static_features)\n",
    "\n",
    "# Train-test split\n",
    "ideas_train, ideas_val, static_train, static_val, hist_train, hist_val, y_train, y_val = train_test_split(\n",
    "    ideas, static_features, historical_data, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the ideas to embeddings using SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model (this will map each idea to a vector of length 384)\n",
    "text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert the text ideas into embeddings (numerical vectors)\n",
    "ideas_train_embeddings = text_encoder.encode(ideas_train, convert_to_numpy=True)\n",
    "ideas_val_embeddings = text_encoder.encode(ideas_val, convert_to_numpy=True)\n",
    "\n",
    "# Now convert these embeddings into torch tensors\n",
    "ideas_train_tensor = torch.tensor(ideas_train_embeddings, dtype=torch.float32)\n",
    "ideas_val_tensor = torch.tensor(ideas_val_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Convert static features and target variables into torch tensors\n",
    "static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
    "static_val_tensor = torch.tensor(static_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)"
   ],
   "id": "7dec583cb6041997",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2ddb54475a0454f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define the PyTorch Dataset",
   "id": "794309d3bc005e40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:57:30.665847Z",
     "start_time": "2024-11-24T16:57:30.661787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, ideas, static_features, historical_data, targets):\n",
    "        self.ideas = ideas\n",
    "        self.static_features = torch.tensor(static_features, dtype=torch.float32)\n",
    "        self.historical_data = torch.tensor(historical_data, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idea (text), static features, historical data, and the target\n",
    "        return self.ideas[idx], self.static_features[idx], self.historical_data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "train_dataset = StockDataset(ideas=ideas_train,\n",
    "                             static_features=static_train,\n",
    "                             historical_data=hist_train,\n",
    "                             targets=y_train)\n",
    "\n",
    "val_dataset = StockDataset(ideas=ideas_val,\n",
    "                           static_features=static_val,\n",
    "                           historical_data=hist_val,\n",
    "                           targets=y_val)\n"
   ],
   "id": "994f632eb01d453c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "458028ca10e6bfe1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "72271fe140850f6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training the model\n",
    "Here we import the model and set it up for training"
   ],
   "id": "9557115e5f9083c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T17:08:19.368327Z",
     "start_time": "2024-11-24T17:08:19.363594Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import Model.HybridStockPredictionModel\n",
    "from Model.HybridStockPredictionModel import StockPerformancePredictionModel\n",
    "\n",
    "# Model initialization\n",
    "static_feature_dim = static_features.shape[1]\n",
    "historical_dim = historical_data.shape[1]\n",
    "hidden_dim = 128  # Example hidden size\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10, step_size=5, gamma=0.1):\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for ideas, static_features, historical_data, targets in train_loader:\n",
    "            # Check for invalid values in data\n",
    "            assert not torch.isnan(static_features).any(), \"Static features contain NaN\"\n",
    "            assert not torch.isinf(static_features).any(), \"Static features contain Inf\"\n",
    "            assert not torch.isnan(historical_data).any(), \"Historical data contains NaN\"\n",
    "            assert not torch.isinf(historical_data).any(), \"Historical data contains Inf\"\n",
    "\n",
    "            # Move to device\n",
    "            static_features, historical_data, targets = (\n",
    "                static_features.to(device),\n",
    "                historical_data.to(device),\n",
    "                targets.to(device),\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(\n",
    "                idea=ideas,\n",
    "                static_features=static_features,\n",
    "                historical_data=historical_data,\n",
    "                use_auxiliary_inputs=True,\n",
    "                predict_autoregressively=False\n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions.squeeze(), targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluate the model after training for the epoch\n",
    "        val_loss = evaluate(model, val_loader, device, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### We can also create a a custom loss function and an optimizer",
   "id": "cc8c8d4c3dfd6e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also possible create custom loss functions:",
   "id": "d00b397e0d5f09c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:57:33.854939Z",
     "start_time": "2024-11-24T16:57:33.852634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suppose the last indicator is categorical\n",
    "def custom_loss(predictions, targets, model, lambda_reg=0.01):\n",
    "    # Mean Squared Error Loss\n",
    "    mse_loss = F.mse_loss(predictions, targets)\n",
    "\n",
    "    # L2 regularization (sum of squared weights of the model parameters)\n",
    "    l2_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l2_loss += torch.sum(param ** 2)\n",
    "\n",
    "    # Combine the two losses\n",
    "    total_loss = mse_loss + lambda_reg * l2_loss\n",
    "    return total_loss\n"
   ],
   "id": "81795ae578e9baec",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6a536c54deb3fb20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "After training we are going to evaluate our model:"
   ],
   "id": "5998d18e19fbe2a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:57:33.893335Z",
     "start_time": "2024-11-24T16:57:33.889805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, val_loader, device, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for ideas_batch, static_batch, historical_batch, target_batch in val_loader:\n",
    "\n",
    "\n",
    "            # Move data to device\n",
    "            static_batch = static_batch.to(device)\n",
    "            historical_batch = historical_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = model(ideas_batch, static_features=static_batch, historical_data=historical_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, target_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect all predictions and targets for evaluation\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(target_batch.cpu().numpy())\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Flatten the lists to make evaluation easier\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'RÂ²: {r2:.4f}')\n",
    "\n",
    "    return avg_val_loss, mse, mae, r2\n"
   ],
   "id": "b65b87305950eae5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Integration with Training Loop",
   "id": "2aafad460b392c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T17:09:17.519928Z",
     "start_time": "2024-11-24T17:08:22.751580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run Training and Evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Static: {static_feature_dim}\")\n",
    "print(f\"Historical: {historical_dim}\")\n",
    "model = StockPerformancePredictionModel(static_feature_dim, historical_dim, hidden_dim, forecast_steps).to(device)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10)\n"
   ],
   "id": "3b5c679b090c90a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static: 3\n",
      "Historical: 24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[1;32m     16\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[13], line 36\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs, step_size, gamma)\u001B[0m\n\u001B[1;32m     29\u001B[0m static_features, historical_data, targets \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     30\u001B[0m     static_features\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[1;32m     31\u001B[0m     historical_data\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[1;32m     32\u001B[0m     targets\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[1;32m     33\u001B[0m )\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43midea\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mideas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstatic_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstatic_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistorical_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistorical_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auxiliary_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpredict_autoregressively\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m     42\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[1;32m     45\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(predictions\u001B[38;5;241m.\u001B[39msqueeze(), targets)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/AIR-Project/Model/HybridStockPredictionModel.py:53\u001B[0m, in \u001B[0;36mStockPerformancePredictionModel.forward\u001B[0;34m(self, idea, static_features, historical_data, use_auxiliary_inputs, predict_autoregressively)\u001B[0m\n\u001B[1;32m     50\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters())\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# Text embedding\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m encoded_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43midea\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# print(type(encoded_output))  # Check if it's a NumPy array\u001B[39;00m\n\u001B[1;32m     55\u001B[0m text_embedding \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(encoded_output)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:591\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatches\u001B[39m\u001B[38;5;124m\"\u001B[39m, disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n\u001B[1;32m    590\u001B[0m     sentences_batch \u001B[38;5;241m=\u001B[39m sentences_sorted[start_index : start_index \u001B[38;5;241m+\u001B[39m batch_size]\n\u001B[0;32m--> 591\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    592\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    593\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1050\u001B[0m, in \u001B[0;36mSentenceTransformer.tokenize\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, texts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Tensor]:\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;124;03m    Tokenizes the texts.\u001B[39;00m\n\u001B[1;32m   1042\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;124;03m            \"attention_mask\", and \"token_type_ids\".\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1050\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_first_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:449\u001B[0m, in \u001B[0;36mTransformer.tokenize\u001B[0;34m(self, texts, padding)\u001B[0m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case:\n\u001B[1;32m    446\u001B[0m     to_tokenize \u001B[38;5;241m=\u001B[39m [[s\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m col] \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m to_tokenize]\n\u001B[1;32m    448\u001B[0m output\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mto_tokenize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlongest_first\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m )\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3021\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   3019\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   3020\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 3021\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3023\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3109\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3104\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3105\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3106\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3107\u001B[0m         )\n\u001B[1;32m   3108\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 3109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3111\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3124\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3127\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3128\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3129\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3130\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   3132\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   3133\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3151\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3152\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3311\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3301\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   3302\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   3303\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3304\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3308\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3309\u001B[0m )\n\u001B[0;32m-> 3311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3313\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3314\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3315\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3317\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3319\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3320\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3321\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3323\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3325\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3326\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3327\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3328\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3329\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3330\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3331\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:577\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[0m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_ids \u001B[38;5;129;01min\u001B[39;00m sanitized_tokens[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    576\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001B[0;32m--> 577\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mBatchEncoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43msanitized_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msanitized_encodings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:240\u001B[0m, in \u001B[0;36mBatchEncoding.__init__\u001B[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001B[0m\n\u001B[1;32m    236\u001B[0m     n_sequences \u001B[38;5;241m=\u001B[39m encoding[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mn_sequences\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_sequences \u001B[38;5;241m=\u001B[39m n_sequences\n\u001B[0;32m--> 240\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprepend_batch_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepend_batch_axis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:776\u001B[0m, in \u001B[0;36mBatchEncoding.convert_to_tensors\u001B[0;34m(self, tensor_type, prepend_batch_axis)\u001B[0m\n\u001B[1;32m    773\u001B[0m     value \u001B[38;5;241m=\u001B[39m [value]\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor(value):\n\u001B[0;32m--> 776\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m \u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    778\u001B[0m     \u001B[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001B[39;00m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;66;03m# # at-least2d\u001B[39;00m\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;66;03m# if tensor.ndim > 2:\u001B[39;00m\n\u001B[1;32m    781\u001B[0m     \u001B[38;5;66;03m#     tensor = tensor.squeeze(0)\u001B[39;00m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;66;03m# elif tensor.ndim < 2:\u001B[39;00m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;66;03m#     tensor = tensor[None, :]\u001B[39;00m\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;28mself\u001B[39m[key] \u001B[38;5;241m=\u001B[39m tensor\n",
      "File \u001B[0;32m~/Documents/AIR-Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:738\u001B[0m, in \u001B[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001B[0;34m(value, dtype)\u001B[0m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value[\u001B[38;5;241m0\u001B[39m], np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m    737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(np\u001B[38;5;241m.\u001B[39marray(value))\n\u001B[0;32m--> 738\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
