{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10006457,"sourceType":"datasetVersion","datasetId":6159769},{"sourceId":10006507,"sourceType":"datasetVersion","datasetId":6159808},{"sourceId":10008951,"sourceType":"datasetVersion","datasetId":6161623},{"sourceId":10012844,"sourceType":"datasetVersion","datasetId":6164501},{"sourceId":10012954,"sourceType":"datasetVersion","datasetId":6164589},{"sourceId":177849,"sourceType":"modelInstanceVersion","modelInstanceId":151509,"modelId":173964}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9e59e4e80ee01ffd","cell_type":"markdown","source":"# Hybrid Stock Prediction Model Training\n\nIn the \"HybridStockPredictionModel\" notebook we created our model that can be used to make efficient stock prediction for new business ideas.\n\n\n\nAt first we create our Dataset class that will be used to train the model:\n\n\n\n\n\n# Load Libraries and Set Up Dependencies","metadata":{}},{"id":"51ffd50e792a2102","cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nfrom tqdm import tqdm\nimport os\n\ntqdm._instances.clear()  # Clear any existing progress bars\ntqdm.pandas(disable=True)  # Disable tqdm globally\n# Suppress tqdm progress bars\ntqdm.disable = True\n\n# (Optional) Disable tqdm via environment variable\nos.environ[\"DISABLE_TQDM\"] = \"1\"\n\nimport sys\nsys.path.append('/kaggle/input/stockpredictionmodel/pytorch/default/1')\nfrom HybridStockPredictionModel import StockPerformancePredictionModel\n\nimport matplotlib.pyplot as plt\n","metadata":{"ExecuteTime":{"end_time":"2024-11-24T16:54:45.913319Z","start_time":"2024-11-24T16:54:42.698716Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:39:07.197740Z","iopub.execute_input":"2024-11-26T12:39:07.198077Z","iopub.status.idle":"2024-11-26T12:39:07.205288Z","shell.execute_reply.started":"2024-11-26T12:39:07.198051Z","shell.execute_reply":"2024-11-26T12:39:07.204539Z"}},"outputs":[],"execution_count":56},{"id":"4b42d729-a352-4029-bbab-790258cae14a","cell_type":"code","source":"import joblib\n\nhidden_dim = 128  # Example hidden size\nhistorical_scaler = joblib.load(\"/kaggle/input/scaler/historical_scaler.pkl\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_save_path = \"/kaggle/input/trainedmodel1/best_model.pth\"\nmodel = StockPerformancePredictionModel(3, 24, hidden_dim, 24)\nmodel.load_state_dict(torch.load(model_save_path, weights_only=True))\nmodel.to(device)\n\n\npredict = model(\n    idea=[\"AbCellera Biologics Inc. builds an engine for antibody drug discovery and development. Its engine discovers antibodies from natural immune responses, which are pre-enriched for antibodies. The company's preclinical products are ABCL635 for metabolic and endocrine conditions; and ABCL575 for atopic dermatitis. It has a research collaboration and license agreement with Eli Lilly and Company; a research collaboration with Confo Therapeutics for the discovery of therapeutic antibody candidates targeting two undisclosed GPCR targets; and strategic collaboration with Biogen Inc. to discover therapeutic antibodies for neurological conditions, as well as collaboration with Viking Global Investors and ArrowMark Partners. The company was incorporated in 2012 and is headquartered in Vancouver, Canada.\"],\n    use_auxiliary_inputs=False\n)\n\nprint(predict)\n\n# Ensure the tensor is detached from the computation graph before converting to numpy\npredictions_np = predict.detach().cpu().numpy()\n\nprediction_historical = historical_scaler.inverse_transform(predictions_np)\n\nprint(prediction_historical)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:59:24.825013Z","iopub.execute_input":"2024-11-26T12:59:24.825398Z","iopub.status.idle":"2024-11-26T12:59:26.116168Z","shell.execute_reply.started":"2024-11-26T12:59:24.825353Z","shell.execute_reply":"2024-11-26T12:59:26.115250Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  n_nonblank = len(\"\".join(repr_.split()))\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d866d7db5e634ca586f6024e4af79be0"}},"metadata":{}},{"name":"stdout","text":"tensor([[0.0559, 0.0691, 0.0856, 0.1086, 0.1388, 0.1745, 0.2124, 0.2493, 0.2823,\n         0.3093, 0.3294, 0.3426, 0.3496, 0.3514, 0.3495, 0.3450, 0.3390, 0.3325,\n         0.3261, 0.3204, 0.3156, 0.3118, 0.3091, 0.3073]], device='cuda:0',\n       grad_fn=<SqueezeBackward1>)\n[[36448.363  52886.816  44280.86   31755.986  24987.928  11462.613\n   2264.2227  2916.9507  1496.0712  1391.8411  1155.2336  1204.2379\n   1296.8995  1266.2158  1212.2828  1251.4788  1170.2217  1255.5984\n   1291.9784  1190.294   1233.674   1313.3424  1445.3121  1601.451 ]]\n","output_type":"stream"}],"execution_count":59},{"id":"bc548ced91f569bf","cell_type":"markdown","source":"# Load and Preprocess the Dataset","metadata":{}},{"id":"7dec583cb6041997","cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/csv-dataset/normalized_real_company_stock_dataset.csv\")\n\n# Define columns\nidea_column = \"business_description\"\nstatic_feature_columns = [\"market_size\", \"investment\", \"team_strength\"]\nhistorical_columns = [col for col in df.columns if col.startswith(\"month_\")]\n\n# Define the target as stock performance for the next 6 months\ntarget_columns = historical_columns  # Last 6 months of performance\nforecast_steps = 24\n\n# Prepare your features and target\nideas = df[idea_column].values\nstatic_features = df[static_feature_columns].values\nhistorical_data = df[historical_columns].values\ntargets = df[target_columns].values\n\n# Scale static features\nscaler_static = StandardScaler()\nstatic_features = scaler_static.fit_transform(static_features)\n\n# Train-test split\nideas_train, ideas_val, static_train, static_val, hist_train, hist_val, y_train, y_val = train_test_split(\n    ideas, static_features, historical_data, targets, test_size=0.2, random_state=42\n)\n\n# Convert the ideas to embeddings using SentenceTransformer\nfrom sentence_transformers import SentenceTransformer\n\n# Load the pre-trained model (this will map each idea to a vector of length 384)\ntext_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Convert the text ideas into embeddings (numerical vectors)\nideas_train_embeddings = text_encoder.encode(ideas_train, convert_to_numpy=True)\nideas_val_embeddings = text_encoder.encode(ideas_val, convert_to_numpy=True)\n\n# Now convert these embeddings into torch tensors\nideas_train_tensor = torch.tensor(ideas_train_embeddings, dtype=torch.float32)\nideas_val_tensor = torch.tensor(ideas_val_embeddings, dtype=torch.float32)\n\n# Convert static features and target variables into torch tensors\nstatic_train_tensor = torch.tensor(static_train, dtype=torch.float32)\nstatic_val_tensor = torch.tensor(static_val, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32)","metadata":{"ExecuteTime":{"end_time":"2024-11-24T16:57:30.518779Z","start_time":"2024-11-24T16:54:45.923217Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:36:59.239604Z","iopub.status.idle":"2024-11-26T12:36:59.239895Z","shell.execute_reply.started":"2024-11-26T12:36:59.239756Z","shell.execute_reply":"2024-11-26T12:36:59.239771Z"}},"outputs":[],"execution_count":null},{"id":"2ddb54475a0454f","cell_type":"markdown","source":"","metadata":{}},{"id":"794309d3bc005e40","cell_type":"markdown","source":"# Define the PyTorch Dataset","metadata":{}},{"id":"994f632eb01d453c","cell_type":"code","source":"class StockDataset(Dataset):\n\n    def __init__(self, ideas, static_features, historical_data, targets):\n\n        self.ideas = ideas\n\n        self.static_features = torch.tensor(static_features, dtype=torch.float32)\n\n        self.historical_data = torch.tensor(historical_data, dtype=torch.float32)\n\n        self.targets = torch.tensor(targets, dtype=torch.float32)\n\n\n\n    def __len__(self):\n\n        return len(self.targets)\n\n\n\n    def __getitem__(self, idx):\n\n        # Return the idea (text), static features, historical data, and the target\n\n        return self.ideas[idx], self.static_features[idx], self.historical_data[idx], self.targets[idx]\n\n\n\n\n\ntrain_dataset = StockDataset(ideas=ideas_train,\n\n                             static_features=static_train,\n\n                             historical_data=hist_train,\n\n                             targets=y_train)\n\n\n\nval_dataset = StockDataset(ideas=ideas_val,\n\n                           static_features=static_val,\n\n                           historical_data=hist_val,\n\n                           targets=y_val)\n","metadata":{"ExecuteTime":{"end_time":"2024-11-24T16:57:30.665847Z","start_time":"2024-11-24T16:57:30.661787Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:36:59.240695Z","iopub.status.idle":"2024-11-26T12:36:59.241003Z","shell.execute_reply.started":"2024-11-26T12:36:59.240857Z","shell.execute_reply":"2024-11-26T12:36:59.240873Z"}},"outputs":[],"execution_count":null},{"id":"458028ca10e6bfe1","cell_type":"markdown","source":"","metadata":{}},{"id":"72271fe140850f6c","cell_type":"markdown","source":"","metadata":{}},{"id":"9557115e5f9083c4","cell_type":"markdown","source":"### Training the model\n\nHere we import the model and set it up for training","metadata":{}},{"id":"initial_id","cell_type":"code","source":"import torch.optim as optim\n\nimport torch.nn as nn\n\nimport sys\nsys.path.append('/kaggle/input/stockpredictionmodel/pytorch/default/1')\nfrom HybridStockPredictionModel import StockPerformancePredictionModel\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n# Model initialization\n\nstatic_feature_dim = static_features.shape[1]\n\nhistorical_dim = historical_data.shape[1]\n\nhidden_dim = 128  # Example hidden size\n\n\n\n# Training loop\ndef train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=100):\n    \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        model.train()  # Set the model to training mode\n\n        train_loss = 0.0\n\n        # Training phase\n        for ideas, static_features, historical_data, targets in train_loader:\n            # Check for invalid values in data\n            assert not torch.isnan(static_features).any(), \"Static features contain NaN\"\n            assert not torch.isinf(static_features).any(), \"Static features contain Inf\"\n            assert not torch.isnan(historical_data).any(), \"Historical data contains NaN\"\n            assert not torch.isinf(historical_data).any(), \"Historical data contains Inf\"\n\n            # Move to device\n            static_features, historical_data, targets = (\n                static_features.to(device),\n                historical_data.to(device),\n                targets.to(device),\n            )\n\n            # Forward pass\n            predictions = model(\n                idea=ideas,\n                static_features=static_features,\n                historical_data=historical_data,\n                use_auxiliary_inputs=True,\n                predict_autoregressively=False\n            )\n\n\n            # Compute TemporalLoss\n            loss = criterion(predictions, targets)\n    \n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n            optimizer.step()\n\n            # Track the training loss\n            train_loss += loss.item()\n\n        # Evaluate the model after training for the epoch\n        val_loss, _, _, _ = evaluate(model, val_loader, device, criterion)  # Use TemporalLoss for evaluation\n\n        # Adjust learning rate based on validation loss\n        scheduler.step(val_loss)\n\n        import os\n    \n        # Early Stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            # Create a folder for models\n            os.makedirs(\"models\", exist_ok=True)\n            torch.save(model.state_dict(), \"models/best_model.pth\")\n\n        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss}\")\n\n\n\n\n\n","metadata":{"ExecuteTime":{"end_time":"2024-11-24T17:08:19.368327Z","start_time":"2024-11-24T17:08:19.363594Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:36:59.242785Z","iopub.status.idle":"2024-11-26T12:36:59.243068Z","shell.execute_reply.started":"2024-11-26T12:36:59.242932Z","shell.execute_reply":"2024-11-26T12:36:59.242946Z"}},"outputs":[],"execution_count":null},{"id":"cc8c8d4c3dfd6e4","cell_type":"markdown","source":"#### We can also create a a custom loss function and an optimizer","metadata":{}},{"id":"d00b397e0d5f09c6","cell_type":"markdown","source":"You can also possible create custom loss functions:","metadata":{}},{"id":"81795ae578e9baec","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TemporalLoss(nn.Module):\n    def __init__(self, lambda_smooth=0.1):\n        super(TemporalLoss, self).__init__()\n        self.lambda_smooth = lambda_smooth\n        self.mse_loss = nn.MSELoss()\n\n    def forward(self, predictions, targets):\n        # Reshape predictions if needed\n        if predictions.dim() == 2:\n            predictions = predictions.unsqueeze(-1)  # (batch_size, sequence_length) -> (batch_size, sequence_length, 1)\n        if targets.dim() == 2:\n            targets = targets.unsqueeze(-1)  # Ensure targets match dimensions\n\n        # Base MSE loss\n        base_loss = self.mse_loss(predictions, targets)\n\n        # Temporal smoothness loss\n        temporal_diff = predictions[:, 1:, :] - predictions[:, :-1, :]\n        smoothness_loss = torch.mean(temporal_diff**2)\n\n        # Combine losses\n        combined_loss = base_loss + self.lambda_smooth * smoothness_loss\n        return combined_loss\n","metadata":{"ExecuteTime":{"end_time":"2024-11-24T16:57:33.854939Z","start_time":"2024-11-24T16:57:33.852634Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:36:59.244552Z","iopub.status.idle":"2024-11-26T12:36:59.244859Z","shell.execute_reply.started":"2024-11-26T12:36:59.244715Z","shell.execute_reply":"2024-11-26T12:36:59.244731Z"}},"outputs":[],"execution_count":null},{"id":"6a536c54deb3fb20","cell_type":"markdown","source":"","metadata":{}},{"id":"5998d18e19fbe2a2","cell_type":"markdown","source":"### Evaluation\n\nAfter training we are going to evaluate our model:","metadata":{}},{"id":"b65b87305950eae5","cell_type":"code","source":"import joblib\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ndef evaluate(model, val_loader, device, criterion):\n    model.eval()  # Set the model to evaluation mode\n\n    val_loss = 0.0\n    all_predictions = []\n    all_targets = []\n    correct = 0\n    total = 0\n\n    # Load the scalers for denormalization\n    static_scaler = joblib.load(\"/kaggle/input/scaler/static_scaler.pkl\")\n    historical_scaler = joblib.load(\"/kaggle/input/scaler/historical_scaler.pkl\")\n\n    with torch.no_grad():  # No need to track gradients during evaluation\n        for ideas_batch, static_batch, historical_batch, target_batch in val_loader:\n            # Move data to device\n            static_batch = static_batch.to(device)\n            historical_batch = historical_batch.to(device)\n            target_batch = target_batch.to(device)\n    \n            # Get predictions\n            predictions = model(idea=ideas_batch, use_auxiliary_inputs=False)\n    \n            # Compute temporal loss\n            loss = criterion(predictions, target_batch)\n            val_loss += loss.item()\n    \n            # Collect all predictions and targets for evaluation\n            all_predictions.append(predictions.cpu().numpy())\n            all_targets.append(target_batch.cpu().numpy())\n\n            print(predictions.shape)\n    \n            # Evaluate similarity between predictions and historical targets\n            predictions_np = predictions.cpu().numpy()\n            target_batch_np = target_batch.cpu().numpy()\n\n            # Denormalize the predication values similarly\n            prediction_historical = historical_scaler.inverse_transform(predictions_np)\n            target_historical = historical_scaler.inverse_transform(target_batch_np)\n    \n            # Compute Mean Absolute Error (MAE) as a similarity measure\n            mae = np.mean(np.abs(predictions_np - target_batch_np))\n            print(f\"Mean Absolute Error (MAE) for batch: {mae}\")\n\n            # Manually calculate MSE in original scale for reporting\n            mse_loss_denorm = np.mean((prediction_historical - target_historical)**2)\n            print(f\"Denormalized Loss: {mse_loss_denorm}\")\n\n            # Compute Mean Absolute Error (MAE) as a similarity measure\n            mae = np.mean(np.abs(prediction_historical - target_historical))\n            print(f\"Mean Absolute Error (MAE) for batch: {mae}\")\n    \n            # Compute correlation (optional, if normalized data allows for it)\n            for i in range(predictions_np.shape[0]):  # Iterate over the batch\n                pred_series = predictions_np[i]\n                target_series = target_batch_np[i]\n    \n                # Pearson correlation coefficient (if meaningful for your data)\n                correlation = np.corrcoef(pred_series.flatten(), target_series.flatten())[0, 1]\n                print(f\"Correlation for batch {i}: {correlation}\")\n\n            # Compute correlation (optional, if normalized data allows for it)\n            for i in range(prediction_historical.shape[0]):  # Iterate over the batch\n                pred_series = prediction_historical[i]\n                target_series = target_historical[i]\n    \n                # Pearson correlation coefficient (if meaningful for your data)\n                correlation = np.corrcoef(pred_series.flatten(), target_series.flatten())[0, 1]\n                print(f\"Correlation for batch {i}: {correlation}\")\n                \n                # Manually calculate MSE in original scale for reporting\n                mse_loss_denorm = np.mean((pred_series - target_series)**2)\n                print(f\"Denormalized Loss: {mse_loss_denorm}\")\n    \n                # Compute Mean Absolute Error (MAE) as a similarity measure\n                mae = np.mean(np.abs(pred_series - target_series))\n                print(f\"Mean Absolute Error (MAE) for batch: {mae}\")\n    \n    # Average validation loss\n    val_loss /= len(val_loader)\n    print(f\"Validation Loss: {val_loss}\")\n\n    # Flatten the lists to make evaluation easier\n    all_predictions = np.concatenate(all_predictions, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n\n    # Denormalize the predication values similarly\n    predictions_historical = historical_scaler.inverse_transform(all_predictions)\n\n    # Denormalize the target values similarly\n    targets_historical = historical_scaler.inverse_transform(all_targets)\n\n    # Calculate MSE and MAE for historical features\n    mse_historical = mean_squared_error(predictions_historical, targets_historical)\n    mae_historical = mean_absolute_error(targets_historical, predictions_historical)\n    r2_historical = r2_score(targets_historical, predictions_historical)\n\n    # Print metrics for each section\n    print(f\"MSE for historical features: {mse_historical}\")\n    print(f\"MAE for historical features: {mae_historical}\")\n    print(f\"R² for historical features: {r2_historical}\")\n    print(f\"___ This is the Prediction ___\")\n    print(f\"{predictions}\")\n\n    # import matplotlib.pyplot as plt\n\n    # plt.scatter(all_targets, all_predictions, alpha=0.5)\n    # plt.xlabel(\"Actual Values\")\n    # plt.ylabel(\"Predicted Values\")\n    # plt.title(\"Actual vs. Predicted\")\n    # plt.show()\n    \n    # residuals = all_targets - all_predictions\n    # plt.hist(residuals, bins=30, edgecolor='k')\n    # plt.xlabel(\"Residual\")\n    # plt.ylabel(\"Frequency\")\n    # plt.title(\"Residual Distribution\")\n    # plt.show()\n\n\n    return val_loss, mse_historical, mae_historical, r2_historical\n","metadata":{"ExecuteTime":{"end_time":"2024-11-24T16:57:33.893335Z","start_time":"2024-11-24T16:57:33.889805Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:36:59.246454Z","iopub.status.idle":"2024-11-26T12:36:59.246746Z","shell.execute_reply.started":"2024-11-26T12:36:59.246609Z","shell.execute_reply":"2024-11-26T12:36:59.246623Z"}},"outputs":[],"execution_count":null},{"id":"2aafad460b392c3","cell_type":"markdown","source":"# Integration with Training Loop","metadata":{}},{"id":"3b5c679b090c90a8","cell_type":"code","source":"# Run Training and Evaluation\nmodel_save_path = \"/kaggle/input/trainedmodel1/best_model.pth\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = StockPerformancePredictionModel(static_feature_dim, historical_dim, hidden_dim, forecast_steps).to(device)\n# model = StockPerformancePredictionModel(static_feature_dim, historical_dim, hidden_dim, forecast_steps)\n# model.load_state_dict(torch.load(model_save_path, weights_only=True))\n# model.to(device)\n\n# Define batch size\nbatch_size = 32\n\n# Create DataLoader for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Loss and optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ntemporal_loss = TemporalLoss(lambda_smooth=0.1)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Loading widget...\")\n\n\ntrain_model(model, train_loader, val_loader, temporal_loss, optimizer, device, epochs=50)\n","metadata":{"ExecuteTime":{"end_time":"2024-11-24T17:09:17.519928Z","start_time":"2024-11-24T17:08:22.751580Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:36:59.247804Z","iopub.status.idle":"2024-11-26T12:36:59.248238Z","shell.execute_reply.started":"2024-11-26T12:36:59.248013Z","shell.execute_reply":"2024-11-26T12:36:59.248035Z"}},"outputs":[],"execution_count":null}]}