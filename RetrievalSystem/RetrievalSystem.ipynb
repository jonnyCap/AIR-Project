{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval System\n",
    "This notebook implementes the retrievel system"
   ],
   "id": "af0ab5b4157a4f1f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-03T12:40:22.721086Z",
     "start_time": "2025-01-03T12:40:22.711092Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "BERT_ENCODING_SIZE = 768\n",
    "\n",
    "class RetrievalSystem:\n",
    "    def __init__(self, path: str, retrieval_number: int = 16):\n",
    "        \"\"\"\n",
    "        Constructor to initialize the RetrievalSystem with a CSV file.\n",
    "        Args:\n",
    "            path (str): The path to the CSV file to load.\n",
    "        \"\"\"\n",
    "        self.model_type = 'all-MiniLM-L6-v2'\n",
    "        self.retrieval_number = retrieval_number\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            self.data = pd.read_csv(path)\n",
    "        self.model = SentenceTransformer(self.model_type)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy for preprocessing\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the input text by removing stop words and applying lemmatization.\n",
    "        Args:\n",
    "            text (str): The text to preprocess.\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        # Remove stop words and punctuation, and apply lemmatization\n",
    "        preprocessed_text = \" \".join(\n",
    "            [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        )\n",
    "        return preprocessed_text\n",
    "\n",
    "    def find_similar_entries_for_batch(self, texts: str, top_n: int = None, excluded_tickers=None):\n",
    "        \"\"\"\n",
    "        Embeds a batch of texts and finds the most similar entries in the dataset for each.\n",
    "        Args:we\n",
    "            texts (list): List of input texts to embed and compare.\n",
    "            excluded_tickers (list): List of tickers to exclude from similarity checks.\n",
    "        Returns:\n",
    "            list: A list of tuples containing embeddings and DataFrames for each text.\n",
    "        \"\"\"\n",
    "\n",
    "        if not top_n:\n",
    "            top_n = self.retrieval_number\n",
    "\n",
    "        # Preprocess all texts\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "\n",
    "        # Generate embeddings for all input texts as a batch\n",
    "        input_embeddings = self.model.encode(processed_texts)\n",
    "\n",
    "        # Prepare the dataset\n",
    "        if 'embedding' not in self.data.columns:\n",
    "            raise ValueError(\"The CSV file must have an 'embedding' column.\")\n",
    "\n",
    "        copied_data = self.data.copy()\n",
    "\n",
    "        # Exclude rows with tickers in excluded_tickers\n",
    "        if excluded_tickers:\n",
    "            copied_data = copied_data[~copied_data['tickers'].isin(excluded_tickers)]\n",
    "\n",
    "        # Convert embeddings column to lists if necessary\n",
    "        if isinstance(copied_data['embedding'].iloc[0], str):\n",
    "            copied_data['embedding'] = copied_data['embedding'].apply(eval)\n",
    "\n",
    "        embeddings = copied_data['embedding'].tolist()\n",
    "        dataset_embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "        # Compute cosine similarity for all input embeddings\n",
    "        input_embeddings = torch.tensor(input_embeddings, dtype=torch.float32)\n",
    "        similarities = torch.matmul(input_embeddings, dataset_embeddings.T)  # Efficient batch cosine similarity\n",
    "\n",
    "        # Collect top-N similar entries for each input text\n",
    "        results = []\n",
    "        for i, sim in enumerate(similarities):\n",
    "            copied_data['similarity'] = sim.numpy()  # Add similarity scores to the dataset\n",
    "            top_results = copied_data.sort_values(by='similarity', ascending=False).head(top_n)\n",
    "            results.append((input_embeddings[i].numpy(), top_results))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def find_similar_entries(self, text: str, top_n: int = None, excluded_tickers=None):\n",
    "        \"\"\"\n",
    "        Embeds the input text using BERT, compares it with the entries in the CSV file,\n",
    "        and returns the most similar entries based on cosine similarity.\n",
    "        Args:\n",
    "            text (str): The input text to embed and compare.\n",
    "            top_n (int): The number of most similar entries to return.\n",
    "            excluded_tickers (list): List of tickers to exclude from similarity checks.\n",
    "        Returns:\n",
    "            pd.DataFrame: The top-n most similar entries from the CSV.\n",
    "        \"\"\"\n",
    "        # Preprocess the input text\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        if not top_n:\n",
    "            top_n = self.retrieval_number\n",
    "\n",
    "        # Generate embedding for the preprocessed text\n",
    "        input_embedding = self.model.encode([text])\n",
    "\n",
    "        # Load embeddings from the CSV\n",
    "        if 'embedding' not in self.data.columns:\n",
    "            raise ValueError(\"The CSV file must have an 'embedding' column.\")\n",
    "\n",
    "        # Create a copy of self.data to work with\n",
    "        copied_data = self.data.copy()\n",
    "\n",
    "        # Exclude rows with tickers in excluded_tickers\n",
    "        if excluded_tickers:\n",
    "            copied_data = copied_data[~copied_data['tickers'].isin(excluded_tickers)]\n",
    "\n",
    "        # Convert strings to lists only if they are strings\n",
    "        if isinstance(copied_data['embedding'].iloc[0], str):\n",
    "            copied_data['embedding'] = copied_data['embedding'].apply(eval)\n",
    "\n",
    "        embeddings = copied_data['embedding'].tolist()\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        similarities = cosine_similarity(input_embedding, embeddings)[0]\n",
    "        copied_data['similarity'] = similarities\n",
    "\n",
    "        # Sort by similarity and return the top N results\n",
    "        return input_embedding, copied_data.sort_values(by='similarity', ascending=False).head(top_n)\n",
    "\n",
    "\n",
    "    def process_and_save_embeddings(self, path: str, output_path: str):\n",
    "        \"\"\"\n",
    "        Embeds the 'business_description' column from a new CSV file, keeps only 'tickers' and 'embedding',\n",
    "        and saves the results in a new CSV with 'tickers' as the index.\n",
    "        Args:\n",
    "            path (str): The path to the CSV file to process.\n",
    "            output_path (str): The path to save the output CSV.\n",
    "        \"\"\"\n",
    "        # Load new data\n",
    "        new_data = pd.read_csv(path)\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        if 'tickers' not in new_data.columns:\n",
    "            raise ValueError(\"The CSV file must have a 'tickers' column.\")\n",
    "        if 'business_description' not in new_data.columns:\n",
    "            raise ValueError(\"The CSV file must have a 'business_description' column.\")\n",
    "\n",
    "        # Preprocess and embed the 'business_description' column\n",
    "        new_data['processed_description'] = new_data['business_description'].apply(self.preprocess_text)\n",
    "        new_data['embedding'] = new_data['processed_description'].apply(lambda x: self.model.encode([x])[0].tolist())\n",
    "\n",
    "        # Keep only 'tickers' and 'embedding' columns\n",
    "        processed_data = new_data[['tickers', 'embedding']]\n",
    "\n",
    "        # Set 'tickers' as the index\n",
    "        processed_data.set_index('tickers', inplace=True)\n",
    "\n",
    "        # Save the processed data\n",
    "        processed_data.to_csv(output_path)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creation of Embedding dataset\n",
    "We create this in order for faster execution in our final user pripeline"
   ],
   "id": "c22e756ec3740f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T12:44:24.374073Z",
     "start_time": "2025-01-03T12:40:22.764685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths relative to the current working directory\n",
    "INPUT_PATH = \"../Dataset/Data/normalized_real_company_stock_dataset_large.csv\"\n",
    "OUTPUT_PATH = \"Embeddings/embeddings.csv\"\n",
    "\n",
    "CREATE_DATASET = True\n",
    "TEST = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if CREATE_DATASET:\n",
    "        retrieval_system = RetrievalSystem(OUTPUT_PATH)\n",
    "        retrieval_system.process_and_save_embeddings(INPUT_PATH, OUTPUT_PATH)\n",
    "\n",
    "    if TEST:\n",
    "        retrieval_system = RetrievalSystem(OUTPUT_PATH)\n",
    "        idea = \"Hello world program that can print hello world\"\n",
    "        idea = \"American Assets Trust, Inc. is a full service, vertically integrated and self-administered real estate investment trust ('REIT'), headquartered in San Diego, California. The company has over 55 years of experience in acquiring, improving, developing and managing premier office, retail, and residential properties throughout the United States in some of the nation's most dynamic, high-barrier-to-entry markets primarily in Southern California, Northern California, Washington, Oregon, Texas and Hawaii. The company's office portfolio comprises approximately 4.1 million rentable square feet, and its retail portfolio comprises approximately 3.1 million rentable square feet. In addition, the company owns one mixed-use property (including approximately 94,000 rentable square feet of retail space and a 369-room all-suite hotel) and 2,110 multifamily units. In 2011, the company was formed to succeed to the real estate business of American Assets, Inc., a privately held corporation founded in 1967 and, as such, has significant experience, long-standing relationships and extensive knowledge of its core markets, submarkets and asset classes.\"\n",
    "        result = retrieval_system.find_similar_entries(idea, 10)\n",
    "        print(result)"
   ],
   "id": "e45952fec3089a43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 9.14912745e-02, -9.23525766e-02, -4.35219333e-02,\n",
      "         9.80848912e-03, -7.02797100e-02,  2.99639292e-02,\n",
      "         2.25051995e-02, -7.87077844e-02,  6.23483397e-02,\n",
      "        -2.21265145e-02,  2.13831943e-02,  4.86220196e-02,\n",
      "         6.48474544e-02, -5.80228418e-02,  3.64770107e-02,\n",
      "         3.52777424e-03,  1.03864484e-02,  2.50134505e-02,\n",
      "        -2.46218890e-02,  7.25172758e-02,  6.29638135e-03,\n",
      "        -5.36850765e-02, -7.23614544e-02, -5.30713797e-03,\n",
      "         3.83347236e-02, -6.81003034e-02, -4.54039983e-02,\n",
      "         8.33974108e-02,  5.67588722e-03, -1.14184789e-01,\n",
      "         4.61822674e-02, -8.77586659e-03,  4.18145917e-02,\n",
      "        -9.97522008e-03,  1.15019344e-01,  7.10957199e-02,\n",
      "        -5.31006530e-02, -5.99623006e-03, -5.12688793e-02,\n",
      "        -1.75157823e-02, -1.89701412e-02,  1.02545107e-02,\n",
      "         4.85525616e-02,  1.21535305e-02,  6.39036205e-03,\n",
      "        -8.80437251e-03, -2.76259072e-02,  6.74320897e-03,\n",
      "         1.05971843e-01,  6.92841485e-02, -2.76570674e-02,\n",
      "         6.44401908e-02,  1.32183516e-02,  5.96993938e-02,\n",
      "        -9.07804519e-02,  6.01291135e-02, -2.53683999e-02,\n",
      "         3.55714001e-02, -6.77603623e-03, -4.32881303e-02,\n",
      "         2.66057700e-02, -3.52588333e-02,  1.37680799e-01,\n",
      "         8.17117281e-03, -1.27780493e-02,  5.48861735e-02,\n",
      "        -9.41574723e-02,  4.41747578e-03, -1.09178079e-02,\n",
      "        -1.82557911e-01,  6.37381971e-02, -7.04216808e-02,\n",
      "        -3.20129730e-02, -5.57020493e-03, -2.95817363e-03,\n",
      "        -4.88549704e-04,  4.39154990e-02,  1.91197768e-02,\n",
      "         4.42229919e-02, -5.82295321e-02, -1.15748020e-02,\n",
      "         2.56399643e-02, -2.55692098e-03,  4.49234992e-02,\n",
      "        -6.45118952e-02,  2.88328417e-02,  5.25026843e-02,\n",
      "        -4.31773812e-02, -3.10514253e-02,  2.64008734e-02,\n",
      "         3.53662074e-02, -5.33199608e-02,  3.72563861e-03,\n",
      "        -2.78599784e-02, -4.26474102e-02, -2.59317178e-02,\n",
      "        -5.81879988e-02, -6.60025477e-02, -9.78239328e-02,\n",
      "        -3.71140130e-02,  2.28151288e-02,  4.73531932e-02,\n",
      "        -1.92915238e-02, -3.39287668e-02, -1.01573564e-01,\n",
      "        -8.41083098e-03,  4.01781611e-02,  2.08232552e-03,\n",
      "        -2.96198651e-02,  2.44204123e-02, -8.01421516e-03,\n",
      "         1.96914896e-02, -9.69339460e-02,  4.77974908e-03,\n",
      "         3.66261043e-02,  1.84633059e-03, -7.22452700e-02,\n",
      "        -5.38360141e-02,  9.77126583e-02, -1.09761193e-01,\n",
      "         1.23095000e-02,  5.29664643e-02,  2.74376273e-02,\n",
      "        -5.05742468e-02, -6.17128164e-02, -3.05564012e-02,\n",
      "        -1.29688289e-02,  1.92223228e-32, -6.28886819e-02,\n",
      "         5.86437695e-02,  3.70475948e-02,  6.10051192e-02,\n",
      "        -5.37074991e-02, -4.93573025e-02,  3.53030153e-02,\n",
      "         6.80149794e-02, -1.06416233e-01, -2.44086292e-02,\n",
      "         7.88826793e-02,  1.03377014e-01, -1.21696265e-02,\n",
      "         5.20075150e-02,  1.08935274e-02, -1.17155630e-02,\n",
      "        -2.97288354e-02,  4.95736636e-02,  7.79310018e-02,\n",
      "        -1.28248528e-01, -6.55945623e-03,  6.05341000e-03,\n",
      "         1.76046360e-02, -1.51160723e-02,  2.44303010e-02,\n",
      "        -4.51865755e-02,  1.53846862e-02,  6.68706223e-02,\n",
      "        -6.79003401e-03,  3.53475697e-02,  8.67502857e-03,\n",
      "        -1.61922909e-02, -1.15498565e-02,  4.13100757e-02,\n",
      "         4.70977947e-02,  1.82272904e-02, -1.46080728e-03,\n",
      "         4.26335894e-02,  2.95325201e-02,  1.14474203e-02,\n",
      "        -1.96663123e-02,  4.83193761e-03,  3.48434746e-02,\n",
      "         4.86626662e-02,  4.30865176e-02, -1.24637326e-02,\n",
      "         5.87196052e-02,  6.34402186e-02, -1.20842727e-02,\n",
      "         4.62853312e-02, -7.48915151e-02,  9.10501741e-03,\n",
      "        -4.72682901e-02,  2.73357015e-02, -6.88838214e-03,\n",
      "        -1.43795721e-02, -4.08187099e-02,  1.07916547e-02,\n",
      "        -2.72567812e-02,  6.29530754e-04, -2.51197275e-02,\n",
      "         4.26301584e-02, -8.23068023e-02, -7.14115333e-03,\n",
      "        -1.08877666e-01,  2.92179361e-02,  4.21184041e-02,\n",
      "        -4.90588928e-03,  5.30990958e-02,  3.11647281e-02,\n",
      "         1.21910190e-02, -3.77001204e-02,  7.76442587e-02,\n",
      "         1.62117779e-02,  2.24088877e-02, -7.82610178e-02,\n",
      "        -7.14586377e-02,  1.02170475e-01,  4.51039784e-02,\n",
      "         1.45086786e-02, -3.51956189e-02,  8.91562027e-04,\n",
      "         2.27922630e-02,  8.94771144e-02, -1.06003657e-02,\n",
      "        -1.82998069e-02,  1.04125485e-01, -5.40078692e-02,\n",
      "        -6.27242494e-03, -2.70332806e-02, -2.21333257e-03,\n",
      "         5.31349480e-02,  4.76492904e-02,  9.68848243e-02,\n",
      "        -2.95850132e-02, -1.82557138e-32, -4.80697770e-03,\n",
      "        -7.07248524e-02,  4.84646298e-02,  1.88820623e-02,\n",
      "         3.95619236e-02, -7.93914497e-02, -1.62375849e-02,\n",
      "         6.26765117e-02, -1.02844965e-02, -4.54104831e-03,\n",
      "        -2.18666624e-03, -1.55672142e-02, -3.89390974e-03,\n",
      "        -5.88672934e-03, -8.07128847e-02, -2.13699881e-02,\n",
      "         6.41150698e-02, -7.29992911e-02,  2.88977530e-02,\n",
      "        -6.19950555e-02,  4.48255725e-02, -2.20651440e-02,\n",
      "         4.45672460e-02,  6.67205974e-02,  4.04844582e-02,\n",
      "         1.93255823e-02, -9.19108465e-02, -1.82433762e-02,\n",
      "         2.17570644e-02,  6.76819496e-03, -2.77567822e-02,\n",
      "         4.76892404e-02, -4.24227044e-02,  1.08548284e-01,\n",
      "        -8.43636841e-02, -8.56544152e-02, -4.52536717e-02,\n",
      "        -2.04019863e-02,  1.54737774e-02,  1.05440859e-02,\n",
      "         2.94555854e-02, -6.91985339e-02, -6.68906141e-03,\n",
      "        -2.92477999e-02,  5.46590760e-02, -2.74376776e-02,\n",
      "         5.27897589e-02, -8.24430734e-02, -2.31512804e-02,\n",
      "        -3.27535383e-02, -6.69630095e-02, -2.75105834e-02,\n",
      "        -1.10723197e-01,  1.43115018e-02, -3.98771651e-02,\n",
      "         1.04438290e-01,  7.22136721e-02,  7.35056996e-02,\n",
      "        -6.28327653e-02,  5.48924617e-02,  1.08768404e-01,\n",
      "         1.07753381e-01,  1.50017662e-03,  7.27641135e-02,\n",
      "         6.95097027e-03, -1.08564999e-02,  3.81410532e-02,\n",
      "        -7.01138079e-02, -7.41821378e-02, -4.98836525e-02,\n",
      "         9.61022545e-03, -4.74321097e-02,  4.82915901e-03,\n",
      "        -9.92875174e-02, -4.74172272e-02,  6.92198845e-03,\n",
      "         2.52057482e-02, -7.86786750e-02,  5.09521849e-02,\n",
      "        -1.55172125e-02, -1.37927867e-02,  2.16935687e-02,\n",
      "        -3.86039503e-02,  7.13415965e-02,  6.50257897e-03,\n",
      "         6.22586384e-02, -6.08594306e-02, -7.71394819e-02,\n",
      "        -4.63320687e-02,  4.39125896e-02, -1.07967831e-01,\n",
      "         1.14388019e-02, -4.40333188e-02, -5.34864739e-02,\n",
      "        -5.46789765e-02, -5.92056892e-08,  5.87623380e-03,\n",
      "         8.40595923e-03,  6.74424395e-02, -2.38151625e-02,\n",
      "         3.54020111e-02, -4.68793809e-02, -9.07839462e-03,\n",
      "         4.68951836e-02,  9.43066739e-03,  2.82766819e-02,\n",
      "         3.64526175e-02, -7.99685568e-02, -8.54264572e-02,\n",
      "        -1.28149567e-02, -1.08587340e-01, -4.71694283e-02,\n",
      "        -3.19253057e-02,  1.08175896e-01, -4.04869160e-03,\n",
      "        -2.44595930e-02,  6.72331974e-02,  3.70855182e-02,\n",
      "        -1.68179311e-02, -2.86784694e-02, -5.74514866e-02,\n",
      "         4.97713313e-02, -7.18493983e-02, -2.55953949e-02,\n",
      "         1.89024322e-02,  2.12163012e-02,  3.72476410e-04,\n",
      "        -2.86316443e-02,  8.93875025e-03, -5.83716147e-02,\n",
      "        -2.19269563e-02,  4.47289869e-02,  5.30447736e-02,\n",
      "         4.05645408e-02, -1.66853108e-02,  4.22468409e-02,\n",
      "        -5.55822365e-02,  1.49318657e-03, -1.19625535e-02,\n",
      "        -5.78573942e-02,  3.36658135e-02, -5.56729827e-03,\n",
      "        -9.62211564e-02,  1.78106260e-02,  4.04412039e-02,\n",
      "         3.94747928e-02,  2.76947208e-02, -8.04336146e-02,\n",
      "        -5.21521904e-02,  1.37495296e-02, -3.87341119e-02,\n",
      "        -3.64710949e-02, -3.37771252e-02, -4.24844772e-02,\n",
      "        -3.03871045e-03,  1.84147507e-02, -8.10906570e-03,\n",
      "        -1.06522553e-01, -2.58353539e-02,  6.91849515e-02]], dtype=float32),      tickers                                          embedding  similarity\n",
      "11       AAT  [0.09149127453565598, -0.09235257655382156, -0...    1.000000\n",
      "5035     SVC  [0.07474841177463531, -0.1564873903989792, -0....    0.685684\n",
      "4768    SILA  [0.04342179745435715, -0.07385082542896271, -0...    0.666683\n",
      "1182    CMCT  [0.04079378396272659, -0.1538570672273636, -0....    0.659057\n",
      "1507     DHC  [0.02887115441262722, -0.12647517025470734, 0....    0.639350\n",
      "4941    SQFT  [0.02567594312131405, -0.06226911023259163, 0....    0.631348\n",
      "4942   SQFTP  [0.02567594312131405, -0.06226911023259163, 0....    0.631348\n",
      "2548   HTIBP  [0.03974471613764763, -0.05112490803003311, -0...    0.623859\n",
      "2547    HTIA  [0.03974471613764763, -0.05112490803003311, -0...    0.623859\n",
      "4250    PPBI  [0.026093803346157074, -0.05643833428621292, -...    0.621110)\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
